{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to samgeo","text":"<p>A Python package for segmenting geospatial data with the Segment Anything Model (SAM) \ud83d\uddfa\ufe0f</p>"},{"location":"#introduction","title":"Introduction","text":"<p>The segment-geospatial package draws its inspiration from segment-anything-eo repository authored by Aliaksandr Hancharenka. To facilitate the use of the Segment Anything Model (SAM) for geospatial data, I have developed the segment-anything-py and segment-geospatial Python packages, which are now available on PyPI and conda-forge. My primary objective is to simplify the process of leveraging SAM for geospatial data analysis by enabling users to achieve this with minimal coding effort. I have adapted the source code of segment-geospatial from the segment-anything-eo repository, and credit for its original version goes to Aliaksandr Hancharenka.</p> <ul> <li>\ud83c\udd93 Free software: MIT license</li> <li>\ud83d\udcd6 Documentation: https://samgeo.gishub.org</li> </ul>"},{"location":"#citations","title":"Citations","text":"<ul> <li>Wu, Q., &amp; Osco, L. (2023). samgeo: A Python package for segmenting geospatial data with the Segment Anything Model (SAM). Journal of Open Source Software, 8(89), 5663, https://doi.org/10.21105/joss.05663</li> </ul>"},{"location":"#features","title":"Features","text":"<ul> <li>Download map tiles from Tile Map Service (TMS) servers and create GeoTIFF files</li> <li>Segment GeoTIFF files using the Segment Anything Model (SAM) and HQ-SAM</li> <li>Segment remote sensing imagery with text prompts</li> <li>Create foreground and background markers interactively</li> <li>Load existing markers from vector datasets</li> <li>Save segmentation results as common vector formats (GeoPackage, Shapefile, GeoJSON)</li> <li>Save input prompts as GeoJSON files</li> <li>Visualize segmentation results on interactive maps</li> </ul>"},{"location":"#examples","title":"Examples","text":"<ul> <li>Segmenting remote sensing imagery</li> <li>Automatically generating object masks</li> <li>Segmenting remote sensing imagery with input prompts</li> <li>Segmenting remote sensing imagery with box prompts</li> <li>Segmenting remote sensing imagery with text prompts</li> <li>Batch segmentation with text prompts</li> <li>Using segment-geospatial with ArcGIS Pro</li> <li>Segmenting swimming pools with text prompts</li> <li>Segmenting satellite imagery from the Maxar Open Data Program</li> </ul>"},{"location":"#demos","title":"Demos","text":"<ul> <li>Automatic mask generator</li> </ul> <ul> <li>Interactive segmentation with input prompts</li> </ul> <ul> <li>Input prompts from existing files</li> </ul> <ul> <li>Interactive segmentation with text prompts</li> </ul>"},{"location":"#tutorials","title":"Tutorials","text":"<p>Video tutorials are available on my YouTube Channel.</p> <ul> <li>Automatic mask generation</li> </ul> <p></p> <ul> <li>Using SAM with ArcGIS Pro</li> </ul> <p></p> <ul> <li>Interactive segmentation with text prompts</li> </ul> <p></p>"},{"location":"#using-sam-with-desktop-gis","title":"Using SAM with Desktop GIS","text":"<ul> <li>QGIS: Check out the Geometric Attributes plugin for QGIS. Credit goes to Bjorn Nyberg.</li> <li>ArcGIS: Check out the Segment Anything Model (SAM) Toolbox for ArcGIS and the Resources for Unlocking the Power of Deep Learning Applications Using ArcGIS. Credit goes to Esri.</li> </ul>"},{"location":"#computing-resources","title":"Computing Resources","text":"<p>The Segment Anything Model is computationally intensive, and a powerful GPU is recommended to process large datasets. It is recommended to have a GPU with at least 8 GB of GPU memory. You can utilize the free GPU resources provided by Google Colab. Alternatively, you can apply for AWS Cloud Credit for Research, which offers cloud credits to support academic research. If you are in the Greater China region, apply for the AWS Cloud Credit here.</p>"},{"location":"#legal-notice","title":"Legal Notice","text":"<p>This repository and its content are provided for educational purposes only. By using the information and code provided, users acknowledge that they are using the APIs and models at their own risk and agree to comply with any applicable laws and regulations. Users who intend to download a large number of image tiles from any basemap are advised to contact the basemap provider to obtain permission before doing so. Unauthorized use of the basemap or any of its components may be a violation of copyright laws or other applicable laws and regulations.</p>"},{"location":"#acknowledgements","title":"Acknowledgements","text":"<p>This project is based upon work partially supported by the National Aeronautics and Space Administration (NASA) under Grant No. 80NSSC22K1742 issued through the Open Source Tools, Frameworks, and Libraries 2020 Program.</p> <p>This project is also supported by Amazon Web Services (AWS). In addition, this package was made possible by the following open source projects. Credit goes to the developers of these projects.</p> <ul> <li>segment-anything \ud83d\udcbb</li> <li>segment-anything-eo \ud83d\udef0\ufe0f</li> <li>tms2geotiff \ud83d\udcf7</li> <li>GroundingDINO \ud83e\udd96</li> <li>lang-segment-anything \ud83d\udcdd</li> </ul>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#v0102-nov-7-2023","title":"v0.10.2 - Nov 7, 2023","text":"<p>What's Changed</p> <ul> <li>Add JOSS paper by @giswqs in #197</li> <li>Add notebook for using Maxar Open Data by @giswqs in #198</li> <li>Add checkpoint to textsam.LangSAM() by @forestbat in #204</li> <li>Add workshop notebook by @giswqs in #209</li> </ul> <p>New Contributors</p> <ul> <li>@forestbat made their first contribution in #204</li> </ul> <p>Full Changelog: v0.10.1...v0.10.2</p>"},{"location":"changelog/#v0101-sep-1-2023","title":"v0.10.1 - Sep 1, 2023","text":"<p>What's Changed</p> <ul> <li>Fix basemap issue by @giswqs in #190</li> </ul> <p>Full Changelog: v0.10.0...v0.10.1)</p>"},{"location":"changelog/#v0100-aug-24-2023","title":"v0.10.0 - Aug 24, 2023","text":""},{"location":"changelog/#whats-changed","title":"What's Changed","text":"<ul> <li>Added fastsam module by @giswqs #167</li> <li>Update optional dependencies by @giswqs in #68</li> <li>Improve contributing guidelines by @giswqs in #169</li> <li>[FIX] Added missing conversions from BGR to RGB by @lbferreira in #171</li> <li>Address JOSS review comments by @giswqs in #175</li> </ul>"},{"location":"changelog/#new-contributors","title":"New Contributors","text":"<ul> <li>@lbferreira made their first contribution in #171</li> </ul>"},{"location":"changelog/#v091-aug-14-2023","title":"v0.9.1 - Aug 14, 2023","text":"<p>New Features</p> <ul> <li>Added support for HQ-SAM (#161)</li> <li>Added HQ-SAM notebooks (#162)</li> </ul>"},{"location":"changelog/#v090-aug-6-2023","title":"v0.9.0 - Aug 6, 2023","text":"<p>New Features</p> <ul> <li>Added support for multiple input boxes (#159)</li> </ul> <p>Improvements</p> <ul> <li>UpdateD groundingdino installation (#147)</li> <li>Updated README (#152)</li> </ul>"},{"location":"changelog/#v085-jul-19-2023","title":"v0.8.5 - Jul 19, 2023","text":"<p>Improvements</p> <ul> <li>Updated installation docs (#146)</li> <li>Updated leafmap and localtileserver to dependencies (#146)</li> <li>Added info about libgl1 dependency install on Linux systems (#141)</li> <li>Fixed save_masks bug without source image (#139)</li> </ul>"},{"location":"changelog/#v084-jul-5-2023","title":"v0.8.4 - Jul 5, 2023","text":"<p>Improvements</p> <ul> <li>Fixed model download bug (#136)</li> <li>Added legal notice (#133)</li> <li>Fixed image source bug for show_anns (#131)</li> <li>Improved exception handling for LangSAM GUI (#130)</li> <li>Added to return pixel coordinates of masks (#129)</li> <li>Added text_sam to docs (#123)</li> <li>Fixed file deletion error on Windows (#122)</li> <li>Fixed mask bug in text_sam/predict when the input is PIL image (#117)</li> </ul>"},{"location":"changelog/#v083-jun-20-2023","title":"v0.8.3 - Jun 20, 2023","text":"<p>New Features</p> <ul> <li>Added support for batch segmentation (#116)</li> <li>Added swimming pools example (#106)</li> </ul> <p>Improvements</p> <ul> <li>Removed 'flag' and 'param' arguments (#112)</li> <li>Used sorted function instead of if statements (#109)</li> </ul>"},{"location":"changelog/#v082-jun-14-2023","title":"v0.8.2 - Jun 14, 2023","text":"<p>New Features</p> <ul> <li>Added regularized option for vector output (#104)</li> <li>Added text prompt GUI (#80)</li> </ul> <p>Improvements</p> <ul> <li>Added more deep learning resources (#90)</li> <li>Use the force_filename parameter with hf_hub_download() (#93)</li> <li>Fixed typo (#94)</li> </ul>"},{"location":"changelog/#v081-may-24-2023","title":"v0.8.1 - May 24, 2023","text":"<p>Improvements</p> <ul> <li>Added huggingface_hub and remove onnx (#87)</li> <li>Added more demos to docs (#82)</li> </ul>"},{"location":"changelog/#v080-may-24-2023","title":"v0.8.0 - May 24, 2023","text":"<p>New Features</p> <ul> <li>Added support for using text prompts with SAM (#73)</li> <li>Added text prompt GUI (#80)</li> </ul> <p>Improvements</p> <ul> <li>Improved text prompt notebook (#79)</li> <li>Fixed notebook typos (#78)</li> <li>Added ArcGIS tutorial to docs (#72)</li> </ul>"},{"location":"changelog/#v070-may-20-2023","title":"v0.7.0 - May 20, 2023","text":"<p>New Features</p> <ul> <li>Added unittest (#58)</li> <li>Added JOSS paper draft (#61)</li> <li>Added ArcGIS notebook example (#63)</li> <li>Added text prompting segmentation (#65)</li> <li>Added support for segmenting non-georeferenced imagery (#66)</li> </ul> <p>Improvements</p> <ul> <li>Added blend option for show_anns method (#59)</li> <li>Updated ArcGIS installation instructions (#68, #70)</li> </ul> <p>Contributors</p> <p>@p-vdp @LucasOsco</p>"},{"location":"changelog/#v062-may-17-2023","title":"v0.6.2 - May 17, 2023","text":"<p>Improvements</p> <ul> <li>Added jupyter-server-proxy to Dockerfile for supporting add_raster (#57)</li> </ul>"},{"location":"changelog/#v061-may-16-2023","title":"v0.6.1 - May 16, 2023","text":"<p>New Features</p> <ul> <li>Added Dockerfile (#51)</li> </ul>"},{"location":"changelog/#v060-may-16-2023","title":"v0.6.0 - May 16, 2023","text":"<p>New Features</p> <ul> <li>Added interactive GUI for creating foreground and background markers (#44)</li> <li>Added support for custom projection bbox (#39)</li> </ul> <p>Improvements</p> <ul> <li>Fixed Colab Marker AwesomeIcon bug (#50)</li> <li>Added info about using SAM with Desktop GIS (#48)</li> <li>Use proper extension in the usage documentation (#43)</li> </ul> <p>Demos</p> <ul> <li>Interactive segmentation with input prompts</li> </ul> <p></p> <ul> <li>Input prompts from existing files</li> </ul> <p></p>"},{"location":"changelog/#v050-may-10-2023","title":"v0.5.0 - May 10, 2023","text":"<p>New Features</p> <ul> <li>Added support for input prompts (#30)</li> </ul> <p>Improvements</p> <ul> <li>Fixed the batch processing bug (#29)</li> </ul> <p>Demos</p> <p></p>"},{"location":"changelog/#v040-may-6-2023","title":"v0.4.0 - May 6, 2023","text":"<p>New Features</p> <ul> <li>Added new methods to <code>SamGeo</code> class, including <code>show_masks</code>, <code>save_masks</code>, <code>show_anns</code>, making it much easier to save segmentation results in GeoTIFF and vector formats.</li> <li>Added new functions to <code>common</code> module, including <code>array_to_image</code>, <code>show_image</code>, <code>download_file</code>, <code>overlay_images</code>, <code>blend_images</code>, and <code>update_package</code></li> <li>Added tow more notebooks, including automatic_mask_generator and satellite-predictor</li> <li>Added <code>SamGeoPredictor</code> class</li> </ul> <p>Improvements</p> <ul> <li>Improved <code>SamGeo.generate()</code> method</li> <li>Improved docstrings and API reference</li> <li>Added demos to docs</li> </ul> <p>Demos</p> <ul> <li>Automatic mask generator</li> </ul> <p></p> <p>Contributors</p> <p>@darrenwiens</p>"},{"location":"changelog/#v030-apr-26-2023","title":"v0.3.0 - Apr 26, 2023","text":"<p>New Features</p> <ul> <li>Added several new functions, including <code>get_basemaps</code>, <code>reproject</code>, <code>tiff_to_shp</code>, and <code>tiff_to_geojson</code></li> <li>Added hundereds of new basemaps through xyzservices</li> </ul> <p>Improvement</p> <ul> <li>Fixed <code>tiff_to_vector</code> crs bug #12</li> <li>Add <code>crs</code> parameter to <code>tms_to_geotiff</code></li> </ul>"},{"location":"changelog/#v020-apr-21-2023","title":"v0.2.0 - Apr 21, 2023","text":"<p>New Features</p> <ul> <li>Added notebook example</li> <li>Added <code>SamGeo.generate</code> method</li> <li>Added <code>SamGeo.tiff_to_vector</code> method</li> </ul>"},{"location":"changelog/#v010-apr-19-2023","title":"v0.1.0 - Apr 19, 2023","text":"<p>New Features</p> <ul> <li>Added <code>SamGeo</code> class</li> <li>Added GitHub Actions</li> <li>Added notebook example</li> </ul>"},{"location":"changelog/#v001-apr-18-2023","title":"v0.0.1 - Apr 18, 2023","text":"<p>Initial release</p>"},{"location":"changelog_update/","title":"Changelog update","text":"In\u00a0[\u00a0]: Copied! <pre>import re\n</pre> import re In\u00a0[\u00a0]: Copied! <pre># Copy the release notes from the GitHub release page\nmarkdown_text = \"\"\"\n## What's Changed\n* Add JOSS paper by @giswqs in https://github.com/opengeos/segment-geospatial/pull/197\n* Add notebook for using Maxar Open Data by @giswqs in https://github.com/opengeos/segment-geospatial/pull/198\n* Add checkpoint to textsam.LangSAM() by @forestbat in https://github.com/opengeos/segment-geospatial/pull/204\n* Add workshop notebook by @giswqs in https://github.com/opengeos/segment-geospatial/pull/209\n\n## New Contributors\n* @forestbat made their first contribution in https://github.com/opengeos/segment-geospatial/pull/204\n\n**Full Changelog**: https://github.com/opengeos/segment-geospatial/compare/v0.10.1...v0.10.2\n\"\"\"\n</pre> # Copy the release notes from the GitHub release page markdown_text = \"\"\" ## What's Changed * Add JOSS paper by @giswqs in https://github.com/opengeos/segment-geospatial/pull/197 * Add notebook for using Maxar Open Data by @giswqs in https://github.com/opengeos/segment-geospatial/pull/198 * Add checkpoint to textsam.LangSAM() by @forestbat in https://github.com/opengeos/segment-geospatial/pull/204 * Add workshop notebook by @giswqs in https://github.com/opengeos/segment-geospatial/pull/209  ## New Contributors * @forestbat made their first contribution in https://github.com/opengeos/segment-geospatial/pull/204  **Full Changelog**: https://github.com/opengeos/segment-geospatial/compare/v0.10.1...v0.10.2 \"\"\" In\u00a0[\u00a0]: Copied! <pre># Regular expression pattern to match the Markdown hyperlinks\npattern = r\"https://github\\.com/opengeos/segment-geospatial/pull/(\\d+)\"\n</pre> # Regular expression pattern to match the Markdown hyperlinks pattern = r\"https://github\\.com/opengeos/segment-geospatial/pull/(\\d+)\" In\u00a0[\u00a0]: Copied! <pre># Function to replace matched URLs with the desired format\ndef replace_url(match):\n    pr_number = match.group(1)\n    return f\"[#{pr_number}](https://github.com/opengeos/segment-geospatial/pull/{pr_number})\"\n</pre> # Function to replace matched URLs with the desired format def replace_url(match):     pr_number = match.group(1)     return f\"[#{pr_number}](https://github.com/opengeos/segment-geospatial/pull/{pr_number})\" In\u00a0[\u00a0]: Copied! <pre># Use re.sub to replace URLs with the desired format\nformatted_text = re.sub(pattern, replace_url, markdown_text)\n</pre> # Use re.sub to replace URLs with the desired format formatted_text = re.sub(pattern, replace_url, markdown_text) In\u00a0[\u00a0]: Copied! <pre>for line in formatted_text.splitlines():\n    if \"Full Changelog\" in line:\n        prefix = line.split(\": \")[0]\n        link = line.split(\": \")[1]\n        version = line.split(\"/\")[-1]\n        formatted_text = (\n            formatted_text.replace(line, f\"{prefix}: [{version}]({link})\")\n            .replace(\"## What's Changed\", \"**What's Changed**\")\n            .replace(\"## New Contributors\", \"**New Contributors**\")\n        )\n</pre> for line in formatted_text.splitlines():     if \"Full Changelog\" in line:         prefix = line.split(\": \")[0]         link = line.split(\": \")[1]         version = line.split(\"/\")[-1]         formatted_text = (             formatted_text.replace(line, f\"{prefix}: [{version}]({link})\")             .replace(\"## What's Changed\", \"**What's Changed**\")             .replace(\"## New Contributors\", \"**New Contributors**\")         ) In\u00a0[\u00a0]: Copied! <pre>with open(\"docs/changelog_update.md\", \"w\") as f:\n    f.write(formatted_text)\n</pre> with open(\"docs/changelog_update.md\", \"w\") as f:     f.write(formatted_text) In\u00a0[\u00a0]: Copied! <pre># Print the formatted text\nprint(formatted_text)\n</pre> # Print the formatted text print(formatted_text) <p>Copy the formatted text and paste it to the CHANGELOG.md file</p>"},{"location":"common/","title":"common module","text":"<p>The source code is adapted from https://github.com/aliaksandr960/segment-anything-eo. Credit to the author Aliaksandr Hancharenka.</p>"},{"location":"common/#samgeo.common.array_to_image","title":"<code>array_to_image(array, output, source=None, dtype=None, compress='deflate', **kwargs)</code>","text":"<p>Save a NumPy array as a GeoTIFF using the projection information from an existing GeoTIFF file.</p> <p>Parameters:</p> Name Type Description Default <code>array</code> <code>np.ndarray</code> <p>The NumPy array to be saved as a GeoTIFF.</p> required <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>source</code> <code>str</code> <p>The path to an existing GeoTIFF file with map projection information. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>np.dtype</code> <p>The data type of the output array. Defaults to None.</p> <code>None</code> <code>compress</code> <code>str</code> <p>The compression method. Can be one of the following: \"deflate\", \"lzw\", \"packbits\", \"jpeg\". Defaults to \"deflate\".</p> <code>'deflate'</code> Source code in <code>samgeo/common.py</code> <pre><code>def array_to_image(\n    array, output, source=None, dtype=None, compress=\"deflate\", **kwargs\n):\n    \"\"\"Save a NumPy array as a GeoTIFF using the projection information from an existing GeoTIFF file.\n\n    Args:\n        array (np.ndarray): The NumPy array to be saved as a GeoTIFF.\n        output (str): The path to the output image.\n        source (str, optional): The path to an existing GeoTIFF file with map projection information. Defaults to None.\n        dtype (np.dtype, optional): The data type of the output array. Defaults to None.\n        compress (str, optional): The compression method. Can be one of the following: \"deflate\", \"lzw\", \"packbits\", \"jpeg\". Defaults to \"deflate\".\n    \"\"\"\n\n    from PIL import Image\n\n    if isinstance(array, str) and os.path.exists(array):\n        array = cv2.imread(array)\n        array = cv2.cvtColor(array, cv2.COLOR_BGR2RGB)\n\n    if output.endswith(\".tif\") and source is not None:\n        with rasterio.open(source) as src:\n            crs = src.crs\n            transform = src.transform\n            if compress is None:\n                compress = src.compression\n\n        # Determine the minimum and maximum values in the array\n\n        min_value = np.min(array)\n        max_value = np.max(array)\n\n        if dtype is None:\n            # Determine the best dtype for the array\n            if min_value &gt;= 0 and max_value &lt;= 1:\n                dtype = np.float32\n            elif min_value &gt;= 0 and max_value &lt;= 255:\n                dtype = np.uint8\n            elif min_value &gt;= -128 and max_value &lt;= 127:\n                dtype = np.int8\n            elif min_value &gt;= 0 and max_value &lt;= 65535:\n                dtype = np.uint16\n            elif min_value &gt;= -32768 and max_value &lt;= 32767:\n                dtype = np.int16\n            else:\n                dtype = np.float64\n\n        # Convert the array to the best dtype\n        array = array.astype(dtype)\n\n        # Define the GeoTIFF metadata\n        if array.ndim == 2:\n            metadata = {\n                \"driver\": \"GTiff\",\n                \"height\": array.shape[0],\n                \"width\": array.shape[1],\n                \"count\": 1,\n                \"dtype\": array.dtype,\n                \"crs\": crs,\n                \"transform\": transform,\n            }\n        elif array.ndim == 3:\n            metadata = {\n                \"driver\": \"GTiff\",\n                \"height\": array.shape[0],\n                \"width\": array.shape[1],\n                \"count\": array.shape[2],\n                \"dtype\": array.dtype,\n                \"crs\": crs,\n                \"transform\": transform,\n            }\n\n        if compress is not None:\n            metadata[\"compress\"] = compress\n        else:\n            raise ValueError(\"Array must be 2D or 3D.\")\n\n        # Create a new GeoTIFF file and write the array to it\n        with rasterio.open(output, \"w\", **metadata) as dst:\n            if array.ndim == 2:\n                dst.write(array, 1)\n            elif array.ndim == 3:\n                for i in range(array.shape[2]):\n                    dst.write(array[:, :, i], i + 1)\n\n    else:\n        img = Image.fromarray(array)\n        img.save(output, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.bbox_to_xy","title":"<code>bbox_to_xy(src_fp, coords, coord_crs='epsg:4326', **kwargs)</code>","text":"<p>Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.     Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright     While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>coords</code> <code>list</code> <p>A list of coordinates in the format of [[minx, miny, maxx, maxy], [minx, miny, maxx, maxy], ...]</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of pixel coordinates in the format of [[minx, maxy, maxx, miny], ...] from top left to bottom right.</p> Source code in <code>samgeo/common.py</code> <pre><code>def bbox_to_xy(\n    src_fp: str, coords: list, coord_crs: str = \"epsg:4326\", **kwargs\n) -&gt; list:\n    \"\"\"Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.\n        Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright\n        While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright\n\n    Args:\n        src_fp (str): The source raster file path.\n        coords (list): A list of coordinates in the format of [[minx, miny, maxx, maxy], [minx, miny, maxx, maxy], ...]\n        coord_crs (str, optional): The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n\n    Returns:\n        list: A list of pixel coordinates in the format of [[minx, maxy, maxx, miny], ...] from top left to bottom right.\n    \"\"\"\n\n    if isinstance(coords, str):\n        gdf = gpd.read_file(coords)\n        coords = gdf.geometry.bounds.values.tolist()\n        if gdf.crs is not None:\n            coord_crs = f\"epsg:{gdf.crs.to_epsg()}\"\n    elif isinstance(coords, np.ndarray):\n        coords = coords.tolist()\n    if isinstance(coords, dict):\n        import json\n\n        geojson = json.dumps(coords)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        coords = gdf.geometry.bounds.values.tolist()\n\n    elif not isinstance(coords, list):\n        raise ValueError(\"coords must be a list of coordinates.\")\n\n    if not isinstance(coords[0], list):\n        coords = [coords]\n\n    new_coords = []\n\n    with rasterio.open(src_fp) as src:\n        width = src.width\n        height = src.height\n\n        for coord in coords:\n            minx, miny, maxx, maxy = coord\n\n            if coord_crs != src.crs:\n                minx, miny = transform_coords(minx, miny, coord_crs, src.crs, **kwargs)\n                maxx, maxy = transform_coords(maxx, maxy, coord_crs, src.crs, **kwargs)\n\n                rows1, cols1 = rasterio.transform.rowcol(\n                    src.transform, minx, miny, **kwargs\n                )\n                rows2, cols2 = rasterio.transform.rowcol(\n                    src.transform, maxx, maxy, **kwargs\n                )\n\n                new_coords.append([cols1, rows1, cols2, rows2])\n\n            else:\n                new_coords.append([minx, miny, maxx, maxy])\n\n    result = []\n\n    for coord in new_coords:\n        minx, miny, maxx, maxy = coord\n\n        if (\n            minx &gt;= 0\n            and miny &gt;= 0\n            and maxx &gt;= 0\n            and maxy &gt;= 0\n            and minx &lt; width\n            and miny &lt; height\n            and maxx &lt; width\n            and maxy &lt; height\n        ):\n            # Note that map bbox coords is [minx, miny, maxx, maxy] from bottomleft to topright\n            # While rasterio bbox coords is [minx, max, maxx, min] from topleft to bottomright\n            result.append([minx, maxy, maxx, miny])\n\n    if len(result) == 0:\n        print(\"No valid pixel coordinates found.\")\n        return None\n    elif len(result) == 1:\n        return result[0]\n    elif len(result) &lt; len(coords):\n        print(\"Some coordinates are out of the image boundary.\")\n\n    return result\n</code></pre>"},{"location":"common/#samgeo.common.blend_images","title":"<code>blend_images(img1, img2, alpha=0.5, output=False, show=True, figsize=(12, 10), axis='off', **kwargs)</code>","text":"<p>Blends two images together using the addWeighted function from the OpenCV library.</p> <p>Parameters:</p> Name Type Description Default <code>img1</code> <code>numpy.ndarray</code> <p>The first input image on top represented as a NumPy array.</p> required <code>img2</code> <code>numpy.ndarray</code> <p>The second input image at the bottom represented as a NumPy array.</p> required <code>alpha</code> <code>float</code> <p>The weighting factor for the first image in the blend. By default, this is set to 0.5.</p> <code>0.5</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to False.</p> <code>False</code> <code>show</code> <code>bool</code> <p>Whether to display the blended image. Defaults to True.</p> <code>True</code> <code>figsize</code> <code>tuple</code> <p>The size of the figure. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>The axis of the figure. Defaults to \"off\".</p> <code>'off'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the cv2.addWeighted() function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>numpy.ndarray</code> <p>The blended image as a NumPy array.</p> Source code in <code>samgeo/common.py</code> <pre><code>def blend_images(\n    img1,\n    img2,\n    alpha=0.5,\n    output=False,\n    show=True,\n    figsize=(12, 10),\n    axis=\"off\",\n    **kwargs,\n):\n    \"\"\"\n    Blends two images together using the addWeighted function from the OpenCV library.\n\n    Args:\n        img1 (numpy.ndarray): The first input image on top represented as a NumPy array.\n        img2 (numpy.ndarray): The second input image at the bottom represented as a NumPy array.\n        alpha (float): The weighting factor for the first image in the blend. By default, this is set to 0.5.\n        output (str, optional): The path to the output image. Defaults to False.\n        show (bool, optional): Whether to display the blended image. Defaults to True.\n        figsize (tuple, optional): The size of the figure. Defaults to (12, 10).\n        axis (str, optional): The axis of the figure. Defaults to \"off\".\n        **kwargs: Additional keyword arguments to pass to the cv2.addWeighted() function.\n\n    Returns:\n        numpy.ndarray: The blended image as a NumPy array.\n    \"\"\"\n    # Resize the images to have the same dimensions\n    if isinstance(img1, str):\n        if img1.startswith(\"http\"):\n            img1 = download_file(img1)\n\n        if not os.path.exists(img1):\n            raise ValueError(f\"Input path {img1} does not exist.\")\n\n        img1 = cv2.imread(img1)\n\n    if isinstance(img2, str):\n        if img2.startswith(\"http\"):\n            img2 = download_file(img2)\n\n        if not os.path.exists(img2):\n            raise ValueError(f\"Input path {img2} does not exist.\")\n\n        img2 = cv2.imread(img2)\n\n    if img1.dtype == np.float32:\n        img1 = (img1 * 255).astype(np.uint8)\n\n    if img2.dtype == np.float32:\n        img2 = (img2 * 255).astype(np.uint8)\n\n    if img1.dtype != img2.dtype:\n        img2 = img2.astype(img1.dtype)\n\n    img1 = cv2.resize(img1, (img2.shape[1], img2.shape[0]))\n\n    # Blend the images using the addWeighted function\n    beta = 1 - alpha\n    blend_img = cv2.addWeighted(img1, alpha, img2, beta, 0, **kwargs)\n\n    if output:\n        array_to_image(blend_img, output, img2)\n\n    if show:\n        plt.figure(figsize=figsize)\n        plt.imshow(blend_img)\n        plt.axis(axis)\n        plt.show()\n    else:\n        return blend_img\n</code></pre>"},{"location":"common/#samgeo.common.boxes_to_vector","title":"<code>boxes_to_vector(coords, src_crs, dst_crs='EPSG:4326', output=None, **kwargs)</code>","text":"<p>Convert a list of bounding box coordinates to vector data.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>list</code> <p>A list of bounding box coordinates in the format [[left, top, right, bottom], [left, top, right, bottom], ...].</p> required <code>src_crs</code> <code>int or str</code> <p>The EPSG code or proj4 string representing the source coordinate reference system (CRS) of the input coordinates.</p> required <code>dst_crs</code> <code>int or str</code> <p>The EPSG code or proj4 string representing the destination CRS to reproject the data (default is \"EPSG:4326\").</p> <code>'EPSG:4326'</code> <code>output</code> <code>str or None</code> <p>The full file path (including the directory and filename without the extension) where the vector data should be saved.                            If None (default), the function returns the GeoDataFrame without saving it to a file.</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to geopandas.GeoDataFrame.to_file() when saving the vector data.</p> <code>{}</code> <p>Returns:</p> Type Description <code>geopandas.GeoDataFrame or None</code> <p>The GeoDataFrame with the converted vector data if output is None, otherwise None if the data is saved to a file.</p> Source code in <code>samgeo/common.py</code> <pre><code>def boxes_to_vector(coords, src_crs, dst_crs=\"EPSG:4326\", output=None, **kwargs):\n    \"\"\"\n    Convert a list of bounding box coordinates to vector data.\n\n    Args:\n        coords (list): A list of bounding box coordinates in the format [[left, top, right, bottom], [left, top, right, bottom], ...].\n        src_crs (int or str): The EPSG code or proj4 string representing the source coordinate reference system (CRS) of the input coordinates.\n        dst_crs (int or str, optional): The EPSG code or proj4 string representing the destination CRS to reproject the data (default is \"EPSG:4326\").\n        output (str or None, optional): The full file path (including the directory and filename without the extension) where the vector data should be saved.\n                                       If None (default), the function returns the GeoDataFrame without saving it to a file.\n        **kwargs: Additional keyword arguments to pass to geopandas.GeoDataFrame.to_file() when saving the vector data.\n\n    Returns:\n        geopandas.GeoDataFrame or None: The GeoDataFrame with the converted vector data if output is None, otherwise None if the data is saved to a file.\n    \"\"\"\n\n    from shapely.geometry import box\n\n    # Create a list of Shapely Polygon objects based on the provided coordinates\n    polygons = [box(*coord) for coord in coords]\n\n    # Create a GeoDataFrame with the Shapely Polygon objects\n    gdf = gpd.GeoDataFrame({\"geometry\": polygons}, crs=src_crs)\n\n    # Reproject the GeoDataFrame to the specified EPSG code\n    gdf_reprojected = gdf.to_crs(dst_crs)\n\n    if output is not None:\n        gdf_reprojected.to_file(output, **kwargs)\n    else:\n        return gdf_reprojected\n</code></pre>"},{"location":"common/#samgeo.common.check_file_path","title":"<code>check_file_path(file_path, make_dirs=True)</code>","text":"<p>Gets the absolute file path.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the file.</p> required <code>make_dirs</code> <code>bool</code> <p>Whether to create the directory if it does not exist. Defaults to True.</p> <code>True</code> <p>Exceptions:</p> Type Description <code>FileNotFoundError</code> <p>If the directory could not be found.</p> <code>TypeError</code> <p>If the input directory path is not a string.</p> <p>Returns:</p> Type Description <code>str</code> <p>The absolute path to the file.</p> Source code in <code>samgeo/common.py</code> <pre><code>def check_file_path(file_path, make_dirs=True):\n    \"\"\"Gets the absolute file path.\n\n    Args:\n        file_path (str): The path to the file.\n        make_dirs (bool, optional): Whether to create the directory if it does not exist. Defaults to True.\n\n    Raises:\n        FileNotFoundError: If the directory could not be found.\n        TypeError: If the input directory path is not a string.\n\n    Returns:\n        str: The absolute path to the file.\n    \"\"\"\n    if isinstance(file_path, str):\n        if file_path.startswith(\"~\"):\n            file_path = os.path.expanduser(file_path)\n        else:\n            file_path = os.path.abspath(file_path)\n\n        file_dir = os.path.dirname(file_path)\n        if not os.path.exists(file_dir) and make_dirs:\n            os.makedirs(file_dir)\n\n        return file_path\n\n    else:\n        raise TypeError(\"The provided file path must be a string.\")\n</code></pre>"},{"location":"common/#samgeo.common.coords_to_geojson","title":"<code>coords_to_geojson(coords, output=None)</code>","text":"<p>Convert a list of coordinates (lon, lat) to a GeoJSON string or file.</p> <p>Parameters:</p> Name Type Description Default <code>coords</code> <code>list</code> <p>A list of coordinates (lon, lat).</p> required <code>output</code> <code>str</code> <p>The output file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>A GeoJSON dictionary.</p> Source code in <code>samgeo/common.py</code> <pre><code>def coords_to_geojson(coords, output=None):\n    \"\"\"Convert a list of coordinates (lon, lat) to a GeoJSON string or file.\n\n    Args:\n        coords (list): A list of coordinates (lon, lat).\n        output (str, optional): The output file path. Defaults to None.\n\n    Returns:\n        dict: A GeoJSON dictionary.\n    \"\"\"\n\n    import json\n\n    if len(coords) == 0:\n        return\n    # Create a GeoJSON FeatureCollection object\n    feature_collection = {\"type\": \"FeatureCollection\", \"features\": []}\n\n    # Iterate through the coordinates list and create a GeoJSON Feature object for each coordinate\n    for coord in coords:\n        feature = {\n            \"type\": \"Feature\",\n            \"geometry\": {\"type\": \"Point\", \"coordinates\": coord},\n            \"properties\": {},\n        }\n        feature_collection[\"features\"].append(feature)\n\n    # Convert the FeatureCollection object to a JSON string\n    geojson_str = json.dumps(feature_collection)\n\n    if output is not None:\n        with open(output, \"w\") as f:\n            f.write(geojson_str)\n    else:\n        return geojson_str\n</code></pre>"},{"location":"common/#samgeo.common.coords_to_xy","title":"<code>coords_to_xy(src_fp, coords, coord_crs='epsg:4326', return_out_of_bounds=False, **kwargs)</code>","text":"<p>Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>coords</code> <code>list</code> <p>A list of coordinates in the format of [[x1, y1], [x2, y2], ...]</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>return_out_of_bounds</code> <p>Whether to return out of bounds coordinates. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rasterio.transform.rowcol.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>samgeo/common.py</code> <pre><code>def coords_to_xy(\n    src_fp: str,\n    coords: list,\n    coord_crs: str = \"epsg:4326\",\n    return_out_of_bounds=False,\n    **kwargs,\n) -&gt; list:\n    \"\"\"Converts a list of coordinates to pixel coordinates, i.e., (col, row) coordinates.\n\n    Args:\n        src_fp: The source raster file path.\n        coords: A list of coordinates in the format of [[x1, y1], [x2, y2], ...]\n        coord_crs: The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n        return_out_of_bounds: Whether to return out of bounds coordinates. Defaults to False.\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.rowcol.\n\n    Returns:\n        A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n    out_of_bounds = []\n\n    if isinstance(coords, np.ndarray):\n        coords = coords.tolist()\n\n    xs, ys = zip(*coords)\n    with rasterio.open(src_fp) as src:\n        width = src.width\n        height = src.height\n        if coord_crs != src.crs:\n            xs, ys = transform_coords(xs, ys, coord_crs, src.crs, **kwargs)\n        rows, cols = rasterio.transform.rowcol(src.transform, xs, ys, **kwargs)\n    result = [[col, row] for col, row in zip(cols, rows)]\n\n    output = []\n\n    for i, (x, y) in enumerate(result):\n        if x &gt;= 0 and y &gt;= 0 and x &lt; width and y &lt; height:\n            output.append([x, y])\n        else:\n            out_of_bounds.append(i)\n\n    # output = [\n    #     [x, y] for x, y in result if x &gt;= 0 and y &gt;= 0 and x &lt; width and y &lt; height\n    # ]\n    if len(output) == 0:\n        print(\"No valid pixel coordinates found.\")\n    elif len(output) &lt; len(coords):\n        print(\"Some coordinates are out of the image boundary.\")\n\n    if return_out_of_bounds:\n        return output, out_of_bounds\n    else:\n        return output\n</code></pre>"},{"location":"common/#samgeo.common.download_checkpoint","title":"<code>download_checkpoint(model_type='vit_h', checkpoint_dir=None, hq=False)</code>","text":"<p>Download the SAM model checkpoint.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. Can be one of ['vit_h', 'vit_l', 'vit_b']. Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> <code>checkpoint_dir</code> <code>str</code> <p>The checkpoint_dir directory. Defaults to None, \"~/.cache/torch/hub/checkpoints\".</p> <code>None</code> <code>hq</code> <code>bool</code> <p>Whether to use HQ-SAM model (https://github.com/SysCV/sam-hq). Defaults to False.</p> <code>False</code> Source code in <code>samgeo/common.py</code> <pre><code>def download_checkpoint(model_type=\"vit_h\", checkpoint_dir=None, hq=False):\n    \"\"\"Download the SAM model checkpoint.\n\n    Args:\n        model_type (str, optional): The model type. Can be one of ['vit_h', 'vit_l', 'vit_b'].\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        checkpoint_dir (str, optional): The checkpoint_dir directory. Defaults to None, \"~/.cache/torch/hub/checkpoints\".\n        hq (bool, optional): Whether to use HQ-SAM model (https://github.com/SysCV/sam-hq). Defaults to False.\n    \"\"\"\n\n    if not hq:\n        model_types = {\n            \"vit_h\": {\n                \"name\": \"sam_vit_h_4b8939.pth\",\n                \"url\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n            },\n            \"vit_l\": {\n                \"name\": \"sam_vit_l_0b3195.pth\",\n                \"url\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n            },\n            \"vit_b\": {\n                \"name\": \"sam_vit_b_01ec64.pth\",\n                \"url\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\",\n            },\n        }\n    else:\n        model_types = {\n            \"vit_h\": {\n                \"name\": \"sam_hq_vit_h.pth\",\n                \"url\": [\n                    \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_h.zip\",\n                    \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_h.z01\",\n                ],\n            },\n            \"vit_l\": {\n                \"name\": \"sam_hq_vit_l.pth\",\n                \"url\": \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_l.pth\",\n            },\n            \"vit_b\": {\n                \"name\": \"sam_hq_vit_b.pth\",\n                \"url\": \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_b.pth\",\n            },\n            \"vit_tiny\": {\n                \"name\": \"sam_hq_vit_tiny.pth\",\n                \"url\": \"https://github.com/opengeos/datasets/releases/download/models/sam_hq_vit_tiny.pth\",\n            },\n        }\n\n    if model_type not in model_types:\n        raise ValueError(\n            f\"Invalid model_type: {model_type}. It must be one of {', '.join(model_types)}\"\n        )\n\n    if checkpoint_dir is None:\n        checkpoint_dir = os.environ.get(\n            \"TORCH_HOME\", os.path.expanduser(\"~/.cache/torch/hub/checkpoints\")\n        )\n\n    checkpoint = os.path.join(checkpoint_dir, model_types[model_type][\"name\"])\n    if not os.path.exists(checkpoint):\n        print(f\"Model checkpoint for {model_type} not found.\")\n        url = model_types[model_type][\"url\"]\n        if isinstance(url, str):\n            download_file(url, checkpoint)\n        elif isinstance(url, list):\n            download_files(url, checkpoint_dir, multi_part=True)\n    return checkpoint\n</code></pre>"},{"location":"common/#samgeo.common.download_checkpoint_legacy","title":"<code>download_checkpoint_legacy(url=None, output=None, overwrite=False, **kwargs)</code>","text":"<p>Download a checkpoint from URL. It can be one of the following: sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The checkpoint URL. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>The output file path. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the file if it already exists. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The output file path.</p> Source code in <code>samgeo/common.py</code> <pre><code>def download_checkpoint_legacy(url=None, output=None, overwrite=False, **kwargs):\n    \"\"\"Download a checkpoint from URL. It can be one of the following: sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n\n    Args:\n        url (str, optional): The checkpoint URL. Defaults to None.\n        output (str, optional): The output file path. Defaults to None.\n        overwrite (bool, optional): Overwrite the file if it already exists. Defaults to False.\n\n    Returns:\n        str: The output file path.\n    \"\"\"\n    checkpoints = {\n        \"sam_vit_h_4b8939.pth\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_h_4b8939.pth\",\n        \"sam_vit_l_0b3195.pth\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_l_0b3195.pth\",\n        \"sam_vit_b_01ec64.pth\": \"https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\",\n    }\n\n    if isinstance(url, str) and url in checkpoints:\n        url = checkpoints[url]\n\n    if url is None:\n        url = checkpoints[\"sam_vit_h_4b8939.pth\"]\n\n    if output is None:\n        output = os.path.basename(url)\n\n    return download_file(url, output, overwrite=overwrite, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.download_file","title":"<code>download_file(url=None, output=None, quiet=False, proxy=None, speed=None, use_cookies=True, verify=True, id=None, fuzzy=False, resume=False, unzip=True, overwrite=False, subfolder=False)</code>","text":"<p>Download a file from URL, including Google Drive shared URL.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>Google Drive URL is also supported. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>Output filename. Default is basename of URL.</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>Suppress terminal output. Default is False.</p> <code>False</code> <code>proxy</code> <code>str</code> <p>Proxy. Defaults to None.</p> <code>None</code> <code>speed</code> <code>float</code> <p>Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.</p> <code>None</code> <code>use_cookies</code> <code>bool</code> <p>Flag to use cookies. Defaults to True.</p> <code>True</code> <code>verify</code> <code>bool | str</code> <p>Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string, in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.</p> <code>True</code> <code>id</code> <code>str</code> <p>Google Drive's file ID. Defaults to None.</p> <code>None</code> <code>fuzzy</code> <code>bool</code> <p>Fuzzy extraction of Google Drive's file Id. Defaults to False.</p> <code>False</code> <code>resume</code> <code>bool</code> <p>Resume the download from existing tmp file if possible. Defaults to False.</p> <code>False</code> <code>unzip</code> <code>bool</code> <p>Unzip the file. Defaults to True.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the file if it already exists. Defaults to False.</p> <code>False</code> <code>subfolder</code> <code>bool</code> <p>Create a subfolder with the same name as the file. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>str</code> <p>The output file path.</p> Source code in <code>samgeo/common.py</code> <pre><code>def download_file(\n    url=None,\n    output=None,\n    quiet=False,\n    proxy=None,\n    speed=None,\n    use_cookies=True,\n    verify=True,\n    id=None,\n    fuzzy=False,\n    resume=False,\n    unzip=True,\n    overwrite=False,\n    subfolder=False,\n):\n    \"\"\"Download a file from URL, including Google Drive shared URL.\n\n    Args:\n        url (str, optional): Google Drive URL is also supported. Defaults to None.\n        output (str, optional): Output filename. Default is basename of URL.\n        quiet (bool, optional): Suppress terminal output. Default is False.\n        proxy (str, optional): Proxy. Defaults to None.\n        speed (float, optional): Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.\n        use_cookies (bool, optional): Flag to use cookies. Defaults to True.\n        verify (bool | str, optional): Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string,\n            in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.\n        id (str, optional): Google Drive's file ID. Defaults to None.\n        fuzzy (bool, optional): Fuzzy extraction of Google Drive's file Id. Defaults to False.\n        resume (bool, optional): Resume the download from existing tmp file if possible. Defaults to False.\n        unzip (bool, optional): Unzip the file. Defaults to True.\n        overwrite (bool, optional): Overwrite the file if it already exists. Defaults to False.\n        subfolder (bool, optional): Create a subfolder with the same name as the file. Defaults to False.\n\n    Returns:\n        str: The output file path.\n    \"\"\"\n    import zipfile\n\n    try:\n        import gdown\n    except ImportError:\n        print(\n            \"The gdown package is required for this function. Use `pip install gdown` to install it.\"\n        )\n        return\n\n    if output is None:\n        if isinstance(url, str) and url.startswith(\"http\"):\n            output = os.path.basename(url)\n\n    out_dir = os.path.abspath(os.path.dirname(output))\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    if isinstance(url, str):\n        if os.path.exists(os.path.abspath(output)) and (not overwrite):\n            print(\n                f\"{output} already exists. Skip downloading. Set overwrite=True to overwrite.\"\n            )\n            return os.path.abspath(output)\n        else:\n            url = github_raw_url(url)\n\n    if \"https://drive.google.com/file/d/\" in url:\n        fuzzy = True\n\n    output = gdown.download(\n        url, output, quiet, proxy, speed, use_cookies, verify, id, fuzzy, resume\n    )\n\n    if unzip and output.endswith(\".zip\"):\n        with zipfile.ZipFile(output, \"r\") as zip_ref:\n            if not quiet:\n                print(\"Extracting files...\")\n            if subfolder:\n                basename = os.path.splitext(os.path.basename(output))[0]\n\n                output = os.path.join(out_dir, basename)\n                if not os.path.exists(output):\n                    os.makedirs(output)\n                zip_ref.extractall(output)\n            else:\n                zip_ref.extractall(os.path.dirname(output))\n\n    return os.path.abspath(output)\n</code></pre>"},{"location":"common/#samgeo.common.download_files","title":"<code>download_files(urls, out_dir=None, filenames=None, quiet=False, proxy=None, speed=None, use_cookies=True, verify=True, id=None, fuzzy=False, resume=False, unzip=True, overwrite=False, subfolder=False, multi_part=False)</code>","text":"<p>Download files from URLs, including Google Drive shared URL.</p> <p>Parameters:</p> Name Type Description Default <code>urls</code> <code>list</code> <p>The list of urls to download. Google Drive URL is also supported.</p> required <code>out_dir</code> <code>str</code> <p>The output directory. Defaults to None.</p> <code>None</code> <code>filenames</code> <code>list</code> <p>Output filename. Default is basename of URL.</p> <code>None</code> <code>quiet</code> <code>bool</code> <p>Suppress terminal output. Default is False.</p> <code>False</code> <code>proxy</code> <code>str</code> <p>Proxy. Defaults to None.</p> <code>None</code> <code>speed</code> <code>float</code> <p>Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.</p> <code>None</code> <code>use_cookies</code> <code>bool</code> <p>Flag to use cookies. Defaults to True.</p> <code>True</code> <code>verify</code> <code>bool | str</code> <p>Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string, in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.</p> <code>True</code> <code>id</code> <code>str</code> <p>Google Drive's file ID. Defaults to None.</p> <code>None</code> <code>fuzzy</code> <code>bool</code> <p>Fuzzy extraction of Google Drive's file Id. Defaults to False.</p> <code>False</code> <code>resume</code> <code>bool</code> <p>Resume the download from existing tmp file if possible. Defaults to False.</p> <code>False</code> <code>unzip</code> <code>bool</code> <p>Unzip the file. Defaults to True.</p> <code>True</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the file if it already exists. Defaults to False.</p> <code>False</code> <code>subfolder</code> <code>bool</code> <p>Create a subfolder with the same name as the file. Defaults to False.</p> <code>False</code> <code>multi_part</code> <code>bool</code> <p>If the file is a multi-part file. Defaults to False.</p> <code>False</code> <p>Examples:</p> <p>files = [\"sam_hq_vit_tiny.zip\", \"sam_hq_vit_tiny.z01\", \"sam_hq_vit_tiny.z02\", \"sam_hq_vit_tiny.z03\"] base_url = \"https://github.com/opengeos/datasets/releases/download/models/\" urls = [base_url + f for f in files] leafmap.download_files(urls, out_dir=\"models\", multi_part=True)</p> Source code in <code>samgeo/common.py</code> <pre><code>def download_files(\n    urls,\n    out_dir=None,\n    filenames=None,\n    quiet=False,\n    proxy=None,\n    speed=None,\n    use_cookies=True,\n    verify=True,\n    id=None,\n    fuzzy=False,\n    resume=False,\n    unzip=True,\n    overwrite=False,\n    subfolder=False,\n    multi_part=False,\n):\n    \"\"\"Download files from URLs, including Google Drive shared URL.\n\n    Args:\n        urls (list): The list of urls to download. Google Drive URL is also supported.\n        out_dir (str, optional): The output directory. Defaults to None.\n        filenames (list, optional): Output filename. Default is basename of URL.\n        quiet (bool, optional): Suppress terminal output. Default is False.\n        proxy (str, optional): Proxy. Defaults to None.\n        speed (float, optional): Download byte size per second (e.g., 256KB/s = 256 * 1024). Defaults to None.\n        use_cookies (bool, optional): Flag to use cookies. Defaults to True.\n        verify (bool | str, optional): Either a bool, in which case it controls whether the server's TLS certificate is verified, or a string, in which case it must be a path to a CA bundle to use. Default is True.. Defaults to True.\n        id (str, optional): Google Drive's file ID. Defaults to None.\n        fuzzy (bool, optional): Fuzzy extraction of Google Drive's file Id. Defaults to False.\n        resume (bool, optional): Resume the download from existing tmp file if possible. Defaults to False.\n        unzip (bool, optional): Unzip the file. Defaults to True.\n        overwrite (bool, optional): Overwrite the file if it already exists. Defaults to False.\n        subfolder (bool, optional): Create a subfolder with the same name as the file. Defaults to False.\n        multi_part (bool, optional): If the file is a multi-part file. Defaults to False.\n\n    Examples:\n\n        files = [\"sam_hq_vit_tiny.zip\", \"sam_hq_vit_tiny.z01\", \"sam_hq_vit_tiny.z02\", \"sam_hq_vit_tiny.z03\"]\n        base_url = \"https://github.com/opengeos/datasets/releases/download/models/\"\n        urls = [base_url + f for f in files]\n        leafmap.download_files(urls, out_dir=\"models\", multi_part=True)\n    \"\"\"\n\n    if out_dir is None:\n        out_dir = os.getcwd()\n\n    if filenames is None:\n        filenames = [None] * len(urls)\n\n    filepaths = []\n    for url, output in zip(urls, filenames):\n        if output is None:\n            filename = os.path.join(out_dir, os.path.basename(url))\n        else:\n            filename = os.path.join(out_dir, output)\n\n        filepaths.append(filename)\n        if multi_part:\n            unzip = False\n\n        download_file(\n            url,\n            filename,\n            quiet,\n            proxy,\n            speed,\n            use_cookies,\n            verify,\n            id,\n            fuzzy,\n            resume,\n            unzip,\n            overwrite,\n            subfolder,\n        )\n\n    if multi_part:\n        archive = os.path.splitext(filename)[0] + \".zip\"\n        out_dir = os.path.dirname(filename)\n        extract_archive(archive, out_dir)\n\n        for file in filepaths:\n            os.remove(file)\n</code></pre>"},{"location":"common/#samgeo.common.extract_archive","title":"<code>extract_archive(archive, outdir=None, **kwargs)</code>","text":"<p>Extracts a multipart archive.</p> <p>This function uses the patoolib library to extract a multipart archive. If the patoolib library is not installed, it attempts to install it. If the archive does not end with \".zip\", it appends \".zip\" to the archive name. If the extraction fails (for example, if the files already exist), it skips the extraction.</p> <p>Parameters:</p> Name Type Description Default <code>archive</code> <code>str</code> <p>The path to the archive file.</p> required <code>outdir</code> <code>str</code> <p>The directory where the archive should be extracted.</p> <code>None</code> <code>**kwargs</code> <p>Arbitrary keyword arguments for the patoolib.extract_archive function.</p> <code>{}</code> <p>Returns:</p> Type Description <p>None</p> <p>Exceptions:</p> Type Description <code>Exception</code> <p>An exception is raised if the extraction fails for reasons other than the files already existing.</p> <p>Examples:</p> <p>files = [\"sam_hq_vit_tiny.zip\", \"sam_hq_vit_tiny.z01\", \"sam_hq_vit_tiny.z02\", \"sam_hq_vit_tiny.z03\"] base_url = \"https://github.com/opengeos/datasets/releases/download/models/\" urls = [base_url + f for f in files] leafmap.download_files(urls, out_dir=\"models\", multi_part=True)</p> Source code in <code>samgeo/common.py</code> <pre><code>def extract_archive(archive, outdir=None, **kwargs):\n    \"\"\"\n    Extracts a multipart archive.\n\n    This function uses the patoolib library to extract a multipart archive.\n    If the patoolib library is not installed, it attempts to install it.\n    If the archive does not end with \".zip\", it appends \".zip\" to the archive name.\n    If the extraction fails (for example, if the files already exist), it skips the extraction.\n\n    Args:\n        archive (str): The path to the archive file.\n        outdir (str): The directory where the archive should be extracted.\n        **kwargs: Arbitrary keyword arguments for the patoolib.extract_archive function.\n\n    Returns:\n        None\n\n    Raises:\n        Exception: An exception is raised if the extraction fails for reasons other than the files already existing.\n\n    Example:\n\n        files = [\"sam_hq_vit_tiny.zip\", \"sam_hq_vit_tiny.z01\", \"sam_hq_vit_tiny.z02\", \"sam_hq_vit_tiny.z03\"]\n        base_url = \"https://github.com/opengeos/datasets/releases/download/models/\"\n        urls = [base_url + f for f in files]\n        leafmap.download_files(urls, out_dir=\"models\", multi_part=True)\n\n    \"\"\"\n    try:\n        import patoolib\n    except ImportError:\n        install_package(\"patool\")\n        import patoolib\n\n    if not archive.endswith(\".zip\"):\n        archive = archive + \".zip\"\n\n    if outdir is None:\n        outdir = os.path.dirname(archive)\n\n    try:\n        patoolib.extract_archive(archive, outdir=outdir, **kwargs)\n    except Exception as e:\n        print(\"The unzipped files might already exist. Skipping extraction.\")\n        return\n</code></pre>"},{"location":"common/#samgeo.common.geojson_to_coords","title":"<code>geojson_to_coords(geojson, src_crs='epsg:4326', dst_crs='epsg:4326')</code>","text":"<p>Converts a geojson file or a dictionary of feature collection to a list of centroid coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>geojson</code> <code>str | dict</code> <p>The geojson file path or a dictionary of feature collection.</p> required <code>src_crs</code> <code>str</code> <p>The source CRS. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of centroid coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>samgeo/common.py</code> <pre><code>def geojson_to_coords(\n    geojson: str, src_crs: str = \"epsg:4326\", dst_crs: str = \"epsg:4326\"\n) -&gt; list:\n    \"\"\"Converts a geojson file or a dictionary of feature collection to a list of centroid coordinates.\n\n    Args:\n        geojson (str | dict): The geojson file path or a dictionary of feature collection.\n        src_crs (str, optional): The source CRS. Defaults to \"epsg:4326\".\n        dst_crs (str, optional): The destination CRS. Defaults to \"epsg:4326\".\n\n    Returns:\n        list: A list of centroid coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n\n    import json\n    import warnings\n\n    warnings.filterwarnings(\"ignore\")\n\n    if isinstance(geojson, dict):\n        geojson = json.dumps(geojson)\n    gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n    centroids = gdf.geometry.centroid\n    centroid_list = [[point.x, point.y] for point in centroids]\n    if src_crs != dst_crs:\n        centroid_list = transform_coords(\n            [x[0] for x in centroid_list],\n            [x[1] for x in centroid_list],\n            src_crs,\n            dst_crs,\n        )\n        centroid_list = [[x, y] for x, y in zip(centroid_list[0], centroid_list[1])]\n    return centroid_list\n</code></pre>"},{"location":"common/#samgeo.common.geojson_to_xy","title":"<code>geojson_to_xy(src_fp, geojson, coord_crs='epsg:4326', **kwargs)</code>","text":"<p>Converts a geojson file or a dictionary of feature collection to a list of pixel coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>geojson</code> <code>str</code> <p>The geojson file path or a dictionary of feature collection.</p> required <code>coord_crs</code> <code>str</code> <p>The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".</p> <code>'epsg:4326'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rasterio.transform.rowcol.</p> <code>{}</code> <p>Returns:</p> Type Description <code>list</code> <p>A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]</p> Source code in <code>samgeo/common.py</code> <pre><code>def geojson_to_xy(\n    src_fp: str, geojson: str, coord_crs: str = \"epsg:4326\", **kwargs\n) -&gt; list:\n    \"\"\"Converts a geojson file or a dictionary of feature collection to a list of pixel coordinates.\n\n    Args:\n        src_fp: The source raster file path.\n        geojson: The geojson file path or a dictionary of feature collection.\n        coord_crs: The coordinate CRS of the input coordinates. Defaults to \"epsg:4326\".\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.rowcol.\n\n    Returns:\n        A list of pixel coordinates in the format of [[x1, y1], [x2, y2], ...]\n    \"\"\"\n    with rasterio.open(src_fp) as src:\n        src_crs = src.crs\n    coords = geojson_to_coords(geojson, coord_crs, src_crs)\n    return coords_to_xy(src_fp, coords, src_crs, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.get_basemaps","title":"<code>get_basemaps(free_only=True)</code>","text":"<p>Returns a dictionary of xyz basemaps.</p> <p>Parameters:</p> Name Type Description Default <code>free_only</code> <code>bool</code> <p>Whether to return only free xyz tile services that do not require an access token. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of xyz basemaps.</p> Source code in <code>samgeo/common.py</code> <pre><code>def get_basemaps(free_only=True):\n    \"\"\"Returns a dictionary of xyz basemaps.\n\n    Args:\n        free_only (bool, optional): Whether to return only free xyz tile services that do not require an access token. Defaults to True.\n\n    Returns:\n        dict: A dictionary of xyz basemaps.\n    \"\"\"\n\n    basemaps = {}\n    xyz_dict = get_xyz_dict(free_only=free_only)\n    for item in xyz_dict:\n        name = xyz_dict[item].name\n        url = xyz_dict[item].build_url()\n        basemaps[name] = url\n\n    return basemaps\n</code></pre>"},{"location":"common/#samgeo.common.get_vector_crs","title":"<code>get_vector_crs(filename, **kwargs)</code>","text":"<p>Gets the CRS of a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The vector file path.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The CRS of the vector file.</p> Source code in <code>samgeo/common.py</code> <pre><code>def get_vector_crs(filename, **kwargs):\n    \"\"\"Gets the CRS of a vector file.\n\n    Args:\n        filename (str): The vector file path.\n\n    Returns:\n        str: The CRS of the vector file.\n    \"\"\"\n    gdf = gpd.read_file(filename, **kwargs)\n    epsg = gdf.crs.to_epsg()\n    if epsg is None:\n        return gdf.crs\n    else:\n        return f\"EPSG:{epsg}\"\n</code></pre>"},{"location":"common/#samgeo.common.get_xyz_dict","title":"<code>get_xyz_dict(free_only=True)</code>","text":"<p>Returns a dictionary of xyz services.</p> <p>Parameters:</p> Name Type Description Default <code>free_only</code> <code>bool</code> <p>Whether to return only free xyz tile services that do not require an access token. Defaults to True.</p> <code>True</code> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary of xyz services.</p> Source code in <code>samgeo/common.py</code> <pre><code>def get_xyz_dict(free_only=True):\n    \"\"\"Returns a dictionary of xyz services.\n\n    Args:\n        free_only (bool, optional): Whether to return only free xyz tile services that do not require an access token. Defaults to True.\n\n    Returns:\n        dict: A dictionary of xyz services.\n    \"\"\"\n    import collections\n    import xyzservices.providers as xyz\n\n    def _unpack_sub_parameters(var, param):\n        temp = var\n        for sub_param in param.split(\".\"):\n            temp = getattr(temp, sub_param)\n        return temp\n\n    xyz_dict = {}\n    for item in xyz.values():\n        try:\n            name = item[\"name\"]\n            tile = _unpack_sub_parameters(xyz, name)\n            if _unpack_sub_parameters(xyz, name).requires_token():\n                if free_only:\n                    pass\n                else:\n                    xyz_dict[name] = tile\n            else:\n                xyz_dict[name] = tile\n\n        except Exception:\n            for sub_item in item:\n                name = item[sub_item][\"name\"]\n                tile = _unpack_sub_parameters(xyz, name)\n                if _unpack_sub_parameters(xyz, name).requires_token():\n                    if free_only:\n                        pass\n                    else:\n                        xyz_dict[name] = tile\n                else:\n                    xyz_dict[name] = tile\n\n    xyz_dict = collections.OrderedDict(sorted(xyz_dict.items()))\n    return xyz_dict\n</code></pre>"},{"location":"common/#samgeo.common.github_raw_url","title":"<code>github_raw_url(url)</code>","text":"<p>Get the raw URL for a GitHub file.</p> <p>Parameters:</p> Name Type Description Default <code>url</code> <code>str</code> <p>The GitHub URL.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The raw URL.</p> Source code in <code>samgeo/common.py</code> <pre><code>def github_raw_url(url):\n    \"\"\"Get the raw URL for a GitHub file.\n\n    Args:\n        url (str): The GitHub URL.\n    Returns:\n        str: The raw URL.\n    \"\"\"\n    if isinstance(url, str) and url.startswith(\"https://github.com/\") and \"blob\" in url:\n        url = url.replace(\"github.com\", \"raw.githubusercontent.com\").replace(\n            \"blob/\", \"\"\n        )\n    return url\n</code></pre>"},{"location":"common/#samgeo.common.image_to_cog","title":"<code>image_to_cog(source, dst_path=None, profile='deflate', **kwargs)</code>","text":"<p>Converts an image to a COG file.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>A dataset path, URL or rasterio.io.DatasetReader object.</p> required <code>dst_path</code> <code>str</code> <p>An output dataset path or or PathLike object. Defaults to None.</p> <code>None</code> <code>profile</code> <code>str</code> <p>COG profile. More at https://cogeotiff.github.io/rio-cogeo/profile. Defaults to \"deflate\".</p> <code>'deflate'</code> <p>Exceptions:</p> Type Description <code>ImportError</code> <p>If rio-cogeo is not installed.</p> <code>FileNotFoundError</code> <p>If the source file could not be found.</p> Source code in <code>samgeo/common.py</code> <pre><code>def image_to_cog(source, dst_path=None, profile=\"deflate\", **kwargs):\n    \"\"\"Converts an image to a COG file.\n\n    Args:\n        source (str): A dataset path, URL or rasterio.io.DatasetReader object.\n        dst_path (str, optional): An output dataset path or or PathLike object. Defaults to None.\n        profile (str, optional): COG profile. More at https://cogeotiff.github.io/rio-cogeo/profile. Defaults to \"deflate\".\n\n    Raises:\n        ImportError: If rio-cogeo is not installed.\n        FileNotFoundError: If the source file could not be found.\n    \"\"\"\n    try:\n        from rio_cogeo.cogeo import cog_translate\n        from rio_cogeo.profiles import cog_profiles\n\n    except ImportError:\n        raise ImportError(\n            \"The rio-cogeo package is not installed. Please install it with `pip install rio-cogeo` or `conda install rio-cogeo -c conda-forge`.\"\n        )\n\n    if not source.startswith(\"http\"):\n        source = check_file_path(source)\n\n        if not os.path.exists(source):\n            raise FileNotFoundError(\"The provided input file could not be found.\")\n\n    if dst_path is None:\n        if not source.startswith(\"http\"):\n            dst_path = os.path.splitext(source)[0] + \"_cog.tif\"\n        else:\n            dst_path = temp_file_path(extension=\".tif\")\n\n    dst_path = check_file_path(dst_path)\n\n    dst_profile = cog_profiles.get(profile)\n    cog_translate(source, dst_path, dst_profile, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.install_package","title":"<code>install_package(package)</code>","text":"<p>Install a Python package.</p> <p>Parameters:</p> Name Type Description Default <code>package</code> <code>str | list</code> <p>The package name or a GitHub URL or a list of package names or GitHub URLs.</p> required Source code in <code>samgeo/common.py</code> <pre><code>def install_package(package):\n    \"\"\"Install a Python package.\n\n    Args:\n        package (str | list): The package name or a GitHub URL or a list of package names or GitHub URLs.\n    \"\"\"\n    import subprocess\n\n    if isinstance(package, str):\n        packages = [package]\n\n    for package in packages:\n        if package.startswith(\"https://github.com\"):\n            package = f\"git+{package}\"\n\n        # Execute pip install command and show output in real-time\n        command = f\"pip install {package}\"\n        process = subprocess.Popen(command.split(), stdout=subprocess.PIPE)\n\n        # Print output in real-time\n        while True:\n            output = process.stdout.readline()\n            if output == b\"\" and process.poll() is not None:\n                break\n            if output:\n                print(output.decode(\"utf-8\").strip())\n\n        # Wait for process to complete\n        process.wait()\n</code></pre>"},{"location":"common/#samgeo.common.is_colab","title":"<code>is_colab()</code>","text":"<p>Tests if the code is being executed within Google Colab.</p> Source code in <code>samgeo/common.py</code> <pre><code>def is_colab():\n    \"\"\"Tests if the code is being executed within Google Colab.\"\"\"\n    import sys\n\n    if \"google.colab\" in sys.modules:\n        return True\n    else:\n        return False\n</code></pre>"},{"location":"common/#samgeo.common.merge_rasters","title":"<code>merge_rasters(input_dir, output, input_pattern='*.tif', output_format='GTiff', output_nodata=None, output_options=['COMPRESS=DEFLATE'])</code>","text":"<p>Merge a directory of rasters into a single raster.</p> <p>Parameters:</p> Name Type Description Default <code>input_dir</code> <code>str</code> <p>The path to the input directory.</p> required <code>output</code> <code>str</code> <p>The path to the output raster.</p> required <code>input_pattern</code> <code>str</code> <p>The pattern to match the input files. Defaults to \"*.tif\".</p> <code>'*.tif'</code> <code>output_format</code> <code>str</code> <p>The output format. Defaults to \"GTiff\".</p> <code>'GTiff'</code> <code>output_nodata</code> <code>float</code> <p>The output nodata value. Defaults to None.</p> <code>None</code> <code>output_options</code> <code>list</code> <p>A list of output options. Defaults to [\"COMPRESS=DEFLATE\"].</p> <code>['COMPRESS=DEFLATE']</code> <p>Exceptions:</p> Type Description <code>ImportError</code> <p>Raised if GDAL is not installed.</p> Source code in <code>samgeo/common.py</code> <pre><code>def merge_rasters(\n    input_dir,\n    output,\n    input_pattern=\"*.tif\",\n    output_format=\"GTiff\",\n    output_nodata=None,\n    output_options=[\"COMPRESS=DEFLATE\"],\n):\n    \"\"\"Merge a directory of rasters into a single raster.\n\n    Args:\n        input_dir (str): The path to the input directory.\n        output (str): The path to the output raster.\n        input_pattern (str, optional): The pattern to match the input files. Defaults to \"*.tif\".\n        output_format (str, optional): The output format. Defaults to \"GTiff\".\n        output_nodata (float, optional): The output nodata value. Defaults to None.\n        output_options (list, optional): A list of output options. Defaults to [\"COMPRESS=DEFLATE\"].\n\n    Raises:\n        ImportError: Raised if GDAL is not installed.\n    \"\"\"\n\n    import glob\n\n    try:\n        from osgeo import gdal\n    except ImportError:\n        raise ImportError(\n            \"GDAL is required to use this function. Install it with `conda install gdal -c conda-forge`\"\n        )\n    # Get a list of all the input files\n    input_files = glob.glob(os.path.join(input_dir, input_pattern))\n\n    # Merge the input files into a single output file\n    gdal.Warp(\n        output,\n        input_files,\n        format=output_format,\n        dstNodata=output_nodata,\n        options=output_options,\n    )\n</code></pre>"},{"location":"common/#samgeo.common.overlay_images","title":"<code>overlay_images(image1, image2, alpha=0.5, backend='TkAgg', height_ratios=[10, 1], show_args1={}, show_args2={})</code>","text":"<p>Overlays two images using a slider to control the opacity of the top image.</p> <p>Parameters:</p> Name Type Description Default <code>image1</code> <code>str | np.ndarray</code> <p>The first input image at the bottom represented as a NumPy array or the path to the image.</p> required <code>image2</code> <code>_type_</code> <p>The second input image on top represented as a NumPy array or the path to the image.</p> required <code>alpha</code> <code>float</code> <p>The alpha value of the top image. Defaults to 0.5.</p> <code>0.5</code> <code>backend</code> <code>str</code> <p>The backend of the matplotlib plot. Defaults to \"TkAgg\".</p> <code>'TkAgg'</code> <code>height_ratios</code> <code>list</code> <p>The height ratios of the two subplots. Defaults to [10, 1].</p> <code>[10, 1]</code> <code>show_args1</code> <code>dict</code> <p>The keyword arguments to pass to the imshow() function for the first image. Defaults to {}.</p> <code>{}</code> <code>show_args2</code> <code>dict</code> <p>The keyword arguments to pass to the imshow() function for the second image. Defaults to {}.</p> <code>{}</code> Source code in <code>samgeo/common.py</code> <pre><code>def overlay_images(\n    image1,\n    image2,\n    alpha=0.5,\n    backend=\"TkAgg\",\n    height_ratios=[10, 1],\n    show_args1={},\n    show_args2={},\n):\n    \"\"\"Overlays two images using a slider to control the opacity of the top image.\n\n    Args:\n        image1 (str | np.ndarray): The first input image at the bottom represented as a NumPy array or the path to the image.\n        image2 (_type_): The second input image on top represented as a NumPy array or the path to the image.\n        alpha (float, optional): The alpha value of the top image. Defaults to 0.5.\n        backend (str, optional): The backend of the matplotlib plot. Defaults to \"TkAgg\".\n        height_ratios (list, optional): The height ratios of the two subplots. Defaults to [10, 1].\n        show_args1 (dict, optional): The keyword arguments to pass to the imshow() function for the first image. Defaults to {}.\n        show_args2 (dict, optional): The keyword arguments to pass to the imshow() function for the second image. Defaults to {}.\n\n    \"\"\"\n    import sys\n    import matplotlib\n    import matplotlib.widgets as mpwidgets\n\n    if \"google.colab\" in sys.modules:\n        backend = \"inline\"\n        print(\n            \"The TkAgg backend is not supported in Google Colab. The overlay_images function will not work on Colab.\"\n        )\n        return\n\n    matplotlib.use(backend)\n\n    if isinstance(image1, str):\n        if image1.startswith(\"http\"):\n            image1 = download_file(image1)\n\n        if not os.path.exists(image1):\n            raise ValueError(f\"Input path {image1} does not exist.\")\n\n    if isinstance(image2, str):\n        if image2.startswith(\"http\"):\n            image2 = download_file(image2)\n\n        if not os.path.exists(image2):\n            raise ValueError(f\"Input path {image2} does not exist.\")\n\n    # Load the two images\n    x = plt.imread(image1)\n    y = plt.imread(image2)\n\n    # Create the plot\n    fig, (ax0, ax1) = plt.subplots(2, 1, gridspec_kw={\"height_ratios\": height_ratios})\n    img0 = ax0.imshow(x, **show_args1)\n    img1 = ax0.imshow(y, alpha=alpha, **show_args2)\n\n    # Define the update function\n    def update(value):\n        img1.set_alpha(value)\n        fig.canvas.draw_idle()\n\n    # Create the slider\n    slider0 = mpwidgets.Slider(ax=ax1, label=\"alpha\", valmin=0, valmax=1, valinit=alpha)\n    slider0.on_changed(update)\n\n    # Display the plot\n    plt.show()\n</code></pre>"},{"location":"common/#samgeo.common.random_string","title":"<code>random_string(string_length=6)</code>","text":"<p>Generates a random string of fixed length.</p> <p>Parameters:</p> Name Type Description Default <code>string_length</code> <code>int</code> <p>Fixed length. Defaults to 3.</p> <code>6</code> <p>Returns:</p> Type Description <code>str</code> <p>A random string</p> Source code in <code>samgeo/common.py</code> <pre><code>def random_string(string_length=6):\n    \"\"\"Generates a random string of fixed length.\n\n    Args:\n        string_length (int, optional): Fixed length. Defaults to 3.\n\n    Returns:\n        str: A random string\n    \"\"\"\n    import random\n    import string\n\n    # random.seed(1001)\n    letters = string.ascii_lowercase\n    return \"\".join(random.choice(letters) for i in range(string_length))\n</code></pre>"},{"location":"common/#samgeo.common.raster_to_geojson","title":"<code>raster_to_geojson(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a GeoJSON file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the GeoJSON file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def raster_to_geojson(tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a GeoJSON file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the GeoJSON file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    if not output.endswith(\".geojson\"):\n        output += \".geojson\"\n\n    raster_to_vector(tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.raster_to_gpkg","title":"<code>raster_to_gpkg(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the gpkg file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def raster_to_gpkg(tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the gpkg file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    if not output.endswith(\".gpkg\"):\n        output += \".gpkg\"\n\n    raster_to_vector(tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.raster_to_shp","title":"<code>raster_to_shp(tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the shapefile.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def raster_to_shp(tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a shapefile.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the shapefile.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    if not output.endswith(\".shp\"):\n        output += \".shp\"\n\n    raster_to_vector(tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.raster_to_vector","title":"<code>raster_to_vector(source, output, simplify_tolerance=None, dst_crs=None, **kwargs)</code>","text":"<p>Vectorize a raster dataset.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def raster_to_vector(source, output, simplify_tolerance=None, dst_crs=None, **kwargs):\n    \"\"\"Vectorize a raster dataset.\n\n    Args:\n        source (str): The path to the tiff file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n    from rasterio import features\n\n    with rasterio.open(source) as src:\n        band = src.read()\n\n        mask = band != 0\n        shapes = features.shapes(band, mask=mask, transform=src.transform)\n\n    fc = [\n        {\"geometry\": shapely.geometry.shape(shape), \"properties\": {\"value\": value}}\n        for shape, value in shapes\n    ]\n    if simplify_tolerance is not None:\n        for i in fc:\n            i[\"geometry\"] = i[\"geometry\"].simplify(tolerance=simplify_tolerance)\n\n    gdf = gpd.GeoDataFrame.from_features(fc)\n    if src.crs is not None:\n        gdf.set_crs(crs=src.crs, inplace=True)\n\n    if dst_crs is not None:\n        gdf = gdf.to_crs(dst_crs)\n\n    gdf.to_file(output, **kwargs)\n</code></pre>"},{"location":"common/#samgeo.common.regularize","title":"<code>regularize(source, output=None, crs='EPSG:4326', **kwargs)</code>","text":"<p>Regularize a polygon GeoDataFrame.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | gpd.GeoDataFrame</code> <p>The input file path or a GeoDataFrame.</p> required <code>output</code> <code>str</code> <p>The output file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>gpd.GeoDataFrame</code> <p>The output GeoDataFrame.</p> Source code in <code>samgeo/common.py</code> <pre><code>def regularize(source, output=None, crs=\"EPSG:4326\", **kwargs):\n    \"\"\"Regularize a polygon GeoDataFrame.\n\n    Args:\n        source (str | gpd.GeoDataFrame): The input file path or a GeoDataFrame.\n        output (str, optional): The output file path. Defaults to None.\n\n\n    Returns:\n        gpd.GeoDataFrame: The output GeoDataFrame.\n    \"\"\"\n    if isinstance(source, str):\n        gdf = gpd.read_file(source)\n    elif isinstance(source, gpd.GeoDataFrame):\n        gdf = source\n    else:\n        raise ValueError(\"The input source must be a GeoDataFrame or a file path.\")\n\n    polygons = gdf.geometry.apply(lambda geom: geom.minimum_rotated_rectangle)\n    result = gpd.GeoDataFrame(geometry=polygons, data=gdf.drop(\"geometry\", axis=1))\n\n    if crs is not None:\n        result.to_crs(crs, inplace=True)\n    if output is not None:\n        result.to_file(output, **kwargs)\n    else:\n        return result\n</code></pre>"},{"location":"common/#samgeo.common.reproject","title":"<code>reproject(image, output, dst_crs='EPSG:4326', resampling='nearest', to_cog=True, **kwargs)</code>","text":"<p>Reprojects an image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The input image filepath.</p> required <code>output</code> <code>str</code> <p>The output image filepath.</p> required <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>resampling</code> <code>Resampling</code> <p>The resampling method. Defaults to \"nearest\".</p> <code>'nearest'</code> <code>to_cog</code> <code>bool</code> <p>Whether to convert the output image to a Cloud Optimized GeoTIFF. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rasterio.open.</p> <code>{}</code> Source code in <code>samgeo/common.py</code> <pre><code>def reproject(\n    image, output, dst_crs=\"EPSG:4326\", resampling=\"nearest\", to_cog=True, **kwargs\n):\n    \"\"\"Reprojects an image.\n\n    Args:\n        image (str): The input image filepath.\n        output (str): The output image filepath.\n        dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n        resampling (Resampling, optional): The resampling method. Defaults to \"nearest\".\n        to_cog (bool, optional): Whether to convert the output image to a Cloud Optimized GeoTIFF. Defaults to True.\n        **kwargs: Additional keyword arguments to pass to rasterio.open.\n\n    \"\"\"\n    import rasterio as rio\n    from rasterio.warp import calculate_default_transform, reproject, Resampling\n\n    if isinstance(resampling, str):\n        resampling = getattr(Resampling, resampling)\n\n    image = os.path.abspath(image)\n    output = os.path.abspath(output)\n\n    if not os.path.exists(os.path.dirname(output)):\n        os.makedirs(os.path.dirname(output))\n\n    with rio.open(image, **kwargs) as src:\n        transform, width, height = calculate_default_transform(\n            src.crs, dst_crs, src.width, src.height, *src.bounds\n        )\n        kwargs = src.meta.copy()\n        kwargs.update(\n            {\n                \"crs\": dst_crs,\n                \"transform\": transform,\n                \"width\": width,\n                \"height\": height,\n            }\n        )\n\n        with rio.open(output, \"w\", **kwargs) as dst:\n            for i in range(1, src.count + 1):\n                reproject(\n                    source=rio.band(src, i),\n                    destination=rio.band(dst, i),\n                    src_transform=src.transform,\n                    src_crs=src.crs,\n                    dst_transform=transform,\n                    dst_crs=dst_crs,\n                    resampling=resampling,\n                    **kwargs,\n                )\n\n    if to_cog:\n        image_to_cog(output, output)\n</code></pre>"},{"location":"common/#samgeo.common.rowcol_to_xy","title":"<code>rowcol_to_xy(src_fp, rows=None, cols=None, boxes=None, zs=None, offset='center', output=None, dst_crs='EPSG:4326', **kwargs)</code>","text":"<p>Converts a list of (row, col) coordinates to (x, y) coordinates.</p> <p>Parameters:</p> Name Type Description Default <code>src_fp</code> <code>str</code> <p>The source raster file path.</p> required <code>rows</code> <code>list</code> <p>A list of row coordinates. Defaults to None.</p> <code>None</code> <code>cols</code> <code>list</code> <p>A list of col coordinates. Defaults to None.</p> <code>None</code> <code>boxes</code> <code>list</code> <p>A list of (row, col) coordinates in the format of [[left, top, right, bottom], [left, top, right, bottom], ...]</p> <code>None</code> <code>zs</code> <p>zs (list or float, optional): Height associated with coordinates. Primarily used for RPC based coordinate transformations.</p> <code>None</code> <code>offset</code> <code>str</code> <p>Determines if the returned coordinates are for the center of the pixel or for a corner.</p> <code>'center'</code> <code>output</code> <code>str</code> <p>The output vector file path. Defaults to None.</p> <code>None</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to rasterio.transform.xy.</p> <code>{}</code> <p>Returns:</p> Type Description <p>A list of (x, y) coordinates.</p> Source code in <code>samgeo/common.py</code> <pre><code>def rowcol_to_xy(\n    src_fp,\n    rows=None,\n    cols=None,\n    boxes=None,\n    zs=None,\n    offset=\"center\",\n    output=None,\n    dst_crs=\"EPSG:4326\",\n    **kwargs,\n):\n    \"\"\"Converts a list of (row, col) coordinates to (x, y) coordinates.\n\n    Args:\n        src_fp (str): The source raster file path.\n        rows (list, optional): A list of row coordinates. Defaults to None.\n        cols (list, optional): A list of col coordinates. Defaults to None.\n        boxes (list, optional): A list of (row, col) coordinates in the format of [[left, top, right, bottom], [left, top, right, bottom], ...]\n        zs: zs (list or float, optional): Height associated with coordinates. Primarily used for RPC based coordinate transformations.\n        offset (str, optional): Determines if the returned coordinates are for the center of the pixel or for a corner.\n        output (str, optional): The output vector file path. Defaults to None.\n        dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n        **kwargs: Additional keyword arguments to pass to rasterio.transform.xy.\n\n    Returns:\n        A list of (x, y) coordinates.\n    \"\"\"\n\n    if boxes is not None:\n        rows = []\n        cols = []\n\n        for box in boxes:\n            rows.append(box[1])\n            rows.append(box[3])\n            cols.append(box[0])\n            cols.append(box[2])\n\n    if rows is None or cols is None:\n        raise ValueError(\"rows and cols must be provided.\")\n\n    with rasterio.open(src_fp) as src:\n        xs, ys = rasterio.transform.xy(src.transform, rows, cols, zs, offset, **kwargs)\n        src_crs = src.crs\n\n    if boxes is None:\n        return [[x, y] for x, y in zip(xs, ys)]\n    else:\n        result = [[xs[i], ys[i + 1], xs[i + 1], ys[i]] for i in range(0, len(xs), 2)]\n\n        if output is not None:\n            boxes_to_vector(result, src_crs, dst_crs, output)\n        else:\n            return result\n</code></pre>"},{"location":"common/#samgeo.common.sam_map_gui","title":"<code>sam_map_gui(sam, basemap='SATELLITE', repeat_mode=True, out_dir=None, **kwargs)</code>","text":"<p>Display the SAM Map GUI.</p> <p>Parameters:</p> Name Type Description Default <code>sam</code> <code>SamGeo</code> required <code>basemap</code> <code>str</code> <p>The basemap to use. Defaults to \"SATELLITE\".</p> <code>'SATELLITE'</code> <code>repeat_mode</code> <code>bool</code> <p>Whether to use the repeat mode for the draw control. Defaults to True.</p> <code>True</code> <code>out_dir</code> <code>str</code> <p>The output directory. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def sam_map_gui(sam, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n    \"\"\"Display the SAM Map GUI.\n\n    Args:\n        sam (SamGeo):\n        basemap (str, optional): The basemap to use. Defaults to \"SATELLITE\".\n        repeat_mode (bool, optional): Whether to use the repeat mode for the draw control. Defaults to True.\n        out_dir (str, optional): The output directory. Defaults to None.\n\n    \"\"\"\n    try:\n        import shutil\n        import tempfile\n        import leafmap\n        import ipyleaflet\n        import ipyevents\n        import ipywidgets as widgets\n        from ipyfilechooser import FileChooser\n    except ImportError:\n        raise ImportError(\n            \"The sam_map function requires the leafmap package. Please install it first.\"\n        )\n\n    if out_dir is None:\n        out_dir = tempfile.gettempdir()\n\n    m = leafmap.Map(repeat_mode=repeat_mode, **kwargs)\n    m.default_style = {\"cursor\": \"crosshair\"}\n    m.add_basemap(basemap, show=False)\n\n    # Skip the image layer if localtileserver is not available\n    try:\n        m.add_raster(sam.source, layer_name=\"Image\")\n    except:\n        pass\n\n    m.fg_markers = []\n    m.bg_markers = []\n\n    fg_layer = ipyleaflet.LayerGroup(layers=m.fg_markers, name=\"Foreground\")\n    bg_layer = ipyleaflet.LayerGroup(layers=m.bg_markers, name=\"Background\")\n    m.add(fg_layer)\n    m.add(bg_layer)\n    m.fg_layer = fg_layer\n    m.bg_layer = bg_layer\n\n    widget_width = \"280px\"\n    button_width = \"90px\"\n    padding = \"0px 0px 0px 4px\"  # upper, right, bottom, left\n    style = {\"description_width\": \"initial\"}\n\n    toolbar_button = widgets.ToggleButton(\n        value=True,\n        tooltip=\"Toolbar\",\n        icon=\"gear\",\n        layout=widgets.Layout(width=\"28px\", height=\"28px\", padding=padding),\n    )\n\n    close_button = widgets.ToggleButton(\n        value=False,\n        tooltip=\"Close the tool\",\n        icon=\"times\",\n        button_style=\"primary\",\n        layout=widgets.Layout(height=\"28px\", width=\"28px\", padding=padding),\n    )\n\n    plus_button = widgets.ToggleButton(\n        value=False,\n        tooltip=\"Load foreground points\",\n        icon=\"plus-circle\",\n        button_style=\"primary\",\n        layout=widgets.Layout(height=\"28px\", width=\"28px\", padding=padding),\n    )\n\n    minus_button = widgets.ToggleButton(\n        value=False,\n        tooltip=\"Load background points\",\n        icon=\"minus-circle\",\n        button_style=\"primary\",\n        layout=widgets.Layout(height=\"28px\", width=\"28px\", padding=padding),\n    )\n\n    radio_buttons = widgets.RadioButtons(\n        options=[\"Foreground\", \"Background\"],\n        description=\"Class Type:\",\n        disabled=False,\n        style=style,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n    )\n\n    fg_count = widgets.IntText(\n        value=0,\n        description=\"Foreground #:\",\n        disabled=True,\n        style=style,\n        layout=widgets.Layout(width=\"135px\", padding=padding),\n    )\n    bg_count = widgets.IntText(\n        value=0,\n        description=\"Background #:\",\n        disabled=True,\n        style=style,\n        layout=widgets.Layout(width=\"135px\", padding=padding),\n    )\n\n    segment_button = widgets.ToggleButton(\n        description=\"Segment\",\n        value=False,\n        button_style=\"primary\",\n        layout=widgets.Layout(padding=padding),\n    )\n\n    save_button = widgets.ToggleButton(\n        description=\"Save\", value=False, button_style=\"primary\"\n    )\n\n    reset_button = widgets.ToggleButton(\n        description=\"Reset\", value=False, button_style=\"primary\"\n    )\n    segment_button.layout.width = button_width\n    save_button.layout.width = button_width\n    reset_button.layout.width = button_width\n\n    opacity_slider = widgets.FloatSlider(\n        description=\"Mask opacity:\",\n        min=0,\n        max=1,\n        value=0.5,\n        readout=True,\n        continuous_update=True,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n        style=style,\n    )\n\n    rectangular = widgets.Checkbox(\n        value=False,\n        description=\"Regularize\",\n        layout=widgets.Layout(width=\"130px\", padding=padding),\n        style=style,\n    )\n\n    colorpicker = widgets.ColorPicker(\n        concise=False,\n        description=\"Color\",\n        value=\"#ffff00\",\n        layout=widgets.Layout(width=\"140px\", padding=padding),\n        style=style,\n    )\n\n    buttons = widgets.VBox(\n        [\n            radio_buttons,\n            widgets.HBox([fg_count, bg_count]),\n            opacity_slider,\n            widgets.HBox([rectangular, colorpicker]),\n            widgets.HBox(\n                [segment_button, save_button, reset_button],\n                layout=widgets.Layout(padding=\"0px 4px 0px 4px\"),\n            ),\n        ]\n    )\n\n    def opacity_changed(change):\n        if change[\"new\"]:\n            mask_layer = m.find_layer(\"Masks\")\n            if mask_layer is not None:\n                mask_layer.interact(opacity=opacity_slider.value)\n\n    opacity_slider.observe(opacity_changed, \"value\")\n\n    output = widgets.Output(\n        layout=widgets.Layout(\n            width=widget_width, padding=padding, max_width=widget_width\n        )\n    )\n\n    toolbar_header = widgets.HBox()\n    toolbar_header.children = [close_button, plus_button, minus_button, toolbar_button]\n    toolbar_footer = widgets.VBox()\n    toolbar_footer.children = [\n        buttons,\n        output,\n    ]\n    toolbar_widget = widgets.VBox()\n    toolbar_widget.children = [toolbar_header, toolbar_footer]\n\n    toolbar_event = ipyevents.Event(\n        source=toolbar_widget, watched_events=[\"mouseenter\", \"mouseleave\"]\n    )\n\n    def marker_callback(chooser):\n        with output:\n            if chooser.selected is not None:\n                try:\n                    gdf = gpd.read_file(chooser.selected)\n                    centroids = gdf.centroid\n                    coords = [[point.x, point.y] for point in centroids]\n                    for coord in coords:\n                        if plus_button.value:\n                            if is_colab():  # Colab does not support AwesomeIcon\n                                marker = ipyleaflet.CircleMarker(\n                                    location=(coord[1], coord[0]),\n                                    radius=2,\n                                    color=\"green\",\n                                    fill_color=\"green\",\n                                )\n                            else:\n                                marker = ipyleaflet.Marker(\n                                    location=[coord[1], coord[0]],\n                                    icon=ipyleaflet.AwesomeIcon(\n                                        name=\"plus-circle\",\n                                        marker_color=\"green\",\n                                        icon_color=\"darkred\",\n                                    ),\n                                )\n                            m.fg_layer.add(marker)\n                            m.fg_markers.append(marker)\n                            fg_count.value = len(m.fg_markers)\n                        elif minus_button.value:\n                            if is_colab():\n                                marker = ipyleaflet.CircleMarker(\n                                    location=(coord[1], coord[0]),\n                                    radius=2,\n                                    color=\"red\",\n                                    fill_color=\"red\",\n                                )\n                            else:\n                                marker = ipyleaflet.Marker(\n                                    location=[coord[1], coord[0]],\n                                    icon=ipyleaflet.AwesomeIcon(\n                                        name=\"minus-circle\",\n                                        marker_color=\"red\",\n                                        icon_color=\"darkred\",\n                                    ),\n                                )\n                            m.bg_layer.add(marker)\n                            m.bg_markers.append(marker)\n                            bg_count.value = len(m.bg_markers)\n\n                except Exception as e:\n                    print(e)\n\n            if m.marker_control in m.controls:\n                m.remove_control(m.marker_control)\n                delattr(m, \"marker_control\")\n\n            plus_button.value = False\n            minus_button.value = False\n\n    def marker_button_click(change):\n        if change[\"new\"]:\n            sandbox_path = os.environ.get(\"SANDBOX_PATH\")\n            filechooser = FileChooser(\n                path=os.getcwd(),\n                sandbox_path=sandbox_path,\n                layout=widgets.Layout(width=\"454px\"),\n            )\n            filechooser.use_dir_icons = True\n            filechooser.filter_pattern = [\"*.shp\", \"*.geojson\", \"*.gpkg\"]\n            filechooser.register_callback(marker_callback)\n            marker_control = ipyleaflet.WidgetControl(\n                widget=filechooser, position=\"topright\"\n            )\n            m.add_control(marker_control)\n            m.marker_control = marker_control\n        else:\n            if hasattr(m, \"marker_control\") and m.marker_control in m.controls:\n                m.remove_control(m.marker_control)\n                m.marker_control.close()\n\n    plus_button.observe(marker_button_click, \"value\")\n    minus_button.observe(marker_button_click, \"value\")\n\n    def handle_toolbar_event(event):\n        if event[\"type\"] == \"mouseenter\":\n            toolbar_widget.children = [toolbar_header, toolbar_footer]\n        elif event[\"type\"] == \"mouseleave\":\n            if not toolbar_button.value:\n                toolbar_widget.children = [toolbar_button]\n                toolbar_button.value = False\n                close_button.value = False\n\n    toolbar_event.on_dom_event(handle_toolbar_event)\n\n    def toolbar_btn_click(change):\n        if change[\"new\"]:\n            close_button.value = False\n            toolbar_widget.children = [toolbar_header, toolbar_footer]\n        else:\n            if not close_button.value:\n                toolbar_widget.children = [toolbar_button]\n\n    toolbar_button.observe(toolbar_btn_click, \"value\")\n\n    def close_btn_click(change):\n        if change[\"new\"]:\n            toolbar_button.value = False\n            if m.toolbar_control in m.controls:\n                m.remove_control(m.toolbar_control)\n            toolbar_widget.close()\n\n    close_button.observe(close_btn_click, \"value\")\n\n    def handle_map_interaction(**kwargs):\n        try:\n            if kwargs.get(\"type\") == \"click\":\n                latlon = kwargs.get(\"coordinates\")\n                if radio_buttons.value == \"Foreground\":\n                    if is_colab():\n                        marker = ipyleaflet.CircleMarker(\n                            location=tuple(latlon),\n                            radius=2,\n                            color=\"green\",\n                            fill_color=\"green\",\n                        )\n                    else:\n                        marker = ipyleaflet.Marker(\n                            location=latlon,\n                            icon=ipyleaflet.AwesomeIcon(\n                                name=\"plus-circle\",\n                                marker_color=\"green\",\n                                icon_color=\"darkred\",\n                            ),\n                        )\n                    fg_layer.add(marker)\n                    m.fg_markers.append(marker)\n                    fg_count.value = len(m.fg_markers)\n                elif radio_buttons.value == \"Background\":\n                    if is_colab():\n                        marker = ipyleaflet.CircleMarker(\n                            location=tuple(latlon),\n                            radius=2,\n                            color=\"red\",\n                            fill_color=\"red\",\n                        )\n                    else:\n                        marker = ipyleaflet.Marker(\n                            location=latlon,\n                            icon=ipyleaflet.AwesomeIcon(\n                                name=\"minus-circle\",\n                                marker_color=\"red\",\n                                icon_color=\"darkred\",\n                            ),\n                        )\n                    bg_layer.add(marker)\n                    m.bg_markers.append(marker)\n                    bg_count.value = len(m.bg_markers)\n\n        except (TypeError, KeyError) as e:\n            print(f\"Error handling map interaction: {e}\")\n\n    m.on_interaction(handle_map_interaction)\n\n    def segment_button_click(change):\n        if change[\"new\"]:\n            segment_button.value = False\n            with output:\n                output.clear_output()\n                if len(m.fg_markers) == 0:\n                    print(\"Please add some foreground markers.\")\n                    segment_button.value = False\n                    return\n\n                else:\n                    try:\n                        fg_points = [\n                            [marker.location[1], marker.location[0]]\n                            for marker in m.fg_markers\n                        ]\n                        bg_points = [\n                            [marker.location[1], marker.location[0]]\n                            for marker in m.bg_markers\n                        ]\n                        point_coords = fg_points + bg_points\n                        point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n\n                        filename = f\"masks_{random_string()}.tif\"\n                        filename = os.path.join(out_dir, filename)\n                        sam.predict(\n                            point_coords=point_coords,\n                            point_labels=point_labels,\n                            point_crs=\"EPSG:4326\",\n                            output=filename,\n                        )\n                        if m.find_layer(\"Masks\") is not None:\n                            m.remove_layer(m.find_layer(\"Masks\"))\n                        if m.find_layer(\"Regularized\") is not None:\n                            m.remove_layer(m.find_layer(\"Regularized\"))\n\n                        if hasattr(sam, \"prediction_fp\") and os.path.exists(\n                            sam.prediction_fp\n                        ):\n                            try:\n                                os.remove(sam.prediction_fp)\n                            except:\n                                pass\n\n                        # Skip the image layer if localtileserver is not available\n                        try:\n                            m.add_raster(\n                                filename,\n                                nodata=0,\n                                cmap=\"Blues\",\n                                opacity=opacity_slider.value,\n                                layer_name=\"Masks\",\n                                zoom_to_layer=False,\n                            )\n\n                            if rectangular.value:\n                                vector = filename.replace(\".tif\", \".gpkg\")\n                                vector_rec = filename.replace(\".tif\", \"_rect.gpkg\")\n                                raster_to_vector(filename, vector)\n                                regularize(vector, vector_rec)\n                                vector_style = {\"color\": colorpicker.value}\n                                m.add_vector(\n                                    vector_rec,\n                                    layer_name=\"Regularized\",\n                                    style=vector_style,\n                                    info_mode=None,\n                                    zoom_to_layer=False,\n                                )\n\n                        except:\n                            pass\n                        output.clear_output()\n                        segment_button.value = False\n                        sam.prediction_fp = filename\n                    except Exception as e:\n                        segment_button.value = False\n                        print(e)\n\n    segment_button.observe(segment_button_click, \"value\")\n\n    def filechooser_callback(chooser):\n        with output:\n            if chooser.selected is not None:\n                try:\n                    filename = chooser.selected\n                    shutil.copy(sam.prediction_fp, filename)\n                    vector = filename.replace(\".tif\", \".gpkg\")\n                    raster_to_vector(filename, vector)\n                    if rectangular.value:\n                        vector_rec = filename.replace(\".tif\", \"_rect.gpkg\")\n                        regularize(vector, vector_rec)\n\n                    fg_points = [\n                        [marker.location[1], marker.location[0]]\n                        for marker in m.fg_markers\n                    ]\n                    bg_points = [\n                        [marker.location[1], marker.location[0]]\n                        for marker in m.bg_markers\n                    ]\n\n                    coords_to_geojson(\n                        fg_points, filename.replace(\".tif\", \"_fg_markers.geojson\")\n                    )\n                    coords_to_geojson(\n                        bg_points, filename.replace(\".tif\", \"_bg_markers.geojson\")\n                    )\n\n                except Exception as e:\n                    print(e)\n\n                if hasattr(m, \"save_control\") and m.save_control in m.controls:\n                    m.remove_control(m.save_control)\n                    delattr(m, \"save_control\")\n                save_button.value = False\n\n    def save_button_click(change):\n        if change[\"new\"]:\n            with output:\n                sandbox_path = os.environ.get(\"SANDBOX_PATH\")\n                filechooser = FileChooser(\n                    path=os.getcwd(),\n                    filename=\"masks.tif\",\n                    sandbox_path=sandbox_path,\n                    layout=widgets.Layout(width=\"454px\"),\n                )\n                filechooser.use_dir_icons = True\n                filechooser.filter_pattern = [\"*.tif\"]\n                filechooser.register_callback(filechooser_callback)\n                save_control = ipyleaflet.WidgetControl(\n                    widget=filechooser, position=\"topright\"\n                )\n                m.add_control(save_control)\n                m.save_control = save_control\n        else:\n            if hasattr(m, \"save_control\") and m.save_control in m.controls:\n                m.remove_control(m.save_control)\n                delattr(m, \"save_control\")\n\n    save_button.observe(save_button_click, \"value\")\n\n    def reset_button_click(change):\n        if change[\"new\"]:\n            segment_button.value = False\n            reset_button.value = False\n            opacity_slider.value = 0.5\n            rectangular.value = False\n            colorpicker.value = \"#ffff00\"\n            output.clear_output()\n            try:\n                m.remove_layer(m.find_layer(\"Masks\"))\n                if m.find_layer(\"Regularized\") is not None:\n                    m.remove_layer(m.find_layer(\"Regularized\"))\n                m.clear_drawings()\n                if hasattr(m, \"fg_markers\"):\n                    m.user_rois = None\n                    m.fg_markers = []\n                    m.bg_markers = []\n                    m.fg_layer.clear_layers()\n                    m.bg_layer.clear_layers()\n                    fg_count.value = 0\n                    bg_count.value = 0\n                try:\n                    os.remove(sam.prediction_fp)\n                except:\n                    pass\n            except:\n                pass\n\n    reset_button.observe(reset_button_click, \"value\")\n\n    toolbar_control = ipyleaflet.WidgetControl(\n        widget=toolbar_widget, position=\"topright\"\n    )\n    m.add_control(toolbar_control)\n    m.toolbar_control = toolbar_control\n\n    return m\n</code></pre>"},{"location":"common/#samgeo.common.show_canvas","title":"<code>show_canvas(image, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5)</code>","text":"<p>Show a canvas to collect foreground and background points.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str | np.ndarray</code> <p>The input image.</p> required <code>fg_color</code> <code>tuple</code> <p>The color for the foreground points. Defaults to (0, 255, 0).</p> <code>(0, 255, 0)</code> <code>bg_color</code> <code>tuple</code> <p>The color for the background points. Defaults to (0, 0, 255).</p> <code>(0, 0, 255)</code> <code>radius</code> <code>int</code> <p>The radius of the points. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple of two lists of foreground and background points.</p> Source code in <code>samgeo/common.py</code> <pre><code>def show_canvas(image, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n    \"\"\"Show a canvas to collect foreground and background points.\n\n    Args:\n        image (str | np.ndarray): The input image.\n        fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n        bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n        radius (int, optional): The radius of the points. Defaults to 5.\n\n    Returns:\n        tuple: A tuple of two lists of foreground and background points.\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        image = cv2.imread(image)\n    elif isinstance(image, np.ndarray):\n        pass\n    else:\n        raise ValueError(\"Input image must be a URL or a NumPy array.\")\n\n    # Create an empty list to store the mouse click coordinates\n    left_clicks = []\n    right_clicks = []\n\n    # Create a mouse callback function\n    def get_mouse_coordinates(event, x, y):\n        if event == cv2.EVENT_LBUTTONDOWN:\n            # Append the coordinates to the mouse_clicks list\n            left_clicks.append((x, y))\n\n            # Draw a green circle at the mouse click coordinates\n            cv2.circle(image, (x, y), radius, fg_color, -1)\n\n            # Show the updated image with the circle\n            cv2.imshow(\"Image\", image)\n\n        elif event == cv2.EVENT_RBUTTONDOWN:\n            # Append the coordinates to the mouse_clicks list\n            right_clicks.append((x, y))\n\n            # Draw a red circle at the mouse click coordinates\n            cv2.circle(image, (x, y), radius, bg_color, -1)\n\n            # Show the updated image with the circle\n            cv2.imshow(\"Image\", image)\n\n    # Create a window to display the image\n    cv2.namedWindow(\"Image\")\n\n    # Set the mouse callback function for the window\n    cv2.setMouseCallback(\"Image\", get_mouse_coordinates)\n\n    # Display the image in the window\n    cv2.imshow(\"Image\", image)\n\n    # Wait for a key press to exit\n    cv2.waitKey(0)\n\n    # Destroy the window\n    cv2.destroyAllWindows()\n\n    return left_clicks, right_clicks\n</code></pre>"},{"location":"common/#samgeo.common.split_raster","title":"<code>split_raster(filename, out_dir, tile_size=256, overlap=0)</code>","text":"<p>Split a raster into tiles.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The path or http URL to the raster file.</p> required <code>out_dir</code> <code>str</code> <p>The path to the output directory.</p> required <code>tile_size</code> <code>int | tuple</code> <p>The size of the tiles. Can be an integer or a tuple of (width, height). Defaults to 256.</p> <code>256</code> <code>overlap</code> <code>int</code> <p>The number of pixels to overlap between tiles. Defaults to 0.</p> <code>0</code> <p>Exceptions:</p> Type Description <code>ImportError</code> <p>Raised if GDAL is not installed.</p> Source code in <code>samgeo/common.py</code> <pre><code>def split_raster(filename, out_dir, tile_size=256, overlap=0):\n    \"\"\"Split a raster into tiles.\n\n    Args:\n        filename (str): The path or http URL to the raster file.\n        out_dir (str): The path to the output directory.\n        tile_size (int | tuple, optional): The size of the tiles. Can be an integer or a tuple of (width, height). Defaults to 256.\n        overlap (int, optional): The number of pixels to overlap between tiles. Defaults to 0.\n\n    Raises:\n        ImportError: Raised if GDAL is not installed.\n    \"\"\"\n\n    try:\n        from osgeo import gdal\n    except ImportError:\n        raise ImportError(\n            \"GDAL is required to use this function. Install it with `conda install gdal -c conda-forge`\"\n        )\n\n    if isinstance(filename, str):\n        if filename.startswith(\"http\"):\n            output = filename.split(\"/\")[-1]\n            download_file(filename, output)\n            filename = output\n\n    # Open the input GeoTIFF file\n    ds = gdal.Open(filename)\n\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    if isinstance(tile_size, int):\n        tile_width = tile_size\n        tile_height = tile_size\n    elif isinstance(tile_size, tuple):\n        tile_width = tile_size[0]\n        tile_height = tile_size[1]\n\n    # Get the size of the input raster\n    width = ds.RasterXSize\n    height = ds.RasterYSize\n\n    # Calculate the number of tiles needed in both directions, taking into account the overlap\n    num_tiles_x = (width - overlap) // (tile_width - overlap) + int(\n        (width - overlap) % (tile_width - overlap) &gt; 0\n    )\n    num_tiles_y = (height - overlap) // (tile_height - overlap) + int(\n        (height - overlap) % (tile_height - overlap) &gt; 0\n    )\n\n    # Get the georeferencing information of the input raster\n    geotransform = ds.GetGeoTransform()\n\n    # Loop over all the tiles\n    for i in range(num_tiles_x):\n        for j in range(num_tiles_y):\n            # Calculate the pixel coordinates of the tile, taking into account the overlap and clamping to the edge of the raster\n            x_min = i * (tile_width - overlap)\n            y_min = j * (tile_height - overlap)\n            x_max = min(x_min + tile_width, width)\n            y_max = min(y_min + tile_height, height)\n\n            # Adjust the size of the last tile in each row and column to include any remaining pixels\n            if i == num_tiles_x - 1:\n                x_min = max(x_max - tile_width, 0)\n            if j == num_tiles_y - 1:\n                y_min = max(y_max - tile_height, 0)\n\n            # Calculate the size of the tile, taking into account the overlap\n            tile_width = x_max - x_min\n            tile_height = y_max - y_min\n\n            # Set the output file name\n            output_file = f\"{out_dir}/tile_{i}_{j}.tif\"\n\n            # Create a new dataset for the tile\n            driver = gdal.GetDriverByName(\"GTiff\")\n            tile_ds = driver.Create(\n                output_file,\n                tile_width,\n                tile_height,\n                ds.RasterCount,\n                ds.GetRasterBand(1).DataType,\n            )\n\n            # Calculate the georeferencing information for the output tile\n            tile_geotransform = (\n                geotransform[0] + x_min * geotransform[1],\n                geotransform[1],\n                0,\n                geotransform[3] + y_min * geotransform[5],\n                0,\n                geotransform[5],\n            )\n\n            # Set the geotransform and projection of the tile\n            tile_ds.SetGeoTransform(tile_geotransform)\n            tile_ds.SetProjection(ds.GetProjection())\n\n            # Read the data from the input raster band(s) and write it to the tile band(s)\n            for k in range(ds.RasterCount):\n                band = ds.GetRasterBand(k + 1)\n                tile_band = tile_ds.GetRasterBand(k + 1)\n                tile_data = band.ReadAsArray(x_min, y_min, tile_width, tile_height)\n                tile_band.WriteArray(tile_data)\n\n            # Close the tile dataset\n            tile_ds = None\n\n    # Close the input dataset\n    ds = None\n</code></pre>"},{"location":"common/#samgeo.common.temp_file_path","title":"<code>temp_file_path(extension)</code>","text":"<p>Returns a temporary file path.</p> <p>Parameters:</p> Name Type Description Default <code>extension</code> <code>str</code> <p>The file extension.</p> required <p>Returns:</p> Type Description <code>str</code> <p>The temporary file path.</p> Source code in <code>samgeo/common.py</code> <pre><code>def temp_file_path(extension):\n    \"\"\"Returns a temporary file path.\n\n    Args:\n        extension (str): The file extension.\n\n    Returns:\n        str: The temporary file path.\n    \"\"\"\n\n    import tempfile\n    import uuid\n\n    if not extension.startswith(\".\"):\n        extension = \".\" + extension\n    file_id = str(uuid.uuid4())\n    file_path = os.path.join(tempfile.gettempdir(), f\"{file_id}{extension}\")\n\n    return file_path\n</code></pre>"},{"location":"common/#samgeo.common.text_sam_gui","title":"<code>text_sam_gui(sam, basemap='SATELLITE', out_dir=None, box_threshold=0.25, text_threshold=0.25, cmap='viridis', opacity=0.5, **kwargs)</code>","text":"<p>Display the SAM Map GUI.</p> <p>Parameters:</p> Name Type Description Default <code>sam</code> <code>SamGeo</code> required <code>basemap</code> <code>str</code> <p>The basemap to use. Defaults to \"SATELLITE\".</p> <code>'SATELLITE'</code> <code>out_dir</code> <code>str</code> <p>The output directory. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/common.py</code> <pre><code>def text_sam_gui(\n    sam,\n    basemap=\"SATELLITE\",\n    out_dir=None,\n    box_threshold=0.25,\n    text_threshold=0.25,\n    cmap=\"viridis\",\n    opacity=0.5,\n    **kwargs,\n):\n    \"\"\"Display the SAM Map GUI.\n\n    Args:\n        sam (SamGeo):\n        basemap (str, optional): The basemap to use. Defaults to \"SATELLITE\".\n        out_dir (str, optional): The output directory. Defaults to None.\n\n    \"\"\"\n    try:\n        import shutil\n        import tempfile\n        import leafmap\n        import ipyleaflet\n        import ipyevents\n        import ipywidgets as widgets\n        import leafmap.colormaps as cm\n        from ipyfilechooser import FileChooser\n    except ImportError:\n        raise ImportError(\n            \"The sam_map function requires the leafmap package. Please install it first.\"\n        )\n\n    if out_dir is None:\n        out_dir = tempfile.gettempdir()\n\n    m = leafmap.Map(**kwargs)\n    m.default_style = {\"cursor\": \"crosshair\"}\n    m.add_basemap(basemap, show=False)\n\n    # Skip the image layer if localtileserver is not available\n    try:\n        m.add_raster(sam.source, layer_name=\"Image\")\n    except:\n        pass\n\n    widget_width = \"280px\"\n    button_width = \"90px\"\n    padding = \"0px 4px 0px 4px\"  # upper, right, bottom, left\n    style = {\"description_width\": \"initial\"}\n\n    toolbar_button = widgets.ToggleButton(\n        value=True,\n        tooltip=\"Toolbar\",\n        icon=\"gear\",\n        layout=widgets.Layout(width=\"28px\", height=\"28px\", padding=\"0px 0px 0px 4px\"),\n    )\n\n    close_button = widgets.ToggleButton(\n        value=False,\n        tooltip=\"Close the tool\",\n        icon=\"times\",\n        button_style=\"primary\",\n        layout=widgets.Layout(height=\"28px\", width=\"28px\", padding=\"0px 0px 0px 4px\"),\n    )\n\n    text_prompt = widgets.Text(\n        description=\"Text prompt:\",\n        style=style,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n    )\n\n    box_slider = widgets.FloatSlider(\n        description=\"Box threshold:\",\n        min=0,\n        max=1,\n        value=box_threshold,\n        step=0.01,\n        readout=True,\n        continuous_update=True,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n        style=style,\n    )\n\n    text_slider = widgets.FloatSlider(\n        description=\"Text threshold:\",\n        min=0,\n        max=1,\n        step=0.01,\n        value=text_threshold,\n        readout=True,\n        continuous_update=True,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n        style=style,\n    )\n\n    cmap_dropdown = widgets.Dropdown(\n        description=\"Palette:\",\n        options=cm.list_colormaps(),\n        value=cmap,\n        style=style,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n    )\n\n    opacity_slider = widgets.FloatSlider(\n        description=\"Opacity:\",\n        min=0,\n        max=1,\n        value=opacity,\n        readout=True,\n        continuous_update=True,\n        layout=widgets.Layout(width=widget_width, padding=padding),\n        style=style,\n    )\n\n    def opacity_changed(change):\n        if change[\"new\"]:\n            if hasattr(m, \"layer_name\"):\n                mask_layer = m.find_layer(m.layer_name)\n                if mask_layer is not None:\n                    mask_layer.interact(opacity=opacity_slider.value)\n\n    opacity_slider.observe(opacity_changed, \"value\")\n\n    rectangular = widgets.Checkbox(\n        value=False,\n        description=\"Regularize\",\n        layout=widgets.Layout(width=\"130px\", padding=padding),\n        style=style,\n    )\n\n    colorpicker = widgets.ColorPicker(\n        concise=False,\n        description=\"Color\",\n        value=\"#ffff00\",\n        layout=widgets.Layout(width=\"140px\", padding=padding),\n        style=style,\n    )\n\n    segment_button = widgets.ToggleButton(\n        description=\"Segment\",\n        value=False,\n        button_style=\"primary\",\n        layout=widgets.Layout(padding=padding),\n    )\n\n    save_button = widgets.ToggleButton(\n        description=\"Save\", value=False, button_style=\"primary\"\n    )\n\n    reset_button = widgets.ToggleButton(\n        description=\"Reset\", value=False, button_style=\"primary\"\n    )\n    segment_button.layout.width = button_width\n    save_button.layout.width = button_width\n    reset_button.layout.width = button_width\n\n    output = widgets.Output(\n        layout=widgets.Layout(\n            width=widget_width, padding=padding, max_width=widget_width\n        )\n    )\n\n    toolbar_header = widgets.HBox()\n    toolbar_header.children = [close_button, toolbar_button]\n    toolbar_footer = widgets.VBox()\n    toolbar_footer.children = [\n        text_prompt,\n        box_slider,\n        text_slider,\n        cmap_dropdown,\n        opacity_slider,\n        widgets.HBox([rectangular, colorpicker]),\n        widgets.HBox(\n            [segment_button, save_button, reset_button],\n            layout=widgets.Layout(padding=\"0px 4px 0px 4px\"),\n        ),\n        output,\n    ]\n    toolbar_widget = widgets.VBox()\n    toolbar_widget.children = [toolbar_header, toolbar_footer]\n\n    toolbar_event = ipyevents.Event(\n        source=toolbar_widget, watched_events=[\"mouseenter\", \"mouseleave\"]\n    )\n\n    def handle_toolbar_event(event):\n        if event[\"type\"] == \"mouseenter\":\n            toolbar_widget.children = [toolbar_header, toolbar_footer]\n        elif event[\"type\"] == \"mouseleave\":\n            if not toolbar_button.value:\n                toolbar_widget.children = [toolbar_button]\n                toolbar_button.value = False\n                close_button.value = False\n\n    toolbar_event.on_dom_event(handle_toolbar_event)\n\n    def toolbar_btn_click(change):\n        if change[\"new\"]:\n            close_button.value = False\n            toolbar_widget.children = [toolbar_header, toolbar_footer]\n        else:\n            if not close_button.value:\n                toolbar_widget.children = [toolbar_button]\n\n    toolbar_button.observe(toolbar_btn_click, \"value\")\n\n    def close_btn_click(change):\n        if change[\"new\"]:\n            toolbar_button.value = False\n            if m.toolbar_control in m.controls:\n                m.remove_control(m.toolbar_control)\n            toolbar_widget.close()\n\n    close_button.observe(close_btn_click, \"value\")\n\n    def segment_button_click(change):\n        if change[\"new\"]:\n            segment_button.value = False\n            with output:\n                output.clear_output()\n                if len(text_prompt.value) == 0:\n                    print(\"Please enter a text prompt first.\")\n                elif sam.source is None:\n                    print(\"Please run sam.set_image() first.\")\n                else:\n                    print(\"Segmenting...\")\n                    layer_name = text_prompt.value.replace(\" \", \"_\")\n                    filename = os.path.join(\n                        out_dir, f\"{layer_name}_{random_string()}.tif\"\n                    )\n                    try:\n                        sam.predict(\n                            sam.source,\n                            text_prompt.value,\n                            box_slider.value,\n                            text_slider.value,\n                            output=filename,\n                        )\n                        sam.output = filename\n                        if m.find_layer(layer_name) is not None:\n                            m.remove_layer(m.find_layer(layer_name))\n                        if m.find_layer(f\"{layer_name}_rect\") is not None:\n                            m.remove_layer(m.find_layer(f\"{layer_name} Regularized\"))\n                    except Exception as e:\n                        output.clear_output()\n                        print(e)\n                    if os.path.exists(filename):\n                        try:\n                            m.add_raster(\n                                filename,\n                                layer_name=layer_name,\n                                palette=cmap_dropdown.value,\n                                opacity=opacity_slider.value,\n                                nodata=0,\n                                zoom_to_layer=False,\n                            )\n                            m.layer_name = layer_name\n\n                            if rectangular.value:\n                                vector = filename.replace(\".tif\", \".gpkg\")\n                                vector_rec = filename.replace(\".tif\", \"_rect.gpkg\")\n                                raster_to_vector(filename, vector)\n                                regularize(vector, vector_rec)\n                                vector_style = {\"color\": colorpicker.value}\n                                m.add_vector(\n                                    vector_rec,\n                                    layer_name=f\"{layer_name} Regularized\",\n                                    style=vector_style,\n                                    info_mode=None,\n                                    zoom_to_layer=False,\n                                )\n\n                            output.clear_output()\n                        except Exception as e:\n                            print(e)\n\n    segment_button.observe(segment_button_click, \"value\")\n\n    def filechooser_callback(chooser):\n        with output:\n            if chooser.selected is not None:\n                try:\n                    filename = chooser.selected\n                    shutil.copy(sam.output, filename)\n                    vector = filename.replace(\".tif\", \".gpkg\")\n                    raster_to_vector(filename, vector)\n                    if rectangular.value:\n                        vector_rec = filename.replace(\".tif\", \"_rect.gpkg\")\n                        regularize(vector, vector_rec)\n                except Exception as e:\n                    print(e)\n\n                if hasattr(m, \"save_control\") and m.save_control in m.controls:\n                    m.remove_control(m.save_control)\n                    delattr(m, \"save_control\")\n                save_button.value = False\n\n    def save_button_click(change):\n        if change[\"new\"]:\n            with output:\n                output.clear_output()\n                if not hasattr(m, \"layer_name\"):\n                    print(\"Please click the Segment button first.\")\n                else:\n                    sandbox_path = os.environ.get(\"SANDBOX_PATH\")\n                    filechooser = FileChooser(\n                        path=os.getcwd(),\n                        filename=f\"{m.layer_name}.tif\",\n                        sandbox_path=sandbox_path,\n                        layout=widgets.Layout(width=\"454px\"),\n                    )\n                    filechooser.use_dir_icons = True\n                    filechooser.filter_pattern = [\"*.tif\"]\n                    filechooser.register_callback(filechooser_callback)\n                    save_control = ipyleaflet.WidgetControl(\n                        widget=filechooser, position=\"topright\"\n                    )\n                    m.add_control(save_control)\n                    m.save_control = save_control\n\n        else:\n            if hasattr(m, \"save_control\") and m.save_control in m.controls:\n                m.remove_control(m.save_control)\n                delattr(m, \"save_control\")\n\n    save_button.observe(save_button_click, \"value\")\n\n    def reset_button_click(change):\n        if change[\"new\"]:\n            segment_button.value = False\n            save_button.value = False\n            reset_button.value = False\n            opacity_slider.value = 0.5\n            box_slider.value = 0.25\n            text_slider.value = 0.25\n            cmap_dropdown.value = \"viridis\"\n            text_prompt.value = \"\"\n            output.clear_output()\n            try:\n                if hasattr(m, \"layer_name\") and m.find_layer(m.layer_name) is not None:\n                    m.remove_layer(m.find_layer(m.layer_name))\n                m.clear_drawings()\n            except:\n                pass\n\n    reset_button.observe(reset_button_click, \"value\")\n\n    toolbar_control = ipyleaflet.WidgetControl(\n        widget=toolbar_widget, position=\"topright\"\n    )\n    m.add_control(toolbar_control)\n    m.toolbar_control = toolbar_control\n\n    return m\n</code></pre>"},{"location":"common/#samgeo.common.tms_to_geotiff","title":"<code>tms_to_geotiff(output, bbox, zoom=None, resolution=None, source='OpenStreetMap', crs='EPSG:3857', to_cog=False, return_image=False, overwrite=False, quiet=False, **kwargs)</code>","text":"<p>Download TMS tiles and convert them to a GeoTIFF. The source is adapted from https://github.com/gumblex/tms2geotiff.     Credits to the GitHub user @gumblex.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The output GeoTIFF file.</p> required <code>bbox</code> <code>list</code> <p>The bounding box [minx, miny, maxx, maxy], e.g., [-122.5216, 37.733, -122.3661, 37.8095]</p> required <code>zoom</code> <code>int</code> <p>The map zoom level. Defaults to None.</p> <code>None</code> <code>resolution</code> <code>float</code> <p>The resolution in meters. Defaults to None.</p> <code>None</code> <code>source</code> <code>str</code> <p>The tile source. It can be one of the following: \"OPENSTREETMAP\", \"ROADMAP\", \"SATELLITE\", \"TERRAIN\", \"HYBRID\", or an HTTP URL. Defaults to \"OpenStreetMap\".</p> <code>'OpenStreetMap'</code> <code>crs</code> <code>str</code> <p>The output CRS. Defaults to \"EPSG:3857\".</p> <code>'EPSG:3857'</code> <code>to_cog</code> <code>bool</code> <p>Convert to Cloud Optimized GeoTIFF. Defaults to False.</p> <code>False</code> <code>return_image</code> <code>bool</code> <p>Return the image as PIL.Image. Defaults to False.</p> <code>False</code> <code>overwrite</code> <code>bool</code> <p>Overwrite the output file if it already exists. Defaults to False.</p> <code>False</code> <code>quiet</code> <code>bool</code> <p>Suppress output. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional arguments to pass to gdal.GetDriverByName(\"GTiff\").Create().</p> <code>{}</code> Source code in <code>samgeo/common.py</code> <pre><code>def tms_to_geotiff(\n    output,\n    bbox,\n    zoom=None,\n    resolution=None,\n    source=\"OpenStreetMap\",\n    crs=\"EPSG:3857\",\n    to_cog=False,\n    return_image=False,\n    overwrite=False,\n    quiet=False,\n    **kwargs,\n):\n    \"\"\"Download TMS tiles and convert them to a GeoTIFF. The source is adapted from https://github.com/gumblex/tms2geotiff.\n        Credits to the GitHub user @gumblex.\n\n    Args:\n        output (str): The output GeoTIFF file.\n        bbox (list): The bounding box [minx, miny, maxx, maxy], e.g., [-122.5216, 37.733, -122.3661, 37.8095]\n        zoom (int, optional): The map zoom level. Defaults to None.\n        resolution (float, optional): The resolution in meters. Defaults to None.\n        source (str, optional): The tile source. It can be one of the following: \"OPENSTREETMAP\", \"ROADMAP\",\n            \"SATELLITE\", \"TERRAIN\", \"HYBRID\", or an HTTP URL. Defaults to \"OpenStreetMap\".\n        crs (str, optional): The output CRS. Defaults to \"EPSG:3857\".\n        to_cog (bool, optional): Convert to Cloud Optimized GeoTIFF. Defaults to False.\n        return_image (bool, optional): Return the image as PIL.Image. Defaults to False.\n        overwrite (bool, optional): Overwrite the output file if it already exists. Defaults to False.\n        quiet (bool, optional): Suppress output. Defaults to False.\n        **kwargs: Additional arguments to pass to gdal.GetDriverByName(\"GTiff\").Create().\n\n    \"\"\"\n\n    import os\n    import io\n    import math\n    import itertools\n    import concurrent.futures\n\n    import numpy\n    from PIL import Image\n\n    try:\n        from osgeo import gdal, osr\n    except ImportError:\n        raise ImportError(\"GDAL is not installed. Install it with pip install GDAL\")\n\n    try:\n        import httpx\n\n        SESSION = httpx.Client()\n    except ImportError:\n        import requests\n\n        SESSION = requests.Session()\n\n    if not overwrite and os.path.exists(output):\n        print(\n            f\"The output file {output} already exists. Use `overwrite=True` to overwrite it.\"\n        )\n        return\n\n    xyz_tiles = {\n        \"OPENSTREETMAP\": \"https://tile.openstreetmap.org/{z}/{x}/{y}.png\",\n        \"ROADMAP\": \"https://mt1.google.com/vt/lyrs=m&amp;x={x}&amp;y={y}&amp;z={z}\",\n        \"SATELLITE\": \"https://mt1.google.com/vt/lyrs=s&amp;x={x}&amp;y={y}&amp;z={z}\",\n        \"TERRAIN\": \"https://mt1.google.com/vt/lyrs=p&amp;x={x}&amp;y={y}&amp;z={z}\",\n        \"HYBRID\": \"https://mt1.google.com/vt/lyrs=y&amp;x={x}&amp;y={y}&amp;z={z}\",\n    }\n\n    basemaps = get_basemaps()\n\n    if isinstance(source, str):\n        if source.upper() in xyz_tiles:\n            source = xyz_tiles[source.upper()]\n        elif source in basemaps:\n            source = basemaps[source]\n        elif source.startswith(\"http\"):\n            pass\n    else:\n        raise ValueError(\n            'source must be one of \"OpenStreetMap\", \"ROADMAP\", \"SATELLITE\", \"TERRAIN\", \"HYBRID\", or a URL'\n        )\n\n    def resolution_to_zoom_level(resolution):\n        \"\"\"\n        Convert map resolution in meters to zoom level for Web Mercator (EPSG:3857) tiles.\n        \"\"\"\n        # Web Mercator tile size in meters at zoom level 0\n        initial_resolution = 156543.03392804097\n\n        # Calculate the zoom level\n        zoom_level = math.log2(initial_resolution / resolution)\n\n        return int(zoom_level)\n\n    if isinstance(bbox, list) and len(bbox) == 4:\n        west, south, east, north = bbox\n    else:\n        raise ValueError(\n            \"bbox must be a list of 4 coordinates in the format of [xmin, ymin, xmax, ymax]\"\n        )\n\n    if zoom is None and resolution is None:\n        raise ValueError(\"Either zoom or resolution must be provided\")\n    elif zoom is not None and resolution is not None:\n        raise ValueError(\"Only one of zoom or resolution can be provided\")\n\n    if resolution is not None:\n        zoom = resolution_to_zoom_level(resolution)\n\n    EARTH_EQUATORIAL_RADIUS = 6378137.0\n\n    Image.MAX_IMAGE_PIXELS = None\n\n    gdal.UseExceptions()\n    web_mercator = osr.SpatialReference()\n    web_mercator.ImportFromEPSG(3857)\n\n    WKT_3857 = web_mercator.ExportToWkt()\n\n    def from4326_to3857(lat, lon):\n        xtile = math.radians(lon) * EARTH_EQUATORIAL_RADIUS\n        ytile = (\n            math.log(math.tan(math.radians(45 + lat / 2.0))) * EARTH_EQUATORIAL_RADIUS\n        )\n        return (xtile, ytile)\n\n    def deg2num(lat, lon, zoom):\n        lat_r = math.radians(lat)\n        n = 2**zoom\n        xtile = (lon + 180) / 360 * n\n        ytile = (1 - math.log(math.tan(lat_r) + 1 / math.cos(lat_r)) / math.pi) / 2 * n\n        return (xtile, ytile)\n\n    def is_empty(im):\n        extrema = im.getextrema()\n        if len(extrema) &gt;= 3:\n            if len(extrema) &gt; 3 and extrema[-1] == (0, 0):\n                return True\n            for ext in extrema[:3]:\n                if ext != (0, 0):\n                    return False\n            return True\n        else:\n            return extrema[0] == (0, 0)\n\n    def paste_tile(bigim, base_size, tile, corner_xy, bbox):\n        if tile is None:\n            return bigim\n        im = Image.open(io.BytesIO(tile))\n        mode = \"RGB\" if im.mode == \"RGB\" else \"RGBA\"\n        size = im.size\n        if bigim is None:\n            base_size[0] = size[0]\n            base_size[1] = size[1]\n            newim = Image.new(\n                mode, (size[0] * (bbox[2] - bbox[0]), size[1] * (bbox[3] - bbox[1]))\n            )\n        else:\n            newim = bigim\n\n        dx = abs(corner_xy[0] - bbox[0])\n        dy = abs(corner_xy[1] - bbox[1])\n        xy0 = (size[0] * dx, size[1] * dy)\n        if mode == \"RGB\":\n            newim.paste(im, xy0)\n        else:\n            if im.mode != mode:\n                im = im.convert(mode)\n            if not is_empty(im):\n                newim.paste(im, xy0)\n        im.close()\n        return newim\n\n    def finish_picture(bigim, base_size, bbox, x0, y0, x1, y1):\n        xfrac = x0 - bbox[0]\n        yfrac = y0 - bbox[1]\n        x2 = round(base_size[0] * xfrac)\n        y2 = round(base_size[1] * yfrac)\n        imgw = round(base_size[0] * (x1 - x0))\n        imgh = round(base_size[1] * (y1 - y0))\n        retim = bigim.crop((x2, y2, x2 + imgw, y2 + imgh))\n        if retim.mode == \"RGBA\" and retim.getextrema()[3] == (255, 255):\n            retim = retim.convert(\"RGB\")\n        bigim.close()\n        return retim\n\n    def get_tile(url):\n        retry = 3\n        while 1:\n            try:\n                r = SESSION.get(url, timeout=60)\n                break\n            except Exception:\n                retry -= 1\n                if not retry:\n                    raise\n        if r.status_code == 404:\n            return None\n        elif not r.content:\n            return None\n        r.raise_for_status()\n        return r.content\n\n    def draw_tile(\n        source, lat0, lon0, lat1, lon1, zoom, filename, quiet=False, **kwargs\n    ):\n        x0, y0 = deg2num(lat0, lon0, zoom)\n        x1, y1 = deg2num(lat1, lon1, zoom)\n        x0, x1 = sorted([x0, x1])\n        y0, y1 = sorted([y0, y1])\n        corners = tuple(\n            itertools.product(\n                range(math.floor(x0), math.ceil(x1)),\n                range(math.floor(y0), math.ceil(y1)),\n            )\n        )\n        totalnum = len(corners)\n        futures = []\n        with concurrent.futures.ThreadPoolExecutor(5) as executor:\n            for x, y in corners:\n                futures.append(\n                    executor.submit(get_tile, source.format(z=zoom, x=x, y=y))\n                )\n            bbox = (math.floor(x0), math.floor(y0), math.ceil(x1), math.ceil(y1))\n            bigim = None\n            base_size = [256, 256]\n            for k, (fut, corner_xy) in enumerate(zip(futures, corners), 1):\n                bigim = paste_tile(bigim, base_size, fut.result(), corner_xy, bbox)\n                if not quiet:\n                    print(\n                        f\"Downloaded image {str(k).zfill(len(str(totalnum)))}/{totalnum}\"\n                    )\n\n        if not quiet:\n            print(\"Saving GeoTIFF. Please wait...\")\n        img = finish_picture(bigim, base_size, bbox, x0, y0, x1, y1)\n        imgbands = len(img.getbands())\n        driver = gdal.GetDriverByName(\"GTiff\")\n\n        if \"options\" not in kwargs:\n            kwargs[\"options\"] = [\n                \"COMPRESS=DEFLATE\",\n                \"PREDICTOR=2\",\n                \"ZLEVEL=9\",\n                \"TILED=YES\",\n            ]\n\n        gtiff = driver.Create(\n            filename,\n            img.size[0],\n            img.size[1],\n            imgbands,\n            gdal.GDT_Byte,\n            **kwargs,\n        )\n        xp0, yp0 = from4326_to3857(lat0, lon0)\n        xp1, yp1 = from4326_to3857(lat1, lon1)\n        pwidth = abs(xp1 - xp0) / img.size[0]\n        pheight = abs(yp1 - yp0) / img.size[1]\n        gtiff.SetGeoTransform((min(xp0, xp1), pwidth, 0, max(yp0, yp1), 0, -pheight))\n        gtiff.SetProjection(WKT_3857)\n        for band in range(imgbands):\n            array = numpy.array(img.getdata(band), dtype=\"u8\")\n            array = array.reshape((img.size[1], img.size[0]))\n            band = gtiff.GetRasterBand(band + 1)\n            band.WriteArray(array)\n        gtiff.FlushCache()\n\n        if not quiet:\n            print(f\"Image saved to {filename}\")\n        return img\n\n    try:\n        image = draw_tile(\n            source, south, west, north, east, zoom, output, quiet, **kwargs\n        )\n        if return_image:\n            return image\n        if crs.upper() != \"EPSG:3857\":\n            reproject(output, output, crs, to_cog=to_cog)\n        elif to_cog:\n            image_to_cog(output, output)\n    except Exception as e:\n        raise Exception(e)\n</code></pre>"},{"location":"common/#samgeo.common.transform_coords","title":"<code>transform_coords(x, y, src_crs, dst_crs, **kwargs)</code>","text":"<p>Transform coordinates from one CRS to another.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>float</code> <p>The x coordinate.</p> required <code>y</code> <code>float</code> <p>The y coordinate.</p> required <code>src_crs</code> <code>str</code> <p>The source CRS, e.g., \"EPSG:4326\".</p> required <code>dst_crs</code> <code>str</code> <p>The destination CRS, e.g., \"EPSG:3857\".</p> required <p>Returns:</p> Type Description <code>dict</code> <p>The transformed coordinates in the format of (x, y)</p> Source code in <code>samgeo/common.py</code> <pre><code>def transform_coords(x, y, src_crs, dst_crs, **kwargs):\n    \"\"\"Transform coordinates from one CRS to another.\n\n    Args:\n        x (float): The x coordinate.\n        y (float): The y coordinate.\n        src_crs (str): The source CRS, e.g., \"EPSG:4326\".\n        dst_crs (str): The destination CRS, e.g., \"EPSG:3857\".\n\n    Returns:\n        dict: The transformed coordinates in the format of (x, y)\n    \"\"\"\n    transformer = pyproj.Transformer.from_crs(\n        src_crs, dst_crs, always_xy=True, **kwargs\n    )\n    return transformer.transform(x, y)\n</code></pre>"},{"location":"common/#samgeo.common.update_package","title":"<code>update_package(out_dir=None, keep=False, **kwargs)</code>","text":"<p>Updates the package from the GitHub repository without the need to use pip or conda.</p> <p>Parameters:</p> Name Type Description Default <code>out_dir</code> <code>str</code> <p>The output directory. Defaults to None.</p> <code>None</code> <code>keep</code> <code>bool</code> <p>Whether to keep the downloaded package. Defaults to False.</p> <code>False</code> <code>**kwargs</code> <p>Additional keyword arguments to pass to the download_file() function.</p> <code>{}</code> Source code in <code>samgeo/common.py</code> <pre><code>def update_package(out_dir=None, keep=False, **kwargs):\n    \"\"\"Updates the package from the GitHub repository without the need to use pip or conda.\n\n    Args:\n        out_dir (str, optional): The output directory. Defaults to None.\n        keep (bool, optional): Whether to keep the downloaded package. Defaults to False.\n        **kwargs: Additional keyword arguments to pass to the download_file() function.\n    \"\"\"\n\n    import shutil\n\n    try:\n        if out_dir is None:\n            out_dir = os.getcwd()\n        url = (\n            \"https://github.com/opengeos/segment-geospatial/archive/refs/heads/main.zip\"\n        )\n        filename = \"segment-geospatial-main.zip\"\n        download_file(url, filename, **kwargs)\n\n        pkg_dir = os.path.join(out_dir, \"segment-geospatial-main\")\n        work_dir = os.getcwd()\n        os.chdir(pkg_dir)\n\n        if shutil.which(\"pip\") is None:\n            cmd = \"pip3 install .\"\n        else:\n            cmd = \"pip install .\"\n\n        os.system(cmd)\n        os.chdir(work_dir)\n\n        if not keep:\n            shutil.rmtree(pkg_dir)\n            try:\n                os.remove(filename)\n            except:\n                pass\n\n        print(\"Package updated successfully.\")\n\n    except Exception as e:\n        raise Exception(e)\n</code></pre>"},{"location":"common/#samgeo.common.vector_to_geojson","title":"<code>vector_to_geojson(filename, output=None, **kwargs)</code>","text":"<p>Converts a vector file to a geojson file.</p> <p>Parameters:</p> Name Type Description Default <code>filename</code> <code>str</code> <p>The vector file path.</p> required <code>output</code> <code>str</code> <p>The output geojson file path. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>dict</code> <p>The geojson dictionary.</p> Source code in <code>samgeo/common.py</code> <pre><code>def vector_to_geojson(filename, output=None, **kwargs):\n    \"\"\"Converts a vector file to a geojson file.\n\n    Args:\n        filename (str): The vector file path.\n        output (str, optional): The output geojson file path. Defaults to None.\n\n    Returns:\n        dict: The geojson dictionary.\n    \"\"\"\n\n    if not filename.startswith(\"http\"):\n        filename = download_file(filename)\n\n    gdf = gpd.read_file(filename, **kwargs)\n    if output is None:\n        return gdf.__geo_interface__\n    else:\n        gdf.to_file(output, driver=\"GeoJSON\")\n</code></pre>"},{"location":"contributing/","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/giswqs/segment-geospatial/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with <code>bug</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with <code>enhancement</code> and <code>help wanted</code> is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>segment-geospatial could always use more documentation, whether as part of the official segment-geospatial docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/giswqs/segment-geospatial/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible, to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project, and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up segment-geospatial for local development.</p> <ol> <li> <p>Fork the segment-geospatial repo on GitHub.</p> </li> <li> <p>Clone your fork locally:</p> <pre><code>$ git clone git@github.com:your_name_here/segment-geospatial.git\n</code></pre> </li> <li> <p>Install your local copy into a virtualenv. Assuming you have     virtualenvwrapper installed, this is how you set up your fork for     local development:</p> <pre><code>$ mkvirtualenv segment-geospatial\n$ cd segment-geospatial/\n$ python setup.py develop\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>$ git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass flake8     and the tests, including testing other Python versions with tox:</p> <pre><code>$ flake8 segment-geospatial tests\n$ python setup.py test or pytest\n$ tox\n</code></pre> <p>To get flake8 and tox, just pip install them into your virtualenv.</p> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>$ git add .\n$ git commit -m \"Your detailed description of your changes.\"\n$ git push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests (see the section below - Unit Testing).</li> <li>If the pull request adds functionality, the docs should be updated.     Put your new functionality into a function with a docstring, and add     the feature to the list in README.md.</li> <li>The pull request should work for Python 3.8, 3.9, 3.10, and 3.11. Check https://github.com/giswqs/segment-geospatial/pull_requests and make sure that the tests pass for all     supported Python versions.</li> </ol>"},{"location":"contributing/#unit-testing","title":"Unit Testing","text":"<p>Unit tests are in the <code>tests</code> folder. If you add new functionality to the package, please add a unit test for it. You can either add the test to an existing test file or create a new one. For example, if you add a new function to <code>samgeo/samgeo.py</code>, you can add the unit test to <code>tests/test_samgeo.py</code>. If you add a new module to <code>samgeo/&lt;MODULE-NAME&gt;</code>, you can create a new test file in <code>tests/test_&lt;MODULE-NAME&gt;</code>. Please refer to <code>tests/test_samgeo.py</code> for examples. For more information about unit testing, please refer to this tutorial - Getting Started With Testing in Python.</p> <p>To run the unit tests, navigate to the root directory of the package and run the following command:</p> <pre><code>python -m unittest discover tests/\n</code></pre>"},{"location":"contributing/#add-new-dependencies","title":"Add new dependencies","text":"<p>If you PR involves adding new dependencies, please make sure that the new dependencies are available on both PyPI and conda-forge. Search here to see if the package is available on conda-forge. If the package is not available on conda-forge, it can't be added as a required dependency in <code>requirements.txt</code>. Instead, it should be added as an optional dependency in <code>requirements_dev.txt</code>.</p> <p>If the package is available on PyPI and conda-forge, but if it is challenging to install the package on some operating systems, we would recommend adding the package as an optional dependency in <code>requirements_dev.txt</code> rather than a required dependency in <code>requirements.txt</code>.</p> <p>The dependencies required for building the documentation should be added to <code>requirements_docs.txt</code>. In most cases, contributors do not need to add new dependencies to <code>requirements_docs.txt</code> unless the documentation fails to build due to missing dependencies.</p>"},{"location":"faq/","title":"FAQ","text":""},{"location":"fast_sam/","title":"fast_sam module","text":"<p>Segmenting remote sensing images with the Fast Segment Anything Model (FastSAM. https://github.com/opengeos/FastSAM</p>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo","title":"<code> SamGeo            (FastSAM)         </code>","text":"<p>Segmenting remote sensing images with the Fast Segment Anything Model (FastSAM).</p> Source code in <code>samgeo/fast_sam.py</code> <pre><code>class SamGeo(FastSAM):\n    \"\"\"Segmenting remote sensing images with the Fast Segment Anything Model (FastSAM).\"\"\"\n\n    def __init__(self, model=\"FastSAM-x.pt\", **kwargs):\n        \"\"\"Initialize the FastSAM algorithm.\"\"\"\n\n        if \"checkpoint_dir\" in kwargs:\n            checkpoint_dir = kwargs[\"checkpoint_dir\"]\n            kwargs.pop(\"checkpoint_dir\")\n        else:\n            checkpoint_dir = os.environ.get(\n                \"TORCH_HOME\", os.path.expanduser(\"~/.cache/torch/hub/checkpoints\")\n            )\n\n        models = {\n            \"FastSAM-x.pt\": \"https://github.com/opengeos/datasets/releases/download/models/FastSAM-x.pt\",\n            \"FastSAM-s.pt\": \"https://github.com/opengeos/datasets/releases/download/models/FastSAM-s.pt\",\n        }\n\n        if model not in models:\n            raise ValueError(\n                f\"Model must be one of {list(models.keys())}, but got {model} instead.\"\n            )\n\n        model_path = os.path.join(checkpoint_dir, model)\n\n        if not os.path.exists(model_path):\n            print(f\"Downloading {model} to {model_path}...\")\n            download_file(models[model], model_path)\n\n        super().__init__(model, **kwargs)\n\n    def set_image(self, image, device=None, **kwargs):\n        \"\"\"Set the input image.\n\n        Args:\n            image (str): The path to the image file or a HTTP URL.\n            device (str, optional): The device to use. Defaults to \"cuda\" if available, otherwise \"cpu\".\n            kwargs: Additional keyword arguments to pass to the FastSAM model.\n        \"\"\"\n\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n        else:\n            self.source = None\n\n        # Use cuda if available\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n\n        everything_results = self(image, device=device, **kwargs)\n\n        self.prompt_process = FastSAMPrompt(image, everything_results, device=device)\n\n    def everything_prompt(self, output=None, **kwargs):\n        \"\"\"Segment the image with the everything prompt. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L451\n\n        Args:\n            output (str, optional): The path to save the output image. Defaults to None.\n        \"\"\"\n\n        prompt_process = self.prompt_process\n        ann = prompt_process.everything_prompt()\n        self.annotations = ann\n\n        if output is not None:\n            self.save_masks(output, **kwargs)\n        else:\n            return ann\n\n    def point_prompt(self, points, pointlabel, output=None, **kwargs):\n        \"\"\"Segment the image with the point prompt. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L414\n\n        Args:\n            points (list): A list of points.\n            pointlabel (list): A list of labels for each point.\n            output (str, optional): The path to save the output image. Defaults to None.\n        \"\"\"\n\n        prompt_process = self.prompt_process\n        ann = prompt_process.point_prompt(points, pointlabel)\n        self.annotations = ann\n\n        if output is not None:\n            self.save_masks(output, **kwargs)\n        else:\n            return ann\n\n    def box_prompt(self, bbox=None, bboxes=None, output=None, **kwargs):\n        \"\"\"Segment the image with the box prompt. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L377\n\n        Args:\n            bbox (list, optional): The bounding box. Defaults to None.\n            bboxes (list, optional): A list of bounding boxes. Defaults to None.\n            output (str, optional): The path to save the output image. Defaults to None.\n        \"\"\"\n\n        prompt_process = self.prompt_process\n        ann = prompt_process.box_prompt(bbox, bboxes)\n        self.annotations = ann\n\n        if output is not None:\n            self.save_masks(output, **kwargs)\n        else:\n            return ann\n\n    def text_prompt(self, text, output=None, **kwargs):\n        \"\"\"Segment the image with the text prompt. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L439\n\n        Args:\n            text (str): The text to segment.\n            output (str, optional): The path to save the output image. Defaults to None.\n        \"\"\"\n\n        prompt_process = self.prompt_process\n        ann = prompt_process.text_prompt(text)\n        self.annotations = ann\n\n        if output is not None:\n            self.save_masks(output, **kwargs)\n        else:\n            return ann\n\n    def save_masks(\n        self,\n        output=None,\n        better_quality=True,\n        dtype=None,\n        mask_multiplier=255,\n        **kwargs,\n    ) -&gt; np.ndarray:\n        \"\"\"Save the mask of the image. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222\n\n        Returns:\n            np.ndarray: The mask of the image.\n        \"\"\"\n        annotations = self.annotations\n        if isinstance(annotations[0], dict):\n            annotations = [annotation[\"segmentation\"] for annotation in annotations]\n        image = self.prompt_process.img\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        height = image.shape[0]\n        width = image.shape[1]\n\n        if better_quality:\n            if isinstance(annotations[0], torch.Tensor):\n                annotations = np.array(annotations.cpu())\n            for i, mask in enumerate(annotations):\n                mask = cv2.morphologyEx(\n                    mask.astype(np.uint8), cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8)\n                )\n                annotations[i] = cv2.morphologyEx(\n                    mask.astype(np.uint8), cv2.MORPH_OPEN, np.ones((8, 8), np.uint8)\n                )\n        if self.device == \"cpu\":\n            annotations = np.array(annotations)\n\n        else:\n            if isinstance(annotations[0], np.ndarray):\n                annotations = torch.from_numpy(annotations)\n\n        if isinstance(annotations, torch.Tensor):\n            annotations = annotations.cpu().numpy()\n\n        if dtype is None:\n            # Set output image data type based on the number of objects\n            if len(annotations) &lt; 255:\n                dtype = np.uint8\n            elif len(annotations) &lt; 65535:\n                dtype = np.uint16\n            else:\n                dtype = np.uint32\n\n        masks = np.sum(annotations, axis=0)\n\n        masks = cv2.resize(masks, (width, height), interpolation=cv2.INTER_NEAREST)\n        masks[masks &gt; 0] = 1\n        masks = masks.astype(dtype) * mask_multiplier\n        self.objects = masks\n\n        if output is not None:  # Save the output image\n            array_to_image(self.objects, output, self.source, **kwargs)\n        else:\n            return masks\n\n    def fast_show_mask(\n        self,\n        random_color=False,\n    ):\n        \"\"\"Show the mask of the image. Adapted from\n        https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222\n\n        Args:\n            random_color (bool, optional): Whether to use random colors for each object. Defaults to False.\n\n        Returns:\n            np.ndarray: The mask of the image.\n        \"\"\"\n\n        target_height = self.image.shape[0]\n        target_width = self.image.shape[1]\n        annotations = self.annotations\n        annotation = np.array(annotations.cpu())\n\n        mask_sum = annotation.shape[0]\n        height = annotation.shape[1]\n        weight = annotation.shape[2]\n        # Sort annotations based on area.\n        areas = np.sum(annotation, axis=(1, 2))\n        sorted_indices = np.argsort(areas)\n        annotation = annotation[sorted_indices]\n\n        index = (annotation != 0).argmax(axis=0)\n        if random_color:\n            color = np.random.random((mask_sum, 1, 1, 3))\n        else:\n            color = np.ones((mask_sum, 1, 1, 3)) * np.array(\n                [30 / 255, 144 / 255, 255 / 255]\n            )\n        transparency = np.ones((mask_sum, 1, 1, 1)) * 0.6\n        visual = np.concatenate([color, transparency], axis=-1)\n        mask_image = np.expand_dims(annotation, -1) * visual\n\n        show = np.zeros((height, weight, 4))\n        h_indices, w_indices = np.meshgrid(\n            np.arange(height), np.arange(weight), indexing=\"ij\"\n        )\n        indices = (index[h_indices, w_indices], h_indices, w_indices, slice(None))\n        # Use vectorized indexing to update the values of 'show'.\n        show[h_indices, w_indices, :] = mask_image[indices]\n\n        show = cv2.resize(\n            show, (target_width, target_height), interpolation=cv2.INTER_NEAREST\n        )\n\n        return show\n\n    def raster_to_vector(\n        self, image, output, simplify_tolerance=None, dst_crs=\"EPSG:4326\", **kwargs\n    ):\n        \"\"\"Save the result to a vector file.\n\n        Args:\n            image (str): The path to the image file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(\n            image,\n            output,\n            simplify_tolerance=simplify_tolerance,\n            dst_crs=dst_crs,\n            **kwargs,\n        )\n\n    def show_anns(\n        self,\n        output=None,\n        **kwargs,\n    ):\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n        \"\"\"\n\n        annotations = self.annotations\n        prompt_process = self.prompt_process\n\n        if output is None:\n            output = temp_file_path(\".png\")\n\n        prompt_process.plot(annotations, output, **kwargs)\n\n        show_image(output)\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.__init__","title":"<code>__init__(self, model='FastSAM-x.pt', **kwargs)</code>  <code>special</code>","text":"<p>Initialize the FastSAM algorithm.</p> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def __init__(self, model=\"FastSAM-x.pt\", **kwargs):\n    \"\"\"Initialize the FastSAM algorithm.\"\"\"\n\n    if \"checkpoint_dir\" in kwargs:\n        checkpoint_dir = kwargs[\"checkpoint_dir\"]\n        kwargs.pop(\"checkpoint_dir\")\n    else:\n        checkpoint_dir = os.environ.get(\n            \"TORCH_HOME\", os.path.expanduser(\"~/.cache/torch/hub/checkpoints\")\n        )\n\n    models = {\n        \"FastSAM-x.pt\": \"https://github.com/opengeos/datasets/releases/download/models/FastSAM-x.pt\",\n        \"FastSAM-s.pt\": \"https://github.com/opengeos/datasets/releases/download/models/FastSAM-s.pt\",\n    }\n\n    if model not in models:\n        raise ValueError(\n            f\"Model must be one of {list(models.keys())}, but got {model} instead.\"\n        )\n\n    model_path = os.path.join(checkpoint_dir, model)\n\n    if not os.path.exists(model_path):\n        print(f\"Downloading {model} to {model_path}...\")\n        download_file(models[model], model_path)\n\n    super().__init__(model, **kwargs)\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.box_prompt","title":"<code>box_prompt(self, bbox=None, bboxes=None, output=None, **kwargs)</code>","text":"<p>Segment the image with the box prompt. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L377</p> <p>Parameters:</p> Name Type Description Default <code>bbox</code> <code>list</code> <p>The bounding box. Defaults to None.</p> <code>None</code> <code>bboxes</code> <code>list</code> <p>A list of bounding boxes. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to save the output image. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def box_prompt(self, bbox=None, bboxes=None, output=None, **kwargs):\n    \"\"\"Segment the image with the box prompt. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L377\n\n    Args:\n        bbox (list, optional): The bounding box. Defaults to None.\n        bboxes (list, optional): A list of bounding boxes. Defaults to None.\n        output (str, optional): The path to save the output image. Defaults to None.\n    \"\"\"\n\n    prompt_process = self.prompt_process\n    ann = prompt_process.box_prompt(bbox, bboxes)\n    self.annotations = ann\n\n    if output is not None:\n        self.save_masks(output, **kwargs)\n    else:\n        return ann\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.everything_prompt","title":"<code>everything_prompt(self, output=None, **kwargs)</code>","text":"<p>Segment the image with the everything prompt. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L451</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to save the output image. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def everything_prompt(self, output=None, **kwargs):\n    \"\"\"Segment the image with the everything prompt. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L451\n\n    Args:\n        output (str, optional): The path to save the output image. Defaults to None.\n    \"\"\"\n\n    prompt_process = self.prompt_process\n    ann = prompt_process.everything_prompt()\n    self.annotations = ann\n\n    if output is not None:\n        self.save_masks(output, **kwargs)\n    else:\n        return ann\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.fast_show_mask","title":"<code>fast_show_mask(self, random_color=False)</code>","text":"<p>Show the mask of the image. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222</p> <p>Parameters:</p> Name Type Description Default <code>random_color</code> <code>bool</code> <p>Whether to use random colors for each object. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The mask of the image.</p> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def fast_show_mask(\n    self,\n    random_color=False,\n):\n    \"\"\"Show the mask of the image. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222\n\n    Args:\n        random_color (bool, optional): Whether to use random colors for each object. Defaults to False.\n\n    Returns:\n        np.ndarray: The mask of the image.\n    \"\"\"\n\n    target_height = self.image.shape[0]\n    target_width = self.image.shape[1]\n    annotations = self.annotations\n    annotation = np.array(annotations.cpu())\n\n    mask_sum = annotation.shape[0]\n    height = annotation.shape[1]\n    weight = annotation.shape[2]\n    # Sort annotations based on area.\n    areas = np.sum(annotation, axis=(1, 2))\n    sorted_indices = np.argsort(areas)\n    annotation = annotation[sorted_indices]\n\n    index = (annotation != 0).argmax(axis=0)\n    if random_color:\n        color = np.random.random((mask_sum, 1, 1, 3))\n    else:\n        color = np.ones((mask_sum, 1, 1, 3)) * np.array(\n            [30 / 255, 144 / 255, 255 / 255]\n        )\n    transparency = np.ones((mask_sum, 1, 1, 1)) * 0.6\n    visual = np.concatenate([color, transparency], axis=-1)\n    mask_image = np.expand_dims(annotation, -1) * visual\n\n    show = np.zeros((height, weight, 4))\n    h_indices, w_indices = np.meshgrid(\n        np.arange(height), np.arange(weight), indexing=\"ij\"\n    )\n    indices = (index[h_indices, w_indices], h_indices, w_indices, slice(None))\n    # Use vectorized indexing to update the values of 'show'.\n    show[h_indices, w_indices, :] = mask_image[indices]\n\n    show = cv2.resize(\n        show, (target_width, target_height), interpolation=cv2.INTER_NEAREST\n    )\n\n    return show\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.point_prompt","title":"<code>point_prompt(self, points, pointlabel, output=None, **kwargs)</code>","text":"<p>Segment the image with the point prompt. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L414</p> <p>Parameters:</p> Name Type Description Default <code>points</code> <code>list</code> <p>A list of points.</p> required <code>pointlabel</code> <code>list</code> <p>A list of labels for each point.</p> required <code>output</code> <code>str</code> <p>The path to save the output image. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def point_prompt(self, points, pointlabel, output=None, **kwargs):\n    \"\"\"Segment the image with the point prompt. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L414\n\n    Args:\n        points (list): A list of points.\n        pointlabel (list): A list of labels for each point.\n        output (str, optional): The path to save the output image. Defaults to None.\n    \"\"\"\n\n    prompt_process = self.prompt_process\n    ann = prompt_process.point_prompt(points, pointlabel)\n    self.annotations = ann\n\n    if output is not None:\n        self.save_masks(output, **kwargs)\n    else:\n        return ann\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.raster_to_vector","title":"<code>raster_to_vector(self, image, output, simplify_tolerance=None, dst_crs='EPSG:4326', **kwargs)</code>","text":"<p>Save the result to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def raster_to_vector(\n    self, image, output, simplify_tolerance=None, dst_crs=\"EPSG:4326\", **kwargs\n):\n    \"\"\"Save the result to a vector file.\n\n    Args:\n        image (str): The path to the image file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(\n        image,\n        output,\n        simplify_tolerance=simplify_tolerance,\n        dst_crs=dst_crs,\n        **kwargs,\n    )\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.save_masks","title":"<code>save_masks(self, output=None, better_quality=True, dtype=None, mask_multiplier=255, **kwargs)</code>","text":"<p>Save the mask of the image. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222</p> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The mask of the image.</p> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def save_masks(\n    self,\n    output=None,\n    better_quality=True,\n    dtype=None,\n    mask_multiplier=255,\n    **kwargs,\n) -&gt; np.ndarray:\n    \"\"\"Save the mask of the image. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L222\n\n    Returns:\n        np.ndarray: The mask of the image.\n    \"\"\"\n    annotations = self.annotations\n    if isinstance(annotations[0], dict):\n        annotations = [annotation[\"segmentation\"] for annotation in annotations]\n    image = self.prompt_process.img\n    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n    height = image.shape[0]\n    width = image.shape[1]\n\n    if better_quality:\n        if isinstance(annotations[0], torch.Tensor):\n            annotations = np.array(annotations.cpu())\n        for i, mask in enumerate(annotations):\n            mask = cv2.morphologyEx(\n                mask.astype(np.uint8), cv2.MORPH_CLOSE, np.ones((3, 3), np.uint8)\n            )\n            annotations[i] = cv2.morphologyEx(\n                mask.astype(np.uint8), cv2.MORPH_OPEN, np.ones((8, 8), np.uint8)\n            )\n    if self.device == \"cpu\":\n        annotations = np.array(annotations)\n\n    else:\n        if isinstance(annotations[0], np.ndarray):\n            annotations = torch.from_numpy(annotations)\n\n    if isinstance(annotations, torch.Tensor):\n        annotations = annotations.cpu().numpy()\n\n    if dtype is None:\n        # Set output image data type based on the number of objects\n        if len(annotations) &lt; 255:\n            dtype = np.uint8\n        elif len(annotations) &lt; 65535:\n            dtype = np.uint16\n        else:\n            dtype = np.uint32\n\n    masks = np.sum(annotations, axis=0)\n\n    masks = cv2.resize(masks, (width, height), interpolation=cv2.INTER_NEAREST)\n    masks[masks &gt; 0] = 1\n    masks = masks.astype(dtype) * mask_multiplier\n    self.objects = masks\n\n    if output is not None:  # Save the output image\n        array_to_image(self.objects, output, self.source, **kwargs)\n    else:\n        return masks\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.set_image","title":"<code>set_image(self, image, device=None, **kwargs)</code>","text":"<p>Set the input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file or a HTTP URL.</p> required <code>device</code> <code>str</code> <p>The device to use. Defaults to \"cuda\" if available, otherwise \"cpu\".</p> <code>None</code> <code>kwargs</code> <p>Additional keyword arguments to pass to the FastSAM model.</p> <code>{}</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def set_image(self, image, device=None, **kwargs):\n    \"\"\"Set the input image.\n\n    Args:\n        image (str): The path to the image file or a HTTP URL.\n        device (str, optional): The device to use. Defaults to \"cuda\" if available, otherwise \"cpu\".\n        kwargs: Additional keyword arguments to pass to the FastSAM model.\n    \"\"\"\n\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n    else:\n        self.source = None\n\n    # Use cuda if available\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        if device == \"cuda\":\n            torch.cuda.empty_cache()\n\n    everything_results = self(image, device=device, **kwargs)\n\n    self.prompt_process = FastSAMPrompt(image, everything_results, device=device)\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.show_anns","title":"<code>show_anns(self, output=None, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> required <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> required <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.35.</p> required <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> required Source code in <code>samgeo/fast_sam.py</code> <pre><code>def show_anns(\n    self,\n    output=None,\n    **kwargs,\n):\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n    \"\"\"\n\n    annotations = self.annotations\n    prompt_process = self.prompt_process\n\n    if output is None:\n        output = temp_file_path(\".png\")\n\n    prompt_process.plot(annotations, output, **kwargs)\n\n    show_image(output)\n</code></pre>"},{"location":"fast_sam/#samgeo.fast_sam.SamGeo.text_prompt","title":"<code>text_prompt(self, text, output=None, **kwargs)</code>","text":"<p>Segment the image with the text prompt. Adapted from https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L439</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text to segment.</p> required <code>output</code> <code>str</code> <p>The path to save the output image. Defaults to None.</p> <code>None</code> Source code in <code>samgeo/fast_sam.py</code> <pre><code>def text_prompt(self, text, output=None, **kwargs):\n    \"\"\"Segment the image with the text prompt. Adapted from\n    https://github.com/CASIA-IVA-Lab/FastSAM/blob/main/fastsam/prompt.py#L439\n\n    Args:\n        text (str): The text to segment.\n        output (str, optional): The path to save the output image. Defaults to None.\n    \"\"\"\n\n    prompt_process = self.prompt_process\n    ann = prompt_process.text_prompt(text)\n    self.annotations = ann\n\n    if output is not None:\n        self.save_masks(output, **kwargs)\n    else:\n        return ann\n</code></pre>"},{"location":"hq_sam/","title":"hq_sam module","text":"<p>Segment remote sensing imagery with HQ-SAM (High Quality Segment Anything Model). See https://github.com/SysCV/sam-hq</p>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo","title":"<code> SamGeo        </code>","text":"<p>The main class for segmenting geospatial data with the Segment Anything Model (SAM). See https://github.com/facebookresearch/segment-anything for details.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>class SamGeo:\n    \"\"\"The main class for segmenting geospatial data with the Segment Anything Model (SAM). See\n    https://github.com/facebookresearch/segment-anything for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type=\"vit_h\",\n        automatic=True,\n        device=None,\n        checkpoint_dir=None,\n        hq=False,\n        sam_kwargs=None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the class.\n\n        Args:\n            model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n                Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n            automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n                The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n            device (str, optional): The device to use. It can be one of the following: cpu, cuda.\n                Defaults to None, which will use cuda if available.\n            hq (bool, optional): Whether to use the HQ-SAM model. Defaults to False.\n            checkpoint_dir (str, optional): The path to the model checkpoint. It can be one of the following:\n                sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n                Defaults to None. See https://bit.ly/3VrpxUh for more details.\n            sam_kwargs (dict, optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n                The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.\n\n                points_per_side: Optional[int] = 32,\n                points_per_batch: int = 64,\n                pred_iou_thresh: float = 0.88,\n                stability_score_thresh: float = 0.95,\n                stability_score_offset: float = 1.0,\n                box_nms_thresh: float = 0.7,\n                crop_n_layers: int = 0,\n                crop_nms_thresh: float = 0.7,\n                crop_overlap_ratio: float = 512 / 1500,\n                crop_n_points_downscale_factor: int = 1,\n                point_grids: Optional[List[np.ndarray]] = None,\n                min_mask_region_area: int = 0,\n                output_mode: str = \"binary_mask\",\n\n        \"\"\"\n\n        hq = True  # Using HQ-SAM\n        if \"checkpoint\" in kwargs:\n            checkpoint = kwargs[\"checkpoint\"]\n            if not os.path.exists(checkpoint):\n                checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n            kwargs.pop(\"checkpoint\")\n        else:\n            checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n\n        # Use cuda if available\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n\n        self.checkpoint = checkpoint\n        self.model_type = model_type\n        self.device = device\n        self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n        self.source = None  # Store the input image path\n        self.image = None  # Store the input image as a numpy array\n        # Store the masks as a list of dictionaries. Each mask is a dictionary\n        # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n        self.masks = None\n        self.objects = None  # Store the mask objects as a numpy array\n        # Store the annotations (objects with random color) as a numpy array.\n        self.annotations = None\n\n        # Store the predicted masks, iou_predictions, and low_res_masks\n        self.prediction = None\n        self.scores = None\n        self.logits = None\n\n        # Build the SAM model\n        self.sam = sam_model_registry[self.model_type](checkpoint=self.checkpoint)\n        self.sam.to(device=self.device)\n        # Use optional arguments for fine-tuning the SAM model\n        sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n        if automatic:\n            # Segment the entire image using the automatic mask generator\n            self.mask_generator = SamAutomaticMaskGenerator(self.sam, **sam_kwargs)\n        else:\n            # Segment selected objects using input prompts\n            self.predictor = SamPredictor(self.sam, **sam_kwargs)\n\n    def __call__(\n        self,\n        image,\n        foreground=True,\n        erosion_kernel=(3, 3),\n        mask_multiplier=255,\n        **kwargs,\n    ):\n        \"\"\"Generate masks for the input tile. This function originates from the segment-anything-eo repository.\n            See https://bit.ly/41pwiHw\n\n        Args:\n            image (np.ndarray): The input image as a numpy array.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n        \"\"\"\n        h, w, _ = image.shape\n\n        masks = self.mask_generator.generate(image)\n\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=np.uint8)\n        else:\n            resulting_mask = np.ones((h, w), dtype=np.uint8)\n        resulting_borders = np.zeros((h, w), dtype=np.uint8)\n\n        for m in masks:\n            mask = (m[\"segmentation\"] &gt; 0).astype(np.uint8)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(np.uint8)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(np.uint8)\n        resulting_borders = (resulting_borders &gt; 0).astype(np.uint8)\n        resulting_mask_with_borders = resulting_mask - resulting_borders\n        return resulting_mask_with_borders * mask_multiplier\n\n    def generate(\n        self,\n        source,\n        output=None,\n        foreground=True,\n        batch=False,\n        erosion_kernel=None,\n        mask_multiplier=255,\n        unique=True,\n        **kwargs,\n    ):\n        \"\"\"Generate masks for the input image.\n\n        Args:\n            source (str | np.ndarray): The path to the input image or the input image as a numpy array.\n            output (str, optional): The path to the output image. Defaults to None.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            batch (bool, optional): Whether to generate masks for a batch of image tiles. Defaults to False.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n                The parameter is ignored if unique is True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n                The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n\n        \"\"\"\n\n        if isinstance(source, str):\n            if source.startswith(\"http\"):\n                source = download_file(source)\n\n            if not os.path.exists(source):\n                raise ValueError(f\"Input path {source} does not exist.\")\n\n            if batch:  # Subdivide the image into tiles and segment each tile\n                self.batch = True\n                self.source = source\n                self.masks = output\n                return tiff_to_tiff(\n                    source,\n                    output,\n                    self,\n                    foreground=foreground,\n                    erosion_kernel=erosion_kernel,\n                    mask_multiplier=mask_multiplier,\n                    **kwargs,\n                )\n\n            image = cv2.imread(source)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif isinstance(source, np.ndarray):\n            image = source\n            source = None\n        else:\n            raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n        self.source = source  # Store the input image path\n        self.image = image  # Store the input image as a numpy array\n        mask_generator = self.mask_generator  # The automatic mask generator\n        masks = mask_generator.generate(image)  # Segment the input image\n        self.masks = masks  # Store the masks as a list of dictionaries\n        self.batch = False\n\n        if output is not None:\n            # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n            self.save_masks(\n                output, foreground, unique, erosion_kernel, mask_multiplier, **kwargs\n            )\n\n    def save_masks(\n        self,\n        output=None,\n        foreground=True,\n        unique=True,\n        erosion_kernel=None,\n        mask_multiplier=255,\n        **kwargs,\n    ):\n        \"\"\"Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n        Args:\n            output (str, optional): The path to the output image. Defaults to None, saving the masks to SamGeo.objects.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n\n        \"\"\"\n\n        if self.masks is None:\n            raise ValueError(\"No masks found. Please run generate() first.\")\n\n        h, w, _ = self.image.shape\n        masks = self.masks\n\n        # Set output image data type based on the number of objects\n        if len(masks) &lt; 255:\n            dtype = np.uint8\n        elif len(masks) &lt; 65535:\n            dtype = np.uint16\n        else:\n            dtype = np.uint32\n\n        # Generate a mask of objects with unique values\n        if unique:\n            # Sort the masks by area in ascending order\n            sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=False)\n\n            # Create an output image with the same size as the input image\n            objects = np.zeros(\n                (\n                    sorted_masks[0][\"segmentation\"].shape[0],\n                    sorted_masks[0][\"segmentation\"].shape[1],\n                )\n            )\n            # Assign a unique value to each object\n            for index, ann in enumerate(sorted_masks):\n                m = ann[\"segmentation\"]\n                objects[m] = index + 1\n\n        # Generate a binary mask\n        else:\n            if foreground:  # Extract foreground objects only\n                resulting_mask = np.zeros((h, w), dtype=dtype)\n            else:\n                resulting_mask = np.ones((h, w), dtype=dtype)\n            resulting_borders = np.zeros((h, w), dtype=dtype)\n\n            for m in masks:\n                mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n                resulting_mask += mask\n\n                # Apply erosion to the mask\n                if erosion_kernel is not None:\n                    mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                    mask_erode = (mask_erode &gt; 0).astype(dtype)\n                    edge_mask = mask - mask_erode\n                    resulting_borders += edge_mask\n\n            resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n            resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n            objects = resulting_mask - resulting_borders\n            objects = objects * mask_multiplier\n\n        objects = objects.astype(dtype)\n        self.objects = objects\n\n        if output is not None:  # Save the output image\n            array_to_image(self.objects, output, self.source, **kwargs)\n\n    def show_masks(\n        self, figsize=(12, 10), cmap=\"binary_r\", axis=\"off\", foreground=True, **kwargs\n    ):\n        \"\"\"Show the binary mask or the mask of objects with unique values.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            cmap (str, optional): The colormap. Defaults to \"binary_r\".\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n            **kwargs: Other arguments for save_masks().\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        if self.batch:\n            self.objects = cv2.imread(self.masks)\n        else:\n            if self.objects is None:\n                self.save_masks(foreground=foreground, **kwargs)\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.objects, cmap=cmap)\n        plt.axis(axis)\n        plt.show()\n\n    def show_anns(\n        self,\n        figsize=(12, 10),\n        axis=\"off\",\n        alpha=0.35,\n        output=None,\n        blend=True,\n        **kwargs,\n    ):\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        anns = self.masks\n\n        if self.image is None:\n            print(\"Please run generate() first.\")\n            return\n\n        if anns is None or len(anns) == 0:\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n        ax = plt.gca()\n        ax.set_autoscale_on(False)\n\n        img = np.ones(\n            (\n                sorted_anns[0][\"segmentation\"].shape[0],\n                sorted_anns[0][\"segmentation\"].shape[1],\n                4,\n            )\n        )\n        img[:, :, 3] = 0\n        for ann in sorted_anns:\n            m = ann[\"segmentation\"]\n            color_mask = np.concatenate([np.random.random(3), [alpha]])\n            img[m] = color_mask\n        ax.imshow(img)\n\n        if \"dpi\" not in kwargs:\n            kwargs[\"dpi\"] = 100\n\n        if \"bbox_inches\" not in kwargs:\n            kwargs[\"bbox_inches\"] = \"tight\"\n\n        plt.axis(axis)\n\n        self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n        if output is not None:\n            if blend:\n                array = blend_images(\n                    self.annotations, self.image, alpha=alpha, show=False\n                )\n            else:\n                array = self.annotations\n            array_to_image(array, output, self.source)\n\n    def set_image(self, image, image_format=\"RGB\"):\n        \"\"\"Set the input image as a numpy array.\n\n        Args:\n            image (np.ndarray): The input image as a numpy array.\n            image_format (str, optional): The image format, can be RGB or BGR. Defaults to \"RGB\".\n        \"\"\"\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            image = cv2.imread(image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            self.image = image\n        elif isinstance(image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n        self.predictor.set_image(image, image_format=image_format)\n\n    def save_prediction(\n        self,\n        output,\n        index=None,\n        mask_multiplier=255,\n        dtype=np.float32,\n        vector=None,\n        simplify_tolerance=None,\n        **kwargs,\n    ):\n        \"\"\"Save the predicted mask to the output path.\n\n        Args:\n            output (str): The path to the output image.\n            index (int, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            vector (str, optional): The path to the output vector file. Defaults to None.\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n\n        \"\"\"\n        if self.scores is None:\n            raise ValueError(\"No predictions found. Please run predict() first.\")\n\n        if index is None:\n            index = self.scores.argmax(axis=0)\n\n        array = self.masks[index] * mask_multiplier\n        self.prediction = array\n        array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n        if vector is not None:\n            raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n\n    def predict(\n        self,\n        point_coords=None,\n        point_labels=None,\n        boxes=None,\n        point_crs=None,\n        mask_input=None,\n        multimask_output=True,\n        return_logits=False,\n        output=None,\n        index=None,\n        mask_multiplier=255,\n        dtype=\"float32\",\n        return_results=False,\n        **kwargs,\n    ):\n        \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n        Args:\n            point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n                model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n                dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n            point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n                point prompts. 1 indicates a foreground point and 0 indicates a background point.\n            point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n            boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n                model, in XYXY format.\n            mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n                multimask_output (bool, optional): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n            return_logits (bool, optional): If true, returns un-thresholded masks logits\n                instead of a binary mask.\n            output (str, optional): The path to the output image. Defaults to None.\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n        \"\"\"\n\n        if isinstance(boxes, str):\n            gdf = gpd.read_file(boxes)\n            if gdf.crs is not None:\n                gdf = gdf.to_crs(\"epsg:4326\")\n            boxes = gdf.geometry.bounds.values.tolist()\n        elif isinstance(boxes, dict):\n            import json\n\n            geojson = json.dumps(boxes)\n            gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n            boxes = gdf.geometry.bounds.values.tolist()\n\n        if isinstance(point_coords, str):\n            point_coords = vector_to_geojson(point_coords)\n\n        if isinstance(point_coords, dict):\n            point_coords = geojson_to_coords(point_coords)\n\n        if hasattr(self, \"point_coords\"):\n            point_coords = self.point_coords\n\n        if hasattr(self, \"point_labels\"):\n            point_labels = self.point_labels\n\n        if (point_crs is not None) and (point_coords is not None):\n            point_coords = coords_to_xy(self.source, point_coords, point_crs)\n\n        if isinstance(point_coords, list):\n            point_coords = np.array(point_coords)\n\n        if point_coords is not None:\n            if point_labels is None:\n                point_labels = [1] * len(point_coords)\n            elif isinstance(point_labels, int):\n                point_labels = [point_labels] * len(point_coords)\n\n        if isinstance(point_labels, list):\n            if len(point_labels) != len(point_coords):\n                if len(point_labels) == 1:\n                    point_labels = point_labels * len(point_coords)\n                else:\n                    raise ValueError(\n                        \"The length of point_labels must be equal to the length of point_coords.\"\n                    )\n            point_labels = np.array(point_labels)\n\n        predictor = self.predictor\n\n        input_boxes = None\n        if isinstance(boxes, list) and (point_crs is not None):\n            coords = bbox_to_xy(self.source, boxes, point_crs)\n            input_boxes = np.array(coords)\n            if isinstance(coords[0], int):\n                input_boxes = input_boxes[None, :]\n            else:\n                input_boxes = torch.tensor(input_boxes, device=self.device)\n                input_boxes = predictor.transform.apply_boxes_torch(\n                    input_boxes, self.image.shape[:2]\n                )\n        elif isinstance(boxes, list) and (point_crs is None):\n            input_boxes = np.array(boxes)\n            if isinstance(boxes[0], int):\n                input_boxes = input_boxes[None, :]\n\n        self.boxes = input_boxes\n\n        if (\n            boxes is None\n            or (len(boxes) == 1)\n            or (len(boxes) == 4 and isinstance(boxes[0], float))\n        ):\n            if isinstance(boxes, list) and isinstance(boxes[0], list):\n                boxes = boxes[0]\n            masks, scores, logits = predictor.predict(\n                point_coords,\n                point_labels,\n                input_boxes,\n                mask_input,\n                multimask_output,\n                return_logits,\n            )\n        else:\n            masks, scores, logits = predictor.predict_torch(\n                point_coords=point_coords,\n                point_labels=point_coords,\n                boxes=input_boxes,\n                multimask_output=True,\n            )\n\n        self.masks = masks\n        self.scores = scores\n        self.logits = logits\n\n        if output is not None:\n            if boxes is None or (not isinstance(boxes[0], list)):\n                self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n            else:\n                self.tensor_to_numpy(\n                    index, output, mask_multiplier, dtype, save_args=kwargs\n                )\n\n        if return_results:\n            return masks, scores, logits\n\n    def tensor_to_numpy(\n        self, index=None, output=None, mask_multiplier=255, dtype=\"uint8\", save_args={}\n    ):\n        \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n        Args:\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            output (str, optional): The path to the output image. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.uint8.\n            save_args (dict, optional): Optional arguments for saving the output image. Defaults to {}.\n\n        Returns:\n            np.ndarray: The predicted mask as a numpy array.\n        \"\"\"\n\n        boxes = self.boxes\n        masks = self.masks\n\n        image_pil = self.image\n        image_np = np.array(image_pil)\n\n        if index is None:\n            index = 1\n\n        masks = masks[:, index, :, :]\n        masks = masks.squeeze(1)\n\n        if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n            print(\"No objects found in the image.\")\n            return\n        else:\n            # Create an empty image to store the mask overlays\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Adjusted for single channel\n\n            for i, (box, mask) in enumerate(zip(boxes, masks)):\n                # Convert tensor to numpy array if necessary and ensure it contains integers\n                if isinstance(mask, torch.Tensor):\n                    mask = (\n                        mask.cpu().numpy().astype(dtype)\n                    )  # If mask is on GPU, use .cpu() before .numpy()\n                mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                    dtype\n                )  # Assign a unique value for each mask\n\n            # Normalize mask_overlay to be in [0, 255]\n            mask_overlay = (\n                mask_overlay &gt; 0\n            ) * mask_multiplier  # Binary mask in [0, 255]\n\n        if output is not None:\n            array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n        else:\n            return mask_overlay\n\n    def show_map(self, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n        \"\"\"Show the interactive map.\n\n        Args:\n            basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n            repeat_mode (bool, optional): Whether to use the repeat mode for draw control. Defaults to True.\n            out_dir (str, optional): The path to the output directory. Defaults to None.\n\n        Returns:\n            leafmap.Map: The map object.\n        \"\"\"\n        return sam_map_gui(\n            self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n        )\n\n    def show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n        \"\"\"Show a canvas to collect foreground and background points.\n\n        Args:\n            image (str | np.ndarray): The input image.\n            fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n            bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n            radius (int, optional): The radius of the points. Defaults to 5.\n\n        Returns:\n            tuple: A tuple of two lists of foreground and background points.\n        \"\"\"\n\n        if self.image is None:\n            raise ValueError(\"Please run set_image() first.\")\n\n        image = self.image\n        fg_points, bg_points = show_canvas(image, fg_color, bg_color, radius)\n        self.fg_points = fg_points\n        self.bg_points = bg_points\n        point_coords = fg_points + bg_points\n        point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n        self.point_coords = point_coords\n        self.point_labels = point_labels\n\n    def clear_cuda_cache(self):\n        \"\"\"Clear the CUDA cache.\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    def image_to_image(self, image, **kwargs):\n        return image_to_image(image, self, **kwargs)\n\n    def download_tms_as_tiff(self, source, pt1, pt2, zoom, dist):\n        image = draw_tile(source, pt1[0], pt1[1], pt2[0], pt2[1], zoom, dist)\n        return image\n\n    def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Save the result to a vector file.\n\n        Args:\n            image (str): The path to the image file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n\n    def tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a gpkg file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a gpkg file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the gpkg file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_gpkg(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a shapefile.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the shapefile.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_shp(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a GeoJSON file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the GeoJSON file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_geojson(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.__call__","title":"<code>__call__(self, image, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255, **kwargs)</code>  <code>special</code>","text":"<p>Generate masks for the input tile. This function originates from the segment-anything-eo repository.     See https://bit.ly/41pwiHw</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p>The input image as a numpy array.</p> required <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).</p> <code>(3, 3)</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def __call__(\n    self,\n    image,\n    foreground=True,\n    erosion_kernel=(3, 3),\n    mask_multiplier=255,\n    **kwargs,\n):\n    \"\"\"Generate masks for the input tile. This function originates from the segment-anything-eo repository.\n        See https://bit.ly/41pwiHw\n\n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n    \"\"\"\n    h, w, _ = image.shape\n\n    masks = self.mask_generator.generate(image)\n\n    if foreground:  # Extract foreground objects only\n        resulting_mask = np.zeros((h, w), dtype=np.uint8)\n    else:\n        resulting_mask = np.ones((h, w), dtype=np.uint8)\n    resulting_borders = np.zeros((h, w), dtype=np.uint8)\n\n    for m in masks:\n        mask = (m[\"segmentation\"] &gt; 0).astype(np.uint8)\n        resulting_mask += mask\n\n        # Apply erosion to the mask\n        if erosion_kernel is not None:\n            mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n            mask_erode = (mask_erode &gt; 0).astype(np.uint8)\n            edge_mask = mask - mask_erode\n            resulting_borders += edge_mask\n\n    resulting_mask = (resulting_mask &gt; 0).astype(np.uint8)\n    resulting_borders = (resulting_borders &gt; 0).astype(np.uint8)\n    resulting_mask_with_borders = resulting_mask - resulting_borders\n    return resulting_mask_with_borders * mask_multiplier\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.__init__","title":"<code>__init__(self, model_type='vit_h', automatic=True, device=None, checkpoint_dir=None, hq=False, sam_kwargs=None, **kwargs)</code>  <code>special</code>","text":"<p>Initialize the class.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. It can be one of the following: vit_h, vit_l, vit_b. Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> <code>automatic</code> <code>bool</code> <p>Whether to use the automatic mask generator or input prompts. Defaults to True. The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.</p> <code>True</code> <code>device</code> <code>str</code> <p>The device to use. It can be one of the following: cpu, cuda. Defaults to None, which will use cuda if available.</p> <code>None</code> <code>hq</code> <code>bool</code> <p>Whether to use the HQ-SAM model. Defaults to False.</p> <code>False</code> <code>checkpoint_dir</code> <code>str</code> <p>The path to the model checkpoint. It can be one of the following: sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth. Defaults to None. See https://bit.ly/3VrpxUh for more details.</p> <code>None</code> <code>sam_kwargs</code> <code>dict</code> <p>Optional arguments for fine-tuning the SAM model. Defaults to None. The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.</p> <p>points_per_side: Optional[int] = 32, points_per_batch: int = 64, pred_iou_thresh: float = 0.88, stability_score_thresh: float = 0.95, stability_score_offset: float = 1.0, box_nms_thresh: float = 0.7, crop_n_layers: int = 0, crop_nms_thresh: float = 0.7, crop_overlap_ratio: float = 512 / 1500, crop_n_points_downscale_factor: int = 1, point_grids: Optional[List[np.ndarray]] = None, min_mask_region_area: int = 0, output_mode: str = \"binary_mask\",</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def __init__(\n    self,\n    model_type=\"vit_h\",\n    automatic=True,\n    device=None,\n    checkpoint_dir=None,\n    hq=False,\n    sam_kwargs=None,\n    **kwargs,\n):\n    \"\"\"Initialize the class.\n\n    Args:\n        model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n            The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n        device (str, optional): The device to use. It can be one of the following: cpu, cuda.\n            Defaults to None, which will use cuda if available.\n        hq (bool, optional): Whether to use the HQ-SAM model. Defaults to False.\n        checkpoint_dir (str, optional): The path to the model checkpoint. It can be one of the following:\n            sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n            Defaults to None. See https://bit.ly/3VrpxUh for more details.\n        sam_kwargs (dict, optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n            The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.\n\n            points_per_side: Optional[int] = 32,\n            points_per_batch: int = 64,\n            pred_iou_thresh: float = 0.88,\n            stability_score_thresh: float = 0.95,\n            stability_score_offset: float = 1.0,\n            box_nms_thresh: float = 0.7,\n            crop_n_layers: int = 0,\n            crop_nms_thresh: float = 0.7,\n            crop_overlap_ratio: float = 512 / 1500,\n            crop_n_points_downscale_factor: int = 1,\n            point_grids: Optional[List[np.ndarray]] = None,\n            min_mask_region_area: int = 0,\n            output_mode: str = \"binary_mask\",\n\n    \"\"\"\n\n    hq = True  # Using HQ-SAM\n    if \"checkpoint\" in kwargs:\n        checkpoint = kwargs[\"checkpoint\"]\n        if not os.path.exists(checkpoint):\n            checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n        kwargs.pop(\"checkpoint\")\n    else:\n        checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n\n    # Use cuda if available\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        if device == \"cuda\":\n            torch.cuda.empty_cache()\n\n    self.checkpoint = checkpoint\n    self.model_type = model_type\n    self.device = device\n    self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n    self.source = None  # Store the input image path\n    self.image = None  # Store the input image as a numpy array\n    # Store the masks as a list of dictionaries. Each mask is a dictionary\n    # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n    self.masks = None\n    self.objects = None  # Store the mask objects as a numpy array\n    # Store the annotations (objects with random color) as a numpy array.\n    self.annotations = None\n\n    # Store the predicted masks, iou_predictions, and low_res_masks\n    self.prediction = None\n    self.scores = None\n    self.logits = None\n\n    # Build the SAM model\n    self.sam = sam_model_registry[self.model_type](checkpoint=self.checkpoint)\n    self.sam.to(device=self.device)\n    # Use optional arguments for fine-tuning the SAM model\n    sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n    if automatic:\n        # Segment the entire image using the automatic mask generator\n        self.mask_generator = SamAutomaticMaskGenerator(self.sam, **sam_kwargs)\n    else:\n        # Segment selected objects using input prompts\n        self.predictor = SamPredictor(self.sam, **sam_kwargs)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.clear_cuda_cache","title":"<code>clear_cuda_cache(self)</code>","text":"<p>Clear the CUDA cache.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def clear_cuda_cache(self):\n    \"\"\"Clear the CUDA cache.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.generate","title":"<code>generate(self, source, output=None, foreground=True, batch=False, erosion_kernel=None, mask_multiplier=255, unique=True, **kwargs)</code>","text":"<p>Generate masks for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | np.ndarray</code> <p>The path to the input image or the input image as a numpy array.</p> required <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>batch</code> <code>bool</code> <p>Whether to generate masks for a batch of image tiles. Defaults to False.</p> <code>False</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255. The parameter is ignored if unique is True.</p> <code>255</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True. The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.</p> <code>True</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def generate(\n    self,\n    source,\n    output=None,\n    foreground=True,\n    batch=False,\n    erosion_kernel=None,\n    mask_multiplier=255,\n    unique=True,\n    **kwargs,\n):\n    \"\"\"Generate masks for the input image.\n\n    Args:\n        source (str | np.ndarray): The path to the input image or the input image as a numpy array.\n        output (str, optional): The path to the output image. Defaults to None.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        batch (bool, optional): Whether to generate masks for a batch of image tiles. Defaults to False.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n            The parameter is ignored if unique is True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n\n    \"\"\"\n\n    if isinstance(source, str):\n        if source.startswith(\"http\"):\n            source = download_file(source)\n\n        if not os.path.exists(source):\n            raise ValueError(f\"Input path {source} does not exist.\")\n\n        if batch:  # Subdivide the image into tiles and segment each tile\n            self.batch = True\n            self.source = source\n            self.masks = output\n            return tiff_to_tiff(\n                source,\n                output,\n                self,\n                foreground=foreground,\n                erosion_kernel=erosion_kernel,\n                mask_multiplier=mask_multiplier,\n                **kwargs,\n            )\n\n        image = cv2.imread(source)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    elif isinstance(source, np.ndarray):\n        image = source\n        source = None\n    else:\n        raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n    self.source = source  # Store the input image path\n    self.image = image  # Store the input image as a numpy array\n    mask_generator = self.mask_generator  # The automatic mask generator\n    masks = mask_generator.generate(image)  # Segment the input image\n    self.masks = masks  # Store the masks as a list of dictionaries\n    self.batch = False\n\n    if output is not None:\n        # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n        self.save_masks(\n            output, foreground, unique, erosion_kernel, mask_multiplier, **kwargs\n        )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.predict","title":"<code>predict(self, point_coords=None, point_labels=None, boxes=None, point_crs=None, mask_input=None, multimask_output=True, return_logits=False, output=None, index=None, mask_multiplier=255, dtype='float32', return_results=False, **kwargs)</code>","text":"<p>Predict masks for the given input prompts, using the currently set image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>str | dict | list | np.ndarray</code> <p>A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.</p> <code>None</code> <code>point_labels</code> <code>list | int | np.ndarray</code> <p>A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>point_crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>boxes</code> <code>list | np.ndarray</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> <code>None</code> <code>mask_input</code> <code>np.ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256. multimask_output (bool, optional): If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>None</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>np.dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>'float32'</code> <code>return_results</code> <code>bool</code> <p>Whether to return the predicted masks, scores, and logits. Defaults to False.</p> <code>False</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def predict(\n    self,\n    point_coords=None,\n    point_labels=None,\n    boxes=None,\n    point_crs=None,\n    mask_input=None,\n    multimask_output=True,\n    return_logits=False,\n    output=None,\n    index=None,\n    mask_multiplier=255,\n    dtype=\"float32\",\n    return_results=False,\n    **kwargs,\n):\n    \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n    Args:\n        point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n            model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n            dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n        point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n            point prompts. 1 indicates a foreground point and 0 indicates a background point.\n        point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n        boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n            model, in XYXY format.\n        mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n            multimask_output (bool, optional): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n        return_logits (bool, optional): If true, returns un-thresholded masks logits\n            instead of a binary mask.\n        output (str, optional): The path to the output image. Defaults to None.\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n    \"\"\"\n\n    if isinstance(boxes, str):\n        gdf = gpd.read_file(boxes)\n        if gdf.crs is not None:\n            gdf = gdf.to_crs(\"epsg:4326\")\n        boxes = gdf.geometry.bounds.values.tolist()\n    elif isinstance(boxes, dict):\n        import json\n\n        geojson = json.dumps(boxes)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        boxes = gdf.geometry.bounds.values.tolist()\n\n    if isinstance(point_coords, str):\n        point_coords = vector_to_geojson(point_coords)\n\n    if isinstance(point_coords, dict):\n        point_coords = geojson_to_coords(point_coords)\n\n    if hasattr(self, \"point_coords\"):\n        point_coords = self.point_coords\n\n    if hasattr(self, \"point_labels\"):\n        point_labels = self.point_labels\n\n    if (point_crs is not None) and (point_coords is not None):\n        point_coords = coords_to_xy(self.source, point_coords, point_crs)\n\n    if isinstance(point_coords, list):\n        point_coords = np.array(point_coords)\n\n    if point_coords is not None:\n        if point_labels is None:\n            point_labels = [1] * len(point_coords)\n        elif isinstance(point_labels, int):\n            point_labels = [point_labels] * len(point_coords)\n\n    if isinstance(point_labels, list):\n        if len(point_labels) != len(point_coords):\n            if len(point_labels) == 1:\n                point_labels = point_labels * len(point_coords)\n            else:\n                raise ValueError(\n                    \"The length of point_labels must be equal to the length of point_coords.\"\n                )\n        point_labels = np.array(point_labels)\n\n    predictor = self.predictor\n\n    input_boxes = None\n    if isinstance(boxes, list) and (point_crs is not None):\n        coords = bbox_to_xy(self.source, boxes, point_crs)\n        input_boxes = np.array(coords)\n        if isinstance(coords[0], int):\n            input_boxes = input_boxes[None, :]\n        else:\n            input_boxes = torch.tensor(input_boxes, device=self.device)\n            input_boxes = predictor.transform.apply_boxes_torch(\n                input_boxes, self.image.shape[:2]\n            )\n    elif isinstance(boxes, list) and (point_crs is None):\n        input_boxes = np.array(boxes)\n        if isinstance(boxes[0], int):\n            input_boxes = input_boxes[None, :]\n\n    self.boxes = input_boxes\n\n    if (\n        boxes is None\n        or (len(boxes) == 1)\n        or (len(boxes) == 4 and isinstance(boxes[0], float))\n    ):\n        if isinstance(boxes, list) and isinstance(boxes[0], list):\n            boxes = boxes[0]\n        masks, scores, logits = predictor.predict(\n            point_coords,\n            point_labels,\n            input_boxes,\n            mask_input,\n            multimask_output,\n            return_logits,\n        )\n    else:\n        masks, scores, logits = predictor.predict_torch(\n            point_coords=point_coords,\n            point_labels=point_coords,\n            boxes=input_boxes,\n            multimask_output=True,\n        )\n\n    self.masks = masks\n    self.scores = scores\n    self.logits = logits\n\n    if output is not None:\n        if boxes is None or (not isinstance(boxes[0], list)):\n            self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n        else:\n            self.tensor_to_numpy(\n                index, output, mask_multiplier, dtype, save_args=kwargs\n            )\n\n    if return_results:\n        return masks, scores, logits\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.raster_to_vector","title":"<code>raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the result to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Save the result to a vector file.\n\n    Args:\n        image (str): The path to the image file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.save_masks","title":"<code>save_masks(self, output=None, foreground=True, unique=True, erosion_kernel=None, mask_multiplier=255, **kwargs)</code>","text":"<p>Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None, saving the masks to SamGeo.objects.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def save_masks(\n    self,\n    output=None,\n    foreground=True,\n    unique=True,\n    erosion_kernel=None,\n    mask_multiplier=255,\n    **kwargs,\n):\n    \"\"\"Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n    Args:\n        output (str, optional): The path to the output image. Defaults to None, saving the masks to SamGeo.objects.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n\n    \"\"\"\n\n    if self.masks is None:\n        raise ValueError(\"No masks found. Please run generate() first.\")\n\n    h, w, _ = self.image.shape\n    masks = self.masks\n\n    # Set output image data type based on the number of objects\n    if len(masks) &lt; 255:\n        dtype = np.uint8\n    elif len(masks) &lt; 65535:\n        dtype = np.uint16\n    else:\n        dtype = np.uint32\n\n    # Generate a mask of objects with unique values\n    if unique:\n        # Sort the masks by area in ascending order\n        sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=False)\n\n        # Create an output image with the same size as the input image\n        objects = np.zeros(\n            (\n                sorted_masks[0][\"segmentation\"].shape[0],\n                sorted_masks[0][\"segmentation\"].shape[1],\n            )\n        )\n        # Assign a unique value to each object\n        for index, ann in enumerate(sorted_masks):\n            m = ann[\"segmentation\"]\n            objects[m] = index + 1\n\n    # Generate a binary mask\n    else:\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=dtype)\n        else:\n            resulting_mask = np.ones((h, w), dtype=dtype)\n        resulting_borders = np.zeros((h, w), dtype=dtype)\n\n        for m in masks:\n            mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(dtype)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n        resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n        objects = resulting_mask - resulting_borders\n        objects = objects * mask_multiplier\n\n    objects = objects.astype(dtype)\n    self.objects = objects\n\n    if output is not None:  # Save the output image\n        array_to_image(self.objects, output, self.source, **kwargs)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.save_prediction","title":"<code>save_prediction(self, output, index=None, mask_multiplier=255, dtype=&lt;class 'numpy.float32'&gt;, vector=None, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the predicted mask to the output path.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>index</code> <code>int</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>vector</code> <code>str</code> <p>The path to the output vector file. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>np.dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>&lt;class 'numpy.float32'&gt;</code> <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def save_prediction(\n    self,\n    output,\n    index=None,\n    mask_multiplier=255,\n    dtype=np.float32,\n    vector=None,\n    simplify_tolerance=None,\n    **kwargs,\n):\n    \"\"\"Save the predicted mask to the output path.\n\n    Args:\n        output (str): The path to the output image.\n        index (int, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        vector (str, optional): The path to the output vector file. Defaults to None.\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n\n    \"\"\"\n    if self.scores is None:\n        raise ValueError(\"No predictions found. Please run predict() first.\")\n\n    if index is None:\n        index = self.scores.argmax(axis=0)\n\n    array = self.masks[index] * mask_multiplier\n    self.prediction = array\n    array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n    if vector is not None:\n        raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.set_image","title":"<code>set_image(self, image, image_format='RGB')</code>","text":"<p>Set the input image as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p>The input image as a numpy array.</p> required <code>image_format</code> <code>str</code> <p>The image format, can be RGB or BGR. Defaults to \"RGB\".</p> <code>'RGB'</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def set_image(self, image, image_format=\"RGB\"):\n    \"\"\"Set the input image as a numpy array.\n\n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        image_format (str, optional): The image format, can be RGB or BGR. Defaults to \"RGB\".\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        image = cv2.imread(image)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        self.image = image\n    elif isinstance(image, np.ndarray):\n        pass\n    else:\n        raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n    self.predictor.set_image(image, image_format=image_format)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.show_anns","title":"<code>show_anns(self, figsize=(12, 10), axis='off', alpha=0.35, output=None, blend=True, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.35.</p> <code>0.35</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> <code>True</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def show_anns(\n    self,\n    figsize=(12, 10),\n    axis=\"off\",\n    alpha=0.35,\n    output=None,\n    blend=True,\n    **kwargs,\n):\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    anns = self.masks\n\n    if self.image is None:\n        print(\"Please run generate() first.\")\n        return\n\n    if anns is None or len(anns) == 0:\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones(\n        (\n            sorted_anns[0][\"segmentation\"].shape[0],\n            sorted_anns[0][\"segmentation\"].shape[1],\n            4,\n        )\n    )\n    img[:, :, 3] = 0\n    for ann in sorted_anns:\n        m = ann[\"segmentation\"]\n        color_mask = np.concatenate([np.random.random(3), [alpha]])\n        img[m] = color_mask\n    ax.imshow(img)\n\n    if \"dpi\" not in kwargs:\n        kwargs[\"dpi\"] = 100\n\n    if \"bbox_inches\" not in kwargs:\n        kwargs[\"bbox_inches\"] = \"tight\"\n\n    plt.axis(axis)\n\n    self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n    if output is not None:\n        if blend:\n            array = blend_images(\n                self.annotations, self.image, alpha=alpha, show=False\n            )\n        else:\n            array = self.annotations\n        array_to_image(array, output, self.source)\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.show_canvas","title":"<code>show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5)</code>","text":"<p>Show a canvas to collect foreground and background points.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str | np.ndarray</code> <p>The input image.</p> required <code>fg_color</code> <code>tuple</code> <p>The color for the foreground points. Defaults to (0, 255, 0).</p> <code>(0, 255, 0)</code> <code>bg_color</code> <code>tuple</code> <p>The color for the background points. Defaults to (0, 0, 255).</p> <code>(0, 0, 255)</code> <code>radius</code> <code>int</code> <p>The radius of the points. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple of two lists of foreground and background points.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n    \"\"\"Show a canvas to collect foreground and background points.\n\n    Args:\n        image (str | np.ndarray): The input image.\n        fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n        bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n        radius (int, optional): The radius of the points. Defaults to 5.\n\n    Returns:\n        tuple: A tuple of two lists of foreground and background points.\n    \"\"\"\n\n    if self.image is None:\n        raise ValueError(\"Please run set_image() first.\")\n\n    image = self.image\n    fg_points, bg_points = show_canvas(image, fg_color, bg_color, radius)\n    self.fg_points = fg_points\n    self.bg_points = bg_points\n    point_coords = fg_points + bg_points\n    point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n    self.point_coords = point_coords\n    self.point_labels = point_labels\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.show_map","title":"<code>show_map(self, basemap='SATELLITE', repeat_mode=True, out_dir=None, **kwargs)</code>","text":"<p>Show the interactive map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.</p> <code>'SATELLITE'</code> <code>repeat_mode</code> <code>bool</code> <p>Whether to use the repeat mode for draw control. Defaults to True.</p> <code>True</code> <code>out_dir</code> <code>str</code> <p>The path to the output directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>leafmap.Map</code> <p>The map object.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def show_map(self, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n    \"\"\"Show the interactive map.\n\n    Args:\n        basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n        repeat_mode (bool, optional): Whether to use the repeat mode for draw control. Defaults to True.\n        out_dir (str, optional): The path to the output directory. Defaults to None.\n\n    Returns:\n        leafmap.Map: The map object.\n    \"\"\"\n    return sam_map_gui(\n        self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.show_masks","title":"<code>show_masks(self, figsize=(12, 10), cmap='binary_r', axis='off', foreground=True, **kwargs)</code>","text":"<p>Show the binary mask or the mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>cmap</code> <code>str</code> <p>The colormap. Defaults to \"binary_r\".</p> <code>'binary_r'</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>foreground</code> <code>bool</code> <p>Whether to show the foreground mask only. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Other arguments for save_masks().</p> <code>{}</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def show_masks(\n    self, figsize=(12, 10), cmap=\"binary_r\", axis=\"off\", foreground=True, **kwargs\n):\n    \"\"\"Show the binary mask or the mask of objects with unique values.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        cmap (str, optional): The colormap. Defaults to \"binary_r\".\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n        **kwargs: Other arguments for save_masks().\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    if self.batch:\n        self.objects = cv2.imread(self.masks)\n    else:\n        if self.objects is None:\n            self.save_masks(foreground=foreground, **kwargs)\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.objects, cmap=cmap)\n    plt.axis(axis)\n    plt.show()\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tensor_to_numpy","title":"<code>tensor_to_numpy(self, index=None, output=None, mask_multiplier=255, dtype='uint8', save_args={})</code>","text":"<p>Convert the predicted masks from tensors to numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>np.dtype</code> <p>The data type of the output image. Defaults to np.uint8.</p> <code>'uint8'</code> <code>save_args</code> <code>dict</code> <p>Optional arguments for saving the output image. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The predicted mask as a numpy array.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tensor_to_numpy(\n    self, index=None, output=None, mask_multiplier=255, dtype=\"uint8\", save_args={}\n):\n    \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n    Args:\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        output (str, optional): The path to the output image. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.uint8.\n        save_args (dict, optional): Optional arguments for saving the output image. Defaults to {}.\n\n    Returns:\n        np.ndarray: The predicted mask as a numpy array.\n    \"\"\"\n\n    boxes = self.boxes\n    masks = self.masks\n\n    image_pil = self.image\n    image_np = np.array(image_pil)\n\n    if index is None:\n        index = 1\n\n    masks = masks[:, index, :, :]\n    masks = masks.squeeze(1)\n\n    if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n        print(\"No objects found in the image.\")\n        return\n    else:\n        # Create an empty image to store the mask overlays\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Adjusted for single channel\n\n        for i, (box, mask) in enumerate(zip(boxes, masks)):\n            # Convert tensor to numpy array if necessary and ensure it contains integers\n            if isinstance(mask, torch.Tensor):\n                mask = (\n                    mask.cpu().numpy().astype(dtype)\n                )  # If mask is on GPU, use .cpu() before .numpy()\n            mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                dtype\n            )  # Assign a unique value for each mask\n\n        # Normalize mask_overlay to be in [0, 255]\n        mask_overlay = (\n            mask_overlay &gt; 0\n        ) * mask_multiplier  # Binary mask in [0, 255]\n\n    if output is not None:\n        array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n    else:\n        return mask_overlay\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tiff_to_geojson","title":"<code>tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a GeoJSON file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the GeoJSON file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a GeoJSON file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the GeoJSON file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_geojson(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tiff_to_gpkg","title":"<code>tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the gpkg file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the gpkg file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_gpkg(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tiff_to_shp","title":"<code>tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the shapefile.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a shapefile.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the shapefile.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_shp(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeo.tiff_to_vector","title":"<code>tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeoPredictor","title":"<code> SamGeoPredictor            (SamPredictor)         </code>","text":"Source code in <code>samgeo/hq_sam.py</code> <pre><code>class SamGeoPredictor(SamPredictor):\n    def __init__(\n        self,\n        sam_model,\n    ):\n        from segment_anything.utils.transforms import ResizeLongestSide\n\n        self.model = sam_model\n        self.transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n\n    def set_image(self, image):\n        super(SamGeoPredictor, self).set_image(image)\n\n    def predict(\n        self,\n        src_fp=None,\n        geo_box=None,\n        point_coords=None,\n        point_labels=None,\n        box=None,\n        mask_input=None,\n        multimask_output=True,\n        return_logits=False,\n    ):\n        if geo_box and src_fp:\n            self.crs = \"EPSG:4326\"\n            dst_crs = get_crs(src_fp)\n            sw = transform_coords(geo_box[0], geo_box[1], self.crs, dst_crs)\n            ne = transform_coords(geo_box[2], geo_box[3], self.crs, dst_crs)\n            xs = np.array([sw[0], ne[0]])\n            ys = np.array([sw[1], ne[1]])\n            box = get_pixel_coords(src_fp, xs, ys)\n            self.geo_box = geo_box\n            self.width = box[2] - box[0]\n            self.height = box[3] - box[1]\n            self.geo_transform = set_transform(geo_box, self.width, self.height)\n\n        masks, iou_predictions, low_res_masks = super(SamGeoPredictor, self).predict(\n            point_coords, point_labels, box, mask_input, multimask_output, return_logits\n        )\n\n        return masks, iou_predictions, low_res_masks\n\n    def masks_to_geotiff(self, src_fp, dst_fp, masks):\n        profile = get_profile(src_fp)\n        write_raster(\n            dst_fp,\n            masks,\n            profile,\n            self.width,\n            self.height,\n            self.geo_transform,\n            self.crs,\n        )\n\n    def geotiff_to_geojson(self, src_fp, dst_fp, bidx=1):\n        gdf = get_features(src_fp, bidx)\n        write_features(gdf, dst_fp)\n        return gdf\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeoPredictor.predict","title":"<code>predict(self, src_fp=None, geo_box=None, point_coords=None, point_labels=None, box=None, mask_input=None, multimask_output=True, return_logits=False)</code>","text":"<p>Predict masks for the given input prompts, using the currently set image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>np.ndarray or None</code> <p>A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels.</p> <code>None</code> <code>point_labels</code> <code>np.ndarray or None</code> <p>A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>box</code> <code>np.ndarray or None</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> <code>None</code> <code>mask_input</code> <code>np.ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.</p> <code>None</code> <code>multimask_output</code> <code>bool</code> <p>If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>True</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <p>Returns:</p> Type Description <code>(np.ndarray)</code> <p>The output masks in CxHxW format, where C is the   number of masks, and (H, W) is the original image size. (np.ndarray): An array of length C containing the model's   predictions for the quality of each mask. (np.ndarray): An array of shape CxHxW, where C is the number   of masks and H=W=256. These low resolution logits can be passed to   a subsequent iteration as mask input.</p> Source code in <code>samgeo/hq_sam.py</code> <pre><code>def predict(\n    self,\n    src_fp=None,\n    geo_box=None,\n    point_coords=None,\n    point_labels=None,\n    box=None,\n    mask_input=None,\n    multimask_output=True,\n    return_logits=False,\n):\n    if geo_box and src_fp:\n        self.crs = \"EPSG:4326\"\n        dst_crs = get_crs(src_fp)\n        sw = transform_coords(geo_box[0], geo_box[1], self.crs, dst_crs)\n        ne = transform_coords(geo_box[2], geo_box[3], self.crs, dst_crs)\n        xs = np.array([sw[0], ne[0]])\n        ys = np.array([sw[1], ne[1]])\n        box = get_pixel_coords(src_fp, xs, ys)\n        self.geo_box = geo_box\n        self.width = box[2] - box[0]\n        self.height = box[3] - box[1]\n        self.geo_transform = set_transform(geo_box, self.width, self.height)\n\n    masks, iou_predictions, low_res_masks = super(SamGeoPredictor, self).predict(\n        point_coords, point_labels, box, mask_input, multimask_output, return_logits\n    )\n\n    return masks, iou_predictions, low_res_masks\n</code></pre>"},{"location":"hq_sam/#samgeo.hq_sam.SamGeoPredictor.set_image","title":"<code>set_image(self, image)</code>","text":"<p>Calculates the image embeddings for the provided image, allowing masks to be predicted with the 'predict' method.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p>The image for calculating masks. Expects an image in HWC uint8 format, with pixel values in [0, 255].</p> required <code>image_format</code> <code>str</code> <p>The color format of the image, in ['RGB', 'BGR'].</p> required Source code in <code>samgeo/hq_sam.py</code> <pre><code>def set_image(self, image):\n    super(SamGeoPredictor, self).set_image(image)\n</code></pre>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#install-from-pypi","title":"Install from PyPI","text":"<p>segment-geospatial is available on PyPI. To install segment-geospatial, run this command in your terminal:</p> <pre><code>pip install segment-geospatial\n</code></pre>"},{"location":"installation/#install-from-conda-forge","title":"Install from conda-forge","text":"<p>segment-geospatial is also available on conda-forge. If you have Anaconda or Miniconda installed on your computer, you can install segment-geospatial using the following commands. It is recommended to create a fresh conda environment for segment-geospatial. The following commands will create a new conda environment named <code>geo</code> and install segment-geospatial and its dependencies:</p> <pre><code>conda create -n geo python\nconda activate geo\nconda install -c conda-forge mamba\nmamba install -c conda-forge segment-geospatial\n</code></pre> <p>If your system has a GPU, but the above commands do not install the GPU version of pytorch, you can force the installation of the GPU version of pytorch using the following command:</p> <pre><code>mamba install -c conda-forge segment-geospatial \"pytorch=*=cuda*\"\n</code></pre> <p>Samgeo-geospatial has some optional dependencies that are not included in the default conda environment. To install these dependencies, run the following command:</p> <pre><code>mamba install -c conda-forge groundingdino-py segment-anything-fast\n</code></pre> <p>As of July 9th, 2023 Linux systems have also required that <code>libgl1</code> be installed for segment-geospatial to work. The following command will install that dependency</p> <pre><code>apt update; apt install -y libgl1\n</code></pre>"},{"location":"installation/#install-from-github","title":"Install from GitHub","text":"<p>To install the development version from GitHub using Git, run the following command in your terminal:</p> <pre><code>pip install git+https://github.com/opengeos/segment-geospatial\n</code></pre>"},{"location":"installation/#use-docker","title":"Use docker","text":"<p>You can also use docker to run segment-geospatial:</p> <pre><code>docker run -it -p 8888:8888 giswqs/segment-geospatial:latest\n</code></pre> <p>To enable GPU for segment-geospatial, run the following command to run a short benchmark on your GPU:</p> <pre><code>docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark\n</code></pre> <p>The output should be similar to the following:</p> <pre><code>Run \"nbody -benchmark [-numbodies=&lt;numBodies&gt;]\" to measure performance.\n        -fullscreen       (run n-body simulation in fullscreen mode)\n        -fp64             (use double precision floating point values for simulation)\n        -hostmem          (stores simulation data in host memory)\n        -benchmark        (run benchmark to measure performance)\n        -numbodies=&lt;N&gt;    (number of bodies (&gt;= 1) to run in simulation)\n        -device=&lt;d&gt;       (where d=0,1,2.... for the CUDA device to use)\n        -numdevices=&lt;i&gt;   (where i=(number of CUDA devices &gt; 0) to use for simulation)\n        -compare          (compares simulation results running once on the default GPU and once on the CPU)\n        -cpu              (run n-body simulation on the CPU)\n        -tipsy=&lt;file.bin&gt; (load a tipsy model file for simulation)\n\nNOTE: The CUDA Samples are not meant for performance measurements. Results may vary when GPU Boost is enabled.\n\n&gt; Windowed mode\n&gt; Simulation data stored in video memory\n&gt; Single precision floating point simulation\n&gt; 1 Devices used for simulation\nGPU Device 0: \"Turing\" with compute capability 7.5\n\n&gt; Compute 7.5 CUDA device: [Quadro RTX 5000]\n49152 bodies, total time for 10 iterations: 69.386 ms\n= 348.185 billion interactions per second\n= 6963.703 single-precision GFLOP/s at 20 flops per interaction\n</code></pre> <p>If you encounter the following error:</p> <pre><code>nvidia-container-cli: initialization error: load library failed: libnvidia-ml.so.1: cannot open shared object file: no such file or directory: unknown.\n</code></pre> <p>Try adding <code>sudo</code> to the command:</p> <pre><code>sudo docker run --rm -it --gpus=all nvcr.io/nvidia/k8s/cuda-sample:nbody nbody -gpu -benchmark\n</code></pre> <p>Once everything is working, you can run the following command to start a Jupyter Notebook server:</p> <pre><code>docker run -it -p 8888:8888 --gpus=all giswqs/segment-geospatial:latest\n</code></pre>"},{"location":"samgeo/","title":"samgeo module","text":"<p>The source code is adapted from https://github.com/aliaksandr960/segment-anything-eo. Credit to the author Aliaksandr Hancharenka.</p>"},{"location":"samgeo/#samgeo.samgeo.SamGeo","title":"<code> SamGeo        </code>","text":"<p>The main class for segmenting geospatial data with the Segment Anything Model (SAM). See https://github.com/facebookresearch/segment-anything for details.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>class SamGeo:\n    \"\"\"The main class for segmenting geospatial data with the Segment Anything Model (SAM). See\n    https://github.com/facebookresearch/segment-anything for details.\n    \"\"\"\n\n    def __init__(\n        self,\n        model_type=\"vit_h\",\n        automatic=True,\n        device=None,\n        checkpoint_dir=None,\n        sam_kwargs=None,\n        **kwargs,\n    ):\n        \"\"\"Initialize the class.\n\n        Args:\n            model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n                Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n            automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n                The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n            device (str, optional): The device to use. It can be one of the following: cpu, cuda.\n                Defaults to None, which will use cuda if available.\n            checkpoint_dir (str, optional): The path to the model checkpoint. It can be one of the following:\n                sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n                Defaults to None. See https://bit.ly/3VrpxUh for more details.\n            sam_kwargs (dict, optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n                The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.\n\n                points_per_side: Optional[int] = 32,\n                points_per_batch: int = 64,\n                pred_iou_thresh: float = 0.88,\n                stability_score_thresh: float = 0.95,\n                stability_score_offset: float = 1.0,\n                box_nms_thresh: float = 0.7,\n                crop_n_layers: int = 0,\n                crop_nms_thresh: float = 0.7,\n                crop_overlap_ratio: float = 512 / 1500,\n                crop_n_points_downscale_factor: int = 1,\n                point_grids: Optional[List[np.ndarray]] = None,\n                min_mask_region_area: int = 0,\n                output_mode: str = \"binary_mask\",\n\n        \"\"\"\n        hq = False  # Not using HQ-SAM\n\n        if \"checkpoint\" in kwargs:\n            checkpoint = kwargs[\"checkpoint\"]\n            if not os.path.exists(checkpoint):\n                checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n            kwargs.pop(\"checkpoint\")\n        else:\n            checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n\n        # Use cuda if available\n        if device is None:\n            device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n            if device == \"cuda\":\n                torch.cuda.empty_cache()\n\n        self.checkpoint = checkpoint\n        self.model_type = model_type\n        self.device = device\n        self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n        self.source = None  # Store the input image path\n        self.image = None  # Store the input image as a numpy array\n        # Store the masks as a list of dictionaries. Each mask is a dictionary\n        # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n        self.masks = None\n        self.objects = None  # Store the mask objects as a numpy array\n        # Store the annotations (objects with random color) as a numpy array.\n        self.annotations = None\n\n        # Store the predicted masks, iou_predictions, and low_res_masks\n        self.prediction = None\n        self.scores = None\n        self.logits = None\n\n        # Build the SAM model\n        self.sam = sam_model_registry[self.model_type](checkpoint=self.checkpoint)\n        self.sam.to(device=self.device)\n        # Use optional arguments for fine-tuning the SAM model\n        sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n        if automatic:\n            # Segment the entire image using the automatic mask generator\n            self.mask_generator = SamAutomaticMaskGenerator(self.sam, **sam_kwargs)\n        else:\n            # Segment selected objects using input prompts\n            self.predictor = SamPredictor(self.sam, **sam_kwargs)\n\n    def __call__(\n        self,\n        image,\n        foreground=True,\n        erosion_kernel=(3, 3),\n        mask_multiplier=255,\n        **kwargs,\n    ):\n        \"\"\"Generate masks for the input tile. This function originates from the segment-anything-eo repository.\n            See https://bit.ly/41pwiHw\n\n        Args:\n            image (np.ndarray): The input image as a numpy array.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n        \"\"\"\n        h, w, _ = image.shape\n\n        masks = self.mask_generator.generate(image)\n\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=np.uint8)\n        else:\n            resulting_mask = np.ones((h, w), dtype=np.uint8)\n        resulting_borders = np.zeros((h, w), dtype=np.uint8)\n\n        for m in masks:\n            mask = (m[\"segmentation\"] &gt; 0).astype(np.uint8)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(np.uint8)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(np.uint8)\n        resulting_borders = (resulting_borders &gt; 0).astype(np.uint8)\n        resulting_mask_with_borders = resulting_mask - resulting_borders\n        return resulting_mask_with_borders * mask_multiplier\n\n    def generate(\n        self,\n        source,\n        output=None,\n        foreground=True,\n        batch=False,\n        erosion_kernel=None,\n        mask_multiplier=255,\n        unique=True,\n        **kwargs,\n    ):\n        \"\"\"Generate masks for the input image.\n\n        Args:\n            source (str | np.ndarray): The path to the input image or the input image as a numpy array.\n            output (str, optional): The path to the output image. Defaults to None.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            batch (bool, optional): Whether to generate masks for a batch of image tiles. Defaults to False.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n                The parameter is ignored if unique is True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n                The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n\n        \"\"\"\n\n        if isinstance(source, str):\n            if source.startswith(\"http\"):\n                source = download_file(source)\n\n            if not os.path.exists(source):\n                raise ValueError(f\"Input path {source} does not exist.\")\n\n            if batch:  # Subdivide the image into tiles and segment each tile\n                self.batch = True\n                self.source = source\n                self.masks = output\n                return tiff_to_tiff(\n                    source,\n                    output,\n                    self,\n                    foreground=foreground,\n                    erosion_kernel=erosion_kernel,\n                    mask_multiplier=mask_multiplier,\n                    **kwargs,\n                )\n\n            image = cv2.imread(source)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        elif isinstance(source, np.ndarray):\n            image = source\n            source = None\n        else:\n            raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n        self.source = source  # Store the input image path\n        self.image = image  # Store the input image as a numpy array\n        mask_generator = self.mask_generator  # The automatic mask generator\n        masks = mask_generator.generate(image)  # Segment the input image\n        self.masks = masks  # Store the masks as a list of dictionaries\n        self.batch = False\n\n        if output is not None:\n            # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n            self.save_masks(\n                output, foreground, unique, erosion_kernel, mask_multiplier, **kwargs\n            )\n\n    def save_masks(\n        self,\n        output=None,\n        foreground=True,\n        unique=True,\n        erosion_kernel=None,\n        mask_multiplier=255,\n        **kwargs,\n    ):\n        \"\"\"Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n        Args:\n            output (str, optional): The path to the output image. Defaults to None, saving the masks to SamGeo.objects.\n            foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n            unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n                Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n                You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n\n        \"\"\"\n\n        if self.masks is None:\n            raise ValueError(\"No masks found. Please run generate() first.\")\n\n        h, w, _ = self.image.shape\n        masks = self.masks\n\n        # Set output image data type based on the number of objects\n        if len(masks) &lt; 255:\n            dtype = np.uint8\n        elif len(masks) &lt; 65535:\n            dtype = np.uint16\n        else:\n            dtype = np.uint32\n\n        # Generate a mask of objects with unique values\n        if unique:\n            # Sort the masks by area in ascending order\n            sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=False)\n\n            # Create an output image with the same size as the input image\n            objects = np.zeros(\n                (\n                    sorted_masks[0][\"segmentation\"].shape[0],\n                    sorted_masks[0][\"segmentation\"].shape[1],\n                )\n            )\n            # Assign a unique value to each object\n            for index, ann in enumerate(sorted_masks):\n                m = ann[\"segmentation\"]\n                objects[m] = index + 1\n\n        # Generate a binary mask\n        else:\n            if foreground:  # Extract foreground objects only\n                resulting_mask = np.zeros((h, w), dtype=dtype)\n            else:\n                resulting_mask = np.ones((h, w), dtype=dtype)\n            resulting_borders = np.zeros((h, w), dtype=dtype)\n\n            for m in masks:\n                mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n                resulting_mask += mask\n\n                # Apply erosion to the mask\n                if erosion_kernel is not None:\n                    mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                    mask_erode = (mask_erode &gt; 0).astype(dtype)\n                    edge_mask = mask - mask_erode\n                    resulting_borders += edge_mask\n\n            resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n            resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n            objects = resulting_mask - resulting_borders\n            objects = objects * mask_multiplier\n\n        objects = objects.astype(dtype)\n        self.objects = objects\n\n        if output is not None:  # Save the output image\n            array_to_image(self.objects, output, self.source, **kwargs)\n\n    def show_masks(\n        self, figsize=(12, 10), cmap=\"binary_r\", axis=\"off\", foreground=True, **kwargs\n    ):\n        \"\"\"Show the binary mask or the mask of objects with unique values.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            cmap (str, optional): The colormap. Defaults to \"binary_r\".\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n            **kwargs: Other arguments for save_masks().\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        if self.batch:\n            self.objects = cv2.imread(self.masks)\n        else:\n            if self.objects is None:\n                self.save_masks(foreground=foreground, **kwargs)\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.objects, cmap=cmap)\n        plt.axis(axis)\n        plt.show()\n\n    def show_anns(\n        self,\n        figsize=(12, 10),\n        axis=\"off\",\n        alpha=0.35,\n        output=None,\n        blend=True,\n        **kwargs,\n    ):\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n        \"\"\"\n\n        import matplotlib.pyplot as plt\n\n        anns = self.masks\n\n        if self.image is None:\n            print(\"Please run generate() first.\")\n            return\n\n        if anns is None or len(anns) == 0:\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n        ax = plt.gca()\n        ax.set_autoscale_on(False)\n\n        img = np.ones(\n            (\n                sorted_anns[0][\"segmentation\"].shape[0],\n                sorted_anns[0][\"segmentation\"].shape[1],\n                4,\n            )\n        )\n        img[:, :, 3] = 0\n        for ann in sorted_anns:\n            m = ann[\"segmentation\"]\n            color_mask = np.concatenate([np.random.random(3), [alpha]])\n            img[m] = color_mask\n        ax.imshow(img)\n\n        if \"dpi\" not in kwargs:\n            kwargs[\"dpi\"] = 100\n\n        if \"bbox_inches\" not in kwargs:\n            kwargs[\"bbox_inches\"] = \"tight\"\n\n        plt.axis(axis)\n\n        self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n        if output is not None:\n            if blend:\n                array = blend_images(\n                    self.annotations, self.image, alpha=alpha, show=False\n                )\n            else:\n                array = self.annotations\n            array_to_image(array, output, self.source)\n\n    def set_image(self, image, image_format=\"RGB\"):\n        \"\"\"Set the input image as a numpy array.\n\n        Args:\n            image (np.ndarray): The input image as a numpy array.\n            image_format (str, optional): The image format, can be RGB or BGR. Defaults to \"RGB\".\n        \"\"\"\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            image = cv2.imread(image)\n            image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n            self.image = image\n        elif isinstance(image, np.ndarray):\n            pass\n        else:\n            raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n        self.predictor.set_image(image, image_format=image_format)\n\n    def save_prediction(\n        self,\n        output,\n        index=None,\n        mask_multiplier=255,\n        dtype=np.float32,\n        vector=None,\n        simplify_tolerance=None,\n        **kwargs,\n    ):\n        \"\"\"Save the predicted mask to the output path.\n\n        Args:\n            output (str): The path to the output image.\n            index (int, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            vector (str, optional): The path to the output vector file. Defaults to None.\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n\n        \"\"\"\n        if self.scores is None:\n            raise ValueError(\"No predictions found. Please run predict() first.\")\n\n        if index is None:\n            index = self.scores.argmax(axis=0)\n\n        array = self.masks[index] * mask_multiplier\n        self.prediction = array\n        array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n        if vector is not None:\n            raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n\n    def predict(\n        self,\n        point_coords=None,\n        point_labels=None,\n        boxes=None,\n        point_crs=None,\n        mask_input=None,\n        multimask_output=True,\n        return_logits=False,\n        output=None,\n        index=None,\n        mask_multiplier=255,\n        dtype=\"float32\",\n        return_results=False,\n        **kwargs,\n    ):\n        \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n        Args:\n            point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n                model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n                dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n            point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n                point prompts. 1 indicates a foreground point and 0 indicates a background point.\n            point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n            boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n                model, in XYXY format.\n            mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n                coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n                multimask_output (bool, optional): If true, the model will return three masks.\n                For ambiguous input prompts (such as a single click), this will often\n                produce better masks than a single prediction. If only a single\n                mask is needed, the model's predicted quality score can be used\n                to select the best mask. For non-ambiguous prompts, such as multiple\n                input prompts, multimask_output=False can give better results.\n            return_logits (bool, optional): If true, returns un-thresholded masks logits\n                instead of a binary mask.\n            output (str, optional): The path to the output image. Defaults to None.\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n            return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n        \"\"\"\n        out_of_bounds = []\n\n        if isinstance(boxes, str):\n            gdf = gpd.read_file(boxes)\n            if gdf.crs is not None:\n                gdf = gdf.to_crs(\"epsg:4326\")\n            boxes = gdf.geometry.bounds.values.tolist()\n        elif isinstance(boxes, dict):\n            import json\n\n            geojson = json.dumps(boxes)\n            gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n            boxes = gdf.geometry.bounds.values.tolist()\n\n        if isinstance(point_coords, str):\n            point_coords = vector_to_geojson(point_coords)\n\n        if isinstance(point_coords, dict):\n            point_coords = geojson_to_coords(point_coords)\n\n        if hasattr(self, \"point_coords\"):\n            point_coords = self.point_coords\n\n        if hasattr(self, \"point_labels\"):\n            point_labels = self.point_labels\n\n        if (point_crs is not None) and (point_coords is not None):\n            point_coords, out_of_bounds = coords_to_xy(\n                self.source, point_coords, point_crs, return_out_of_bounds=True\n            )\n\n        if isinstance(point_coords, list):\n            point_coords = np.array(point_coords)\n\n        if point_coords is not None:\n            if point_labels is None:\n                point_labels = [1] * len(point_coords)\n            elif isinstance(point_labels, int):\n                point_labels = [point_labels] * len(point_coords)\n\n        if isinstance(point_labels, list):\n            if len(point_labels) != len(point_coords):\n                if len(point_labels) == 1:\n                    point_labels = point_labels * len(point_coords)\n                elif len(out_of_bounds) &gt; 0:\n                    print(f\"Removing {len(out_of_bounds)} out-of-bound points.\")\n                    point_labels_new = []\n                    for i, p in enumerate(point_labels):\n                        if i not in out_of_bounds:\n                            point_labels_new.append(p)\n                    point_labels = point_labels_new\n                else:\n                    raise ValueError(\n                        \"The length of point_labels must be equal to the length of point_coords.\"\n                    )\n            point_labels = np.array(point_labels)\n\n        predictor = self.predictor\n\n        input_boxes = None\n        if isinstance(boxes, list) and (point_crs is not None):\n            coords = bbox_to_xy(self.source, boxes, point_crs)\n            input_boxes = np.array(coords)\n            if isinstance(coords[0], int):\n                input_boxes = input_boxes[None, :]\n            else:\n                input_boxes = torch.tensor(input_boxes, device=self.device)\n                input_boxes = predictor.transform.apply_boxes_torch(\n                    input_boxes, self.image.shape[:2]\n                )\n        elif isinstance(boxes, list) and (point_crs is None):\n            input_boxes = np.array(boxes)\n            if isinstance(boxes[0], int):\n                input_boxes = input_boxes[None, :]\n\n        self.boxes = input_boxes\n\n        if (\n            boxes is None\n            or (len(boxes) == 1)\n            or (len(boxes) == 4 and isinstance(boxes[0], float))\n        ):\n            if isinstance(boxes, list) and isinstance(boxes[0], list):\n                boxes = boxes[0]\n            masks, scores, logits = predictor.predict(\n                point_coords,\n                point_labels,\n                input_boxes,\n                mask_input,\n                multimask_output,\n                return_logits,\n            )\n        else:\n            masks, scores, logits = predictor.predict_torch(\n                point_coords=point_coords,\n                point_labels=point_coords,\n                boxes=input_boxes,\n                multimask_output=True,\n            )\n\n        self.masks = masks\n        self.scores = scores\n        self.logits = logits\n\n        if output is not None:\n            if boxes is None or (not isinstance(boxes[0], list)):\n                self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n            else:\n                self.tensor_to_numpy(\n                    index, output, mask_multiplier, dtype, save_args=kwargs\n                )\n\n        if return_results:\n            return masks, scores, logits\n\n    def tensor_to_numpy(\n        self, index=None, output=None, mask_multiplier=255, dtype=\"uint8\", save_args={}\n    ):\n        \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n        Args:\n            index (index, optional): The index of the mask to save. Defaults to None,\n                which will save the mask with the highest score.\n            output (str, optional): The path to the output image. Defaults to None.\n            mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            dtype (np.dtype, optional): The data type of the output image. Defaults to np.uint8.\n            save_args (dict, optional): Optional arguments for saving the output image. Defaults to {}.\n\n        Returns:\n            np.ndarray: The predicted mask as a numpy array.\n        \"\"\"\n\n        boxes = self.boxes\n        masks = self.masks\n\n        image_pil = self.image\n        image_np = np.array(image_pil)\n\n        if index is None:\n            index = 1\n\n        masks = masks[:, index, :, :]\n        masks = masks.squeeze(1)\n\n        if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n            print(\"No objects found in the image.\")\n            return\n        else:\n            # Create an empty image to store the mask overlays\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Adjusted for single channel\n\n            for i, (box, mask) in enumerate(zip(boxes, masks)):\n                # Convert tensor to numpy array if necessary and ensure it contains integers\n                if isinstance(mask, torch.Tensor):\n                    mask = (\n                        mask.cpu().numpy().astype(dtype)\n                    )  # If mask is on GPU, use .cpu() before .numpy()\n                mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                    dtype\n                )  # Assign a unique value for each mask\n\n            # Normalize mask_overlay to be in [0, 255]\n            mask_overlay = (\n                mask_overlay &gt; 0\n            ) * mask_multiplier  # Binary mask in [0, 255]\n\n        if output is not None:\n            array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n        else:\n            return mask_overlay\n\n    def show_map(self, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n        \"\"\"Show the interactive map.\n\n        Args:\n            basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n            repeat_mode (bool, optional): Whether to use the repeat mode for draw control. Defaults to True.\n            out_dir (str, optional): The path to the output directory. Defaults to None.\n\n        Returns:\n            leafmap.Map: The map object.\n        \"\"\"\n        return sam_map_gui(\n            self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n        )\n\n    def show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n        \"\"\"Show a canvas to collect foreground and background points.\n\n        Args:\n            image (str | np.ndarray): The input image.\n            fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n            bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n            radius (int, optional): The radius of the points. Defaults to 5.\n\n        Returns:\n            tuple: A tuple of two lists of foreground and background points.\n        \"\"\"\n\n        if self.image is None:\n            raise ValueError(\"Please run set_image() first.\")\n\n        image = self.image\n        fg_points, bg_points = show_canvas(image, fg_color, bg_color, radius)\n        self.fg_points = fg_points\n        self.bg_points = bg_points\n        point_coords = fg_points + bg_points\n        point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n        self.point_coords = point_coords\n        self.point_labels = point_labels\n\n    def clear_cuda_cache(self):\n        \"\"\"Clear the CUDA cache.\"\"\"\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()\n\n    def image_to_image(self, image, **kwargs):\n        return image_to_image(image, self, **kwargs)\n\n    def download_tms_as_tiff(self, source, pt1, pt2, zoom, dist):\n        image = draw_tile(source, pt1[0], pt1[1], pt2[0], pt2[1], zoom, dist)\n        return image\n\n    def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Save the result to a vector file.\n\n        Args:\n            image (str): The path to the image file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n\n    def tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a gpkg file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a gpkg file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the gpkg file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_gpkg(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a shapefile.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the shapefile.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_shp(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n\n    def tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Convert a tiff file to a GeoJSON file.\n\n        Args:\n            tiff_path (str): The path to the tiff file.\n            output (str): The path to the GeoJSON file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_geojson(\n            tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n        )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.__call__","title":"<code>__call__(self, image, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255, **kwargs)</code>  <code>special</code>","text":"<p>Generate masks for the input tile. This function originates from the segment-anything-eo repository.     See https://bit.ly/41pwiHw</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p>The input image as a numpy array.</p> required <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).</p> <code>(3, 3)</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def __call__(\n    self,\n    image,\n    foreground=True,\n    erosion_kernel=(3, 3),\n    mask_multiplier=255,\n    **kwargs,\n):\n    \"\"\"Generate masks for the input tile. This function originates from the segment-anything-eo repository.\n        See https://bit.ly/41pwiHw\n\n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders. Defaults to (3, 3).\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n    \"\"\"\n    h, w, _ = image.shape\n\n    masks = self.mask_generator.generate(image)\n\n    if foreground:  # Extract foreground objects only\n        resulting_mask = np.zeros((h, w), dtype=np.uint8)\n    else:\n        resulting_mask = np.ones((h, w), dtype=np.uint8)\n    resulting_borders = np.zeros((h, w), dtype=np.uint8)\n\n    for m in masks:\n        mask = (m[\"segmentation\"] &gt; 0).astype(np.uint8)\n        resulting_mask += mask\n\n        # Apply erosion to the mask\n        if erosion_kernel is not None:\n            mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n            mask_erode = (mask_erode &gt; 0).astype(np.uint8)\n            edge_mask = mask - mask_erode\n            resulting_borders += edge_mask\n\n    resulting_mask = (resulting_mask &gt; 0).astype(np.uint8)\n    resulting_borders = (resulting_borders &gt; 0).astype(np.uint8)\n    resulting_mask_with_borders = resulting_mask - resulting_borders\n    return resulting_mask_with_borders * mask_multiplier\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.__init__","title":"<code>__init__(self, model_type='vit_h', automatic=True, device=None, checkpoint_dir=None, sam_kwargs=None, **kwargs)</code>  <code>special</code>","text":"<p>Initialize the class.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. It can be one of the following: vit_h, vit_l, vit_b. Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> <code>automatic</code> <code>bool</code> <p>Whether to use the automatic mask generator or input prompts. Defaults to True. The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.</p> <code>True</code> <code>device</code> <code>str</code> <p>The device to use. It can be one of the following: cpu, cuda. Defaults to None, which will use cuda if available.</p> <code>None</code> <code>checkpoint_dir</code> <code>str</code> <p>The path to the model checkpoint. It can be one of the following: sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth. Defaults to None. See https://bit.ly/3VrpxUh for more details.</p> <code>None</code> <code>sam_kwargs</code> <code>dict</code> <p>Optional arguments for fine-tuning the SAM model. Defaults to None. The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.</p> <p>points_per_side: Optional[int] = 32, points_per_batch: int = 64, pred_iou_thresh: float = 0.88, stability_score_thresh: float = 0.95, stability_score_offset: float = 1.0, box_nms_thresh: float = 0.7, crop_n_layers: int = 0, crop_nms_thresh: float = 0.7, crop_overlap_ratio: float = 512 / 1500, crop_n_points_downscale_factor: int = 1, point_grids: Optional[List[np.ndarray]] = None, min_mask_region_area: int = 0, output_mode: str = \"binary_mask\",</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def __init__(\n    self,\n    model_type=\"vit_h\",\n    automatic=True,\n    device=None,\n    checkpoint_dir=None,\n    sam_kwargs=None,\n    **kwargs,\n):\n    \"\"\"Initialize the class.\n\n    Args:\n        model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        automatic (bool, optional): Whether to use the automatic mask generator or input prompts. Defaults to True.\n            The automatic mask generator will segment the entire image, while the input prompts will segment selected objects.\n        device (str, optional): The device to use. It can be one of the following: cpu, cuda.\n            Defaults to None, which will use cuda if available.\n        checkpoint_dir (str, optional): The path to the model checkpoint. It can be one of the following:\n            sam_vit_h_4b8939.pth, sam_vit_l_0b3195.pth, sam_vit_b_01ec64.pth.\n            Defaults to None. See https://bit.ly/3VrpxUh for more details.\n        sam_kwargs (dict, optional): Optional arguments for fine-tuning the SAM model. Defaults to None.\n            The available arguments with default values are listed below. See https://bit.ly/410RV0v for more details.\n\n            points_per_side: Optional[int] = 32,\n            points_per_batch: int = 64,\n            pred_iou_thresh: float = 0.88,\n            stability_score_thresh: float = 0.95,\n            stability_score_offset: float = 1.0,\n            box_nms_thresh: float = 0.7,\n            crop_n_layers: int = 0,\n            crop_nms_thresh: float = 0.7,\n            crop_overlap_ratio: float = 512 / 1500,\n            crop_n_points_downscale_factor: int = 1,\n            point_grids: Optional[List[np.ndarray]] = None,\n            min_mask_region_area: int = 0,\n            output_mode: str = \"binary_mask\",\n\n    \"\"\"\n    hq = False  # Not using HQ-SAM\n\n    if \"checkpoint\" in kwargs:\n        checkpoint = kwargs[\"checkpoint\"]\n        if not os.path.exists(checkpoint):\n            checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n        kwargs.pop(\"checkpoint\")\n    else:\n        checkpoint = download_checkpoint(model_type, checkpoint_dir, hq)\n\n    # Use cuda if available\n    if device is None:\n        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        if device == \"cuda\":\n            torch.cuda.empty_cache()\n\n    self.checkpoint = checkpoint\n    self.model_type = model_type\n    self.device = device\n    self.sam_kwargs = sam_kwargs  # Optional arguments for fine-tuning the SAM model\n    self.source = None  # Store the input image path\n    self.image = None  # Store the input image as a numpy array\n    # Store the masks as a list of dictionaries. Each mask is a dictionary\n    # containing segmentation, area, bbox, predicted_iou, point_coords, stability_score, and crop_box\n    self.masks = None\n    self.objects = None  # Store the mask objects as a numpy array\n    # Store the annotations (objects with random color) as a numpy array.\n    self.annotations = None\n\n    # Store the predicted masks, iou_predictions, and low_res_masks\n    self.prediction = None\n    self.scores = None\n    self.logits = None\n\n    # Build the SAM model\n    self.sam = sam_model_registry[self.model_type](checkpoint=self.checkpoint)\n    self.sam.to(device=self.device)\n    # Use optional arguments for fine-tuning the SAM model\n    sam_kwargs = self.sam_kwargs if self.sam_kwargs is not None else {}\n\n    if automatic:\n        # Segment the entire image using the automatic mask generator\n        self.mask_generator = SamAutomaticMaskGenerator(self.sam, **sam_kwargs)\n    else:\n        # Segment selected objects using input prompts\n        self.predictor = SamPredictor(self.sam, **sam_kwargs)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.clear_cuda_cache","title":"<code>clear_cuda_cache(self)</code>","text":"<p>Clear the CUDA cache.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def clear_cuda_cache(self):\n    \"\"\"Clear the CUDA cache.\"\"\"\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.generate","title":"<code>generate(self, source, output=None, foreground=True, batch=False, erosion_kernel=None, mask_multiplier=255, unique=True, **kwargs)</code>","text":"<p>Generate masks for the input image.</p> <p>Parameters:</p> Name Type Description Default <code>source</code> <code>str | np.ndarray</code> <p>The path to the input image or the input image as a numpy array.</p> required <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>batch</code> <code>bool</code> <p>Whether to generate masks for a batch of image tiles. Defaults to False.</p> <code>False</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255. The parameter is ignored if unique is True.</p> <code>255</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True. The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.</p> <code>True</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def generate(\n    self,\n    source,\n    output=None,\n    foreground=True,\n    batch=False,\n    erosion_kernel=None,\n    mask_multiplier=255,\n    unique=True,\n    **kwargs,\n):\n    \"\"\"Generate masks for the input image.\n\n    Args:\n        source (str | np.ndarray): The path to the input image or the input image as a numpy array.\n        output (str, optional): The path to the output image. Defaults to None.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        batch (bool, optional): Whether to generate masks for a batch of image tiles. Defaults to False.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n            The parameter is ignored if unique is True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n            The unique value increases from 1 to the number of objects. The larger the number, the larger the object area.\n\n    \"\"\"\n\n    if isinstance(source, str):\n        if source.startswith(\"http\"):\n            source = download_file(source)\n\n        if not os.path.exists(source):\n            raise ValueError(f\"Input path {source} does not exist.\")\n\n        if batch:  # Subdivide the image into tiles and segment each tile\n            self.batch = True\n            self.source = source\n            self.masks = output\n            return tiff_to_tiff(\n                source,\n                output,\n                self,\n                foreground=foreground,\n                erosion_kernel=erosion_kernel,\n                mask_multiplier=mask_multiplier,\n                **kwargs,\n            )\n\n        image = cv2.imread(source)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n    elif isinstance(source, np.ndarray):\n        image = source\n        source = None\n    else:\n        raise ValueError(\"Input source must be either a path or a numpy array.\")\n\n    self.source = source  # Store the input image path\n    self.image = image  # Store the input image as a numpy array\n    mask_generator = self.mask_generator  # The automatic mask generator\n    masks = mask_generator.generate(image)  # Segment the input image\n    self.masks = masks  # Store the masks as a list of dictionaries\n    self.batch = False\n\n    if output is not None:\n        # Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n        self.save_masks(\n            output, foreground, unique, erosion_kernel, mask_multiplier, **kwargs\n        )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.predict","title":"<code>predict(self, point_coords=None, point_labels=None, boxes=None, point_crs=None, mask_input=None, multimask_output=True, return_logits=False, output=None, index=None, mask_multiplier=255, dtype='float32', return_results=False, **kwargs)</code>","text":"<p>Predict masks for the given input prompts, using the currently set image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>str | dict | list | np.ndarray</code> <p>A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.</p> <code>None</code> <code>point_labels</code> <code>list | int | np.ndarray</code> <p>A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>point_crs</code> <code>str</code> <p>The coordinate reference system (CRS) of the point prompts.</p> <code>None</code> <code>boxes</code> <code>list | np.ndarray</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> <code>None</code> <code>mask_input</code> <code>np.ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256. multimask_output (bool, optional): If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>None</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>np.dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>'float32'</code> <code>return_results</code> <code>bool</code> <p>Whether to return the predicted masks, scores, and logits. Defaults to False.</p> <code>False</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def predict(\n    self,\n    point_coords=None,\n    point_labels=None,\n    boxes=None,\n    point_crs=None,\n    mask_input=None,\n    multimask_output=True,\n    return_logits=False,\n    output=None,\n    index=None,\n    mask_multiplier=255,\n    dtype=\"float32\",\n    return_results=False,\n    **kwargs,\n):\n    \"\"\"Predict masks for the given input prompts, using the currently set image.\n\n    Args:\n        point_coords (str | dict | list | np.ndarray, optional): A Nx2 array of point prompts to the\n            model. Each point is in (X,Y) in pixels. It can be a path to a vector file, a GeoJSON\n            dictionary, a list of coordinates [lon, lat], or a numpy array. Defaults to None.\n        point_labels (list | int | np.ndarray, optional): A length N array of labels for the\n            point prompts. 1 indicates a foreground point and 0 indicates a background point.\n        point_crs (str, optional): The coordinate reference system (CRS) of the point prompts.\n        boxes (list | np.ndarray, optional): A length 4 array given a box prompt to the\n            model, in XYXY format.\n        mask_input (np.ndarray, optional): A low resolution mask input to the model, typically\n            coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.\n            multimask_output (bool, optional): If true, the model will return three masks.\n            For ambiguous input prompts (such as a single click), this will often\n            produce better masks than a single prediction. If only a single\n            mask is needed, the model's predicted quality score can be used\n            to select the best mask. For non-ambiguous prompts, such as multiple\n            input prompts, multimask_output=False can give better results.\n        return_logits (bool, optional): If true, returns un-thresholded masks logits\n            instead of a binary mask.\n        output (str, optional): The path to the output image. Defaults to None.\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        return_results (bool, optional): Whether to return the predicted masks, scores, and logits. Defaults to False.\n\n    \"\"\"\n    out_of_bounds = []\n\n    if isinstance(boxes, str):\n        gdf = gpd.read_file(boxes)\n        if gdf.crs is not None:\n            gdf = gdf.to_crs(\"epsg:4326\")\n        boxes = gdf.geometry.bounds.values.tolist()\n    elif isinstance(boxes, dict):\n        import json\n\n        geojson = json.dumps(boxes)\n        gdf = gpd.read_file(geojson, driver=\"GeoJSON\")\n        boxes = gdf.geometry.bounds.values.tolist()\n\n    if isinstance(point_coords, str):\n        point_coords = vector_to_geojson(point_coords)\n\n    if isinstance(point_coords, dict):\n        point_coords = geojson_to_coords(point_coords)\n\n    if hasattr(self, \"point_coords\"):\n        point_coords = self.point_coords\n\n    if hasattr(self, \"point_labels\"):\n        point_labels = self.point_labels\n\n    if (point_crs is not None) and (point_coords is not None):\n        point_coords, out_of_bounds = coords_to_xy(\n            self.source, point_coords, point_crs, return_out_of_bounds=True\n        )\n\n    if isinstance(point_coords, list):\n        point_coords = np.array(point_coords)\n\n    if point_coords is not None:\n        if point_labels is None:\n            point_labels = [1] * len(point_coords)\n        elif isinstance(point_labels, int):\n            point_labels = [point_labels] * len(point_coords)\n\n    if isinstance(point_labels, list):\n        if len(point_labels) != len(point_coords):\n            if len(point_labels) == 1:\n                point_labels = point_labels * len(point_coords)\n            elif len(out_of_bounds) &gt; 0:\n                print(f\"Removing {len(out_of_bounds)} out-of-bound points.\")\n                point_labels_new = []\n                for i, p in enumerate(point_labels):\n                    if i not in out_of_bounds:\n                        point_labels_new.append(p)\n                point_labels = point_labels_new\n            else:\n                raise ValueError(\n                    \"The length of point_labels must be equal to the length of point_coords.\"\n                )\n        point_labels = np.array(point_labels)\n\n    predictor = self.predictor\n\n    input_boxes = None\n    if isinstance(boxes, list) and (point_crs is not None):\n        coords = bbox_to_xy(self.source, boxes, point_crs)\n        input_boxes = np.array(coords)\n        if isinstance(coords[0], int):\n            input_boxes = input_boxes[None, :]\n        else:\n            input_boxes = torch.tensor(input_boxes, device=self.device)\n            input_boxes = predictor.transform.apply_boxes_torch(\n                input_boxes, self.image.shape[:2]\n            )\n    elif isinstance(boxes, list) and (point_crs is None):\n        input_boxes = np.array(boxes)\n        if isinstance(boxes[0], int):\n            input_boxes = input_boxes[None, :]\n\n    self.boxes = input_boxes\n\n    if (\n        boxes is None\n        or (len(boxes) == 1)\n        or (len(boxes) == 4 and isinstance(boxes[0], float))\n    ):\n        if isinstance(boxes, list) and isinstance(boxes[0], list):\n            boxes = boxes[0]\n        masks, scores, logits = predictor.predict(\n            point_coords,\n            point_labels,\n            input_boxes,\n            mask_input,\n            multimask_output,\n            return_logits,\n        )\n    else:\n        masks, scores, logits = predictor.predict_torch(\n            point_coords=point_coords,\n            point_labels=point_coords,\n            boxes=input_boxes,\n            multimask_output=True,\n        )\n\n    self.masks = masks\n    self.scores = scores\n    self.logits = logits\n\n    if output is not None:\n        if boxes is None or (not isinstance(boxes[0], list)):\n            self.save_prediction(output, index, mask_multiplier, dtype, **kwargs)\n        else:\n            self.tensor_to_numpy(\n                index, output, mask_multiplier, dtype, save_args=kwargs\n            )\n\n    if return_results:\n        return masks, scores, logits\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.raster_to_vector","title":"<code>raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the result to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Save the result to a vector file.\n\n    Args:\n        image (str): The path to the image file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.save_masks","title":"<code>save_masks(self, output=None, foreground=True, unique=True, erosion_kernel=None, mask_multiplier=255, **kwargs)</code>","text":"<p>Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None, saving the masks to SamGeo.objects.</p> <code>None</code> <code>foreground</code> <code>bool</code> <p>Whether to generate the foreground mask. Defaults to True.</p> <code>True</code> <code>unique</code> <code>bool</code> <p>Whether to assign a unique value to each object. Defaults to True.</p> <code>True</code> <code>erosion_kernel</code> <code>tuple</code> <p>The erosion kernel for filtering object masks and extract borders. Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1]. You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.</p> <code>255</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def save_masks(\n    self,\n    output=None,\n    foreground=True,\n    unique=True,\n    erosion_kernel=None,\n    mask_multiplier=255,\n    **kwargs,\n):\n    \"\"\"Save the masks to the output path. The output is either a binary mask or a mask of objects with unique values.\n\n    Args:\n        output (str, optional): The path to the output image. Defaults to None, saving the masks to SamGeo.objects.\n        foreground (bool, optional): Whether to generate the foreground mask. Defaults to True.\n        unique (bool, optional): Whether to assign a unique value to each object. Defaults to True.\n        erosion_kernel (tuple, optional): The erosion kernel for filtering object masks and extract borders.\n            Such as (3, 3) or (5, 5). Set to None to disable it. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n            You can use this parameter to scale the mask to a larger range, for example [0, 255]. Defaults to 255.\n\n    \"\"\"\n\n    if self.masks is None:\n        raise ValueError(\"No masks found. Please run generate() first.\")\n\n    h, w, _ = self.image.shape\n    masks = self.masks\n\n    # Set output image data type based on the number of objects\n    if len(masks) &lt; 255:\n        dtype = np.uint8\n    elif len(masks) &lt; 65535:\n        dtype = np.uint16\n    else:\n        dtype = np.uint32\n\n    # Generate a mask of objects with unique values\n    if unique:\n        # Sort the masks by area in ascending order\n        sorted_masks = sorted(masks, key=(lambda x: x[\"area\"]), reverse=False)\n\n        # Create an output image with the same size as the input image\n        objects = np.zeros(\n            (\n                sorted_masks[0][\"segmentation\"].shape[0],\n                sorted_masks[0][\"segmentation\"].shape[1],\n            )\n        )\n        # Assign a unique value to each object\n        for index, ann in enumerate(sorted_masks):\n            m = ann[\"segmentation\"]\n            objects[m] = index + 1\n\n    # Generate a binary mask\n    else:\n        if foreground:  # Extract foreground objects only\n            resulting_mask = np.zeros((h, w), dtype=dtype)\n        else:\n            resulting_mask = np.ones((h, w), dtype=dtype)\n        resulting_borders = np.zeros((h, w), dtype=dtype)\n\n        for m in masks:\n            mask = (m[\"segmentation\"] &gt; 0).astype(dtype)\n            resulting_mask += mask\n\n            # Apply erosion to the mask\n            if erosion_kernel is not None:\n                mask_erode = cv2.erode(mask, erosion_kernel, iterations=1)\n                mask_erode = (mask_erode &gt; 0).astype(dtype)\n                edge_mask = mask - mask_erode\n                resulting_borders += edge_mask\n\n        resulting_mask = (resulting_mask &gt; 0).astype(dtype)\n        resulting_borders = (resulting_borders &gt; 0).astype(dtype)\n        objects = resulting_mask - resulting_borders\n        objects = objects * mask_multiplier\n\n    objects = objects.astype(dtype)\n    self.objects = objects\n\n    if output is not None:  # Save the output image\n        array_to_image(self.objects, output, self.source, **kwargs)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.save_prediction","title":"<code>save_prediction(self, output, index=None, mask_multiplier=255, dtype=&lt;class 'numpy.float32'&gt;, vector=None, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the predicted mask to the output path.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output image.</p> required <code>index</code> <code>int</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>vector</code> <code>str</code> <p>The path to the output vector file. Defaults to None.</p> <code>None</code> <code>dtype</code> <code>np.dtype</code> <p>The data type of the output image. Defaults to np.float32.</p> <code>&lt;class 'numpy.float32'&gt;</code> <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def save_prediction(\n    self,\n    output,\n    index=None,\n    mask_multiplier=255,\n    dtype=np.float32,\n    vector=None,\n    simplify_tolerance=None,\n    **kwargs,\n):\n    \"\"\"Save the predicted mask to the output path.\n\n    Args:\n        output (str): The path to the output image.\n        index (int, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        vector (str, optional): The path to the output vector file. Defaults to None.\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.float32.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n\n    \"\"\"\n    if self.scores is None:\n        raise ValueError(\"No predictions found. Please run predict() first.\")\n\n    if index is None:\n        index = self.scores.argmax(axis=0)\n\n    array = self.masks[index] * mask_multiplier\n    self.prediction = array\n    array_to_image(array, output, self.source, dtype=dtype, **kwargs)\n\n    if vector is not None:\n        raster_to_vector(output, vector, simplify_tolerance=simplify_tolerance)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.set_image","title":"<code>set_image(self, image, image_format='RGB')</code>","text":"<p>Set the input image as a numpy array.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p>The input image as a numpy array.</p> required <code>image_format</code> <code>str</code> <p>The image format, can be RGB or BGR. Defaults to \"RGB\".</p> <code>'RGB'</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def set_image(self, image, image_format=\"RGB\"):\n    \"\"\"Set the input image as a numpy array.\n\n    Args:\n        image (np.ndarray): The input image as a numpy array.\n        image_format (str, optional): The image format, can be RGB or BGR. Defaults to \"RGB\".\n    \"\"\"\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        image = cv2.imread(image)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        self.image = image\n    elif isinstance(image, np.ndarray):\n        pass\n    else:\n        raise ValueError(\"Input image must be either a path or a numpy array.\")\n\n    self.predictor.set_image(image, image_format=image_format)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.show_anns","title":"<code>show_anns(self, figsize=(12, 10), axis='off', alpha=0.35, output=None, blend=True, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.35.</p> <code>0.35</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> <code>True</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def show_anns(\n    self,\n    figsize=(12, 10),\n    axis=\"off\",\n    alpha=0.35,\n    output=None,\n    blend=True,\n    **kwargs,\n):\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.35.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    anns = self.masks\n\n    if self.image is None:\n        print(\"Please run generate() first.\")\n        return\n\n    if anns is None or len(anns) == 0:\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    sorted_anns = sorted(anns, key=(lambda x: x[\"area\"]), reverse=True)\n\n    ax = plt.gca()\n    ax.set_autoscale_on(False)\n\n    img = np.ones(\n        (\n            sorted_anns[0][\"segmentation\"].shape[0],\n            sorted_anns[0][\"segmentation\"].shape[1],\n            4,\n        )\n    )\n    img[:, :, 3] = 0\n    for ann in sorted_anns:\n        m = ann[\"segmentation\"]\n        color_mask = np.concatenate([np.random.random(3), [alpha]])\n        img[m] = color_mask\n    ax.imshow(img)\n\n    if \"dpi\" not in kwargs:\n        kwargs[\"dpi\"] = 100\n\n    if \"bbox_inches\" not in kwargs:\n        kwargs[\"bbox_inches\"] = \"tight\"\n\n    plt.axis(axis)\n\n    self.annotations = (img[:, :, 0:3] * 255).astype(np.uint8)\n\n    if output is not None:\n        if blend:\n            array = blend_images(\n                self.annotations, self.image, alpha=alpha, show=False\n            )\n        else:\n            array = self.annotations\n        array_to_image(array, output, self.source)\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.show_canvas","title":"<code>show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5)</code>","text":"<p>Show a canvas to collect foreground and background points.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str | np.ndarray</code> <p>The input image.</p> required <code>fg_color</code> <code>tuple</code> <p>The color for the foreground points. Defaults to (0, 255, 0).</p> <code>(0, 255, 0)</code> <code>bg_color</code> <code>tuple</code> <p>The color for the background points. Defaults to (0, 0, 255).</p> <code>(0, 0, 255)</code> <code>radius</code> <code>int</code> <p>The radius of the points. Defaults to 5.</p> <code>5</code> <p>Returns:</p> Type Description <code>tuple</code> <p>A tuple of two lists of foreground and background points.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def show_canvas(self, fg_color=(0, 255, 0), bg_color=(0, 0, 255), radius=5):\n    \"\"\"Show a canvas to collect foreground and background points.\n\n    Args:\n        image (str | np.ndarray): The input image.\n        fg_color (tuple, optional): The color for the foreground points. Defaults to (0, 255, 0).\n        bg_color (tuple, optional): The color for the background points. Defaults to (0, 0, 255).\n        radius (int, optional): The radius of the points. Defaults to 5.\n\n    Returns:\n        tuple: A tuple of two lists of foreground and background points.\n    \"\"\"\n\n    if self.image is None:\n        raise ValueError(\"Please run set_image() first.\")\n\n    image = self.image\n    fg_points, bg_points = show_canvas(image, fg_color, bg_color, radius)\n    self.fg_points = fg_points\n    self.bg_points = bg_points\n    point_coords = fg_points + bg_points\n    point_labels = [1] * len(fg_points) + [0] * len(bg_points)\n    self.point_coords = point_coords\n    self.point_labels = point_labels\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.show_map","title":"<code>show_map(self, basemap='SATELLITE', repeat_mode=True, out_dir=None, **kwargs)</code>","text":"<p>Show the interactive map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.</p> <code>'SATELLITE'</code> <code>repeat_mode</code> <code>bool</code> <p>Whether to use the repeat mode for draw control. Defaults to True.</p> <code>True</code> <code>out_dir</code> <code>str</code> <p>The path to the output directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>leafmap.Map</code> <p>The map object.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def show_map(self, basemap=\"SATELLITE\", repeat_mode=True, out_dir=None, **kwargs):\n    \"\"\"Show the interactive map.\n\n    Args:\n        basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n        repeat_mode (bool, optional): Whether to use the repeat mode for draw control. Defaults to True.\n        out_dir (str, optional): The path to the output directory. Defaults to None.\n\n    Returns:\n        leafmap.Map: The map object.\n    \"\"\"\n    return sam_map_gui(\n        self, basemap=basemap, repeat_mode=repeat_mode, out_dir=out_dir, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.show_masks","title":"<code>show_masks(self, figsize=(12, 10), cmap='binary_r', axis='off', foreground=True, **kwargs)</code>","text":"<p>Show the binary mask or the mask of objects with unique values.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>cmap</code> <code>str</code> <p>The colormap. Defaults to \"binary_r\".</p> <code>'binary_r'</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>foreground</code> <code>bool</code> <p>Whether to show the foreground mask only. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <p>Other arguments for save_masks().</p> <code>{}</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def show_masks(\n    self, figsize=(12, 10), cmap=\"binary_r\", axis=\"off\", foreground=True, **kwargs\n):\n    \"\"\"Show the binary mask or the mask of objects with unique values.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        cmap (str, optional): The colormap. Defaults to \"binary_r\".\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        foreground (bool, optional): Whether to show the foreground mask only. Defaults to True.\n        **kwargs: Other arguments for save_masks().\n    \"\"\"\n\n    import matplotlib.pyplot as plt\n\n    if self.batch:\n        self.objects = cv2.imread(self.masks)\n    else:\n        if self.objects is None:\n            self.save_masks(foreground=foreground, **kwargs)\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.objects, cmap=cmap)\n    plt.axis(axis)\n    plt.show()\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tensor_to_numpy","title":"<code>tensor_to_numpy(self, index=None, output=None, mask_multiplier=255, dtype='uint8', save_args={})</code>","text":"<p>Convert the predicted masks from tensors to numpy arrays.</p> <p>Parameters:</p> Name Type Description Default <code>index</code> <code>index</code> <p>The index of the mask to save. Defaults to None, which will save the mask with the highest score.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>The mask multiplier for the output mask, which is usually a binary mask [0, 1].</p> <code>255</code> <code>dtype</code> <code>np.dtype</code> <p>The data type of the output image. Defaults to np.uint8.</p> <code>'uint8'</code> <code>save_args</code> <code>dict</code> <p>Optional arguments for saving the output image. Defaults to {}.</p> <code>{}</code> <p>Returns:</p> Type Description <code>np.ndarray</code> <p>The predicted mask as a numpy array.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tensor_to_numpy(\n    self, index=None, output=None, mask_multiplier=255, dtype=\"uint8\", save_args={}\n):\n    \"\"\"Convert the predicted masks from tensors to numpy arrays.\n\n    Args:\n        index (index, optional): The index of the mask to save. Defaults to None,\n            which will save the mask with the highest score.\n        output (str, optional): The path to the output image. Defaults to None.\n        mask_multiplier (int, optional): The mask multiplier for the output mask, which is usually a binary mask [0, 1].\n        dtype (np.dtype, optional): The data type of the output image. Defaults to np.uint8.\n        save_args (dict, optional): Optional arguments for saving the output image. Defaults to {}.\n\n    Returns:\n        np.ndarray: The predicted mask as a numpy array.\n    \"\"\"\n\n    boxes = self.boxes\n    masks = self.masks\n\n    image_pil = self.image\n    image_np = np.array(image_pil)\n\n    if index is None:\n        index = 1\n\n    masks = masks[:, index, :, :]\n    masks = masks.squeeze(1)\n\n    if boxes is None or (len(boxes) == 0):  # No \"object\" instances found\n        print(\"No objects found in the image.\")\n        return\n    else:\n        # Create an empty image to store the mask overlays\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Adjusted for single channel\n\n        for i, (box, mask) in enumerate(zip(boxes, masks)):\n            # Convert tensor to numpy array if necessary and ensure it contains integers\n            if isinstance(mask, torch.Tensor):\n                mask = (\n                    mask.cpu().numpy().astype(dtype)\n                )  # If mask is on GPU, use .cpu() before .numpy()\n            mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                dtype\n            )  # Assign a unique value for each mask\n\n        # Normalize mask_overlay to be in [0, 255]\n        mask_overlay = (\n            mask_overlay &gt; 0\n        ) * mask_multiplier  # Binary mask in [0, 255]\n\n    if output is not None:\n        array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n    else:\n        return mask_overlay\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tiff_to_geojson","title":"<code>tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a GeoJSON file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the GeoJSON file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tiff_to_geojson(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a GeoJSON file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the GeoJSON file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_geojson(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tiff_to_gpkg","title":"<code>tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the gpkg file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tiff_to_gpkg(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the gpkg file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_gpkg(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tiff_to_shp","title":"<code>tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a shapefile.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the shapefile.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tiff_to_shp(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a shapefile.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the shapefile.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_shp(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeo.tiff_to_vector","title":"<code>tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Convert a tiff file to a gpkg file.</p> <p>Parameters:</p> Name Type Description Default <code>tiff_path</code> <code>str</code> <p>The path to the tiff file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/samgeo.py</code> <pre><code>def tiff_to_vector(self, tiff_path, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Convert a tiff file to a gpkg file.\n\n    Args:\n        tiff_path (str): The path to the tiff file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(\n        tiff_path, output, simplify_tolerance=simplify_tolerance, **kwargs\n    )\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeoPredictor","title":"<code> SamGeoPredictor            (SamPredictor)         </code>","text":"Source code in <code>samgeo/samgeo.py</code> <pre><code>class SamGeoPredictor(SamPredictor):\n    def __init__(\n        self,\n        sam_model,\n    ):\n        from segment_anything.utils.transforms import ResizeLongestSide\n\n        self.model = sam_model\n        self.transform = ResizeLongestSide(sam_model.image_encoder.img_size)\n\n    def set_image(self, image):\n        super(SamGeoPredictor, self).set_image(image)\n\n    def predict(\n        self,\n        src_fp=None,\n        geo_box=None,\n        point_coords=None,\n        point_labels=None,\n        box=None,\n        mask_input=None,\n        multimask_output=True,\n        return_logits=False,\n    ):\n        if geo_box and src_fp:\n            self.crs = \"EPSG:4326\"\n            dst_crs = get_crs(src_fp)\n            sw = transform_coords(geo_box[0], geo_box[1], self.crs, dst_crs)\n            ne = transform_coords(geo_box[2], geo_box[3], self.crs, dst_crs)\n            xs = np.array([sw[0], ne[0]])\n            ys = np.array([sw[1], ne[1]])\n            box = get_pixel_coords(src_fp, xs, ys)\n            self.geo_box = geo_box\n            self.width = box[2] - box[0]\n            self.height = box[3] - box[1]\n            self.geo_transform = set_transform(geo_box, self.width, self.height)\n\n        masks, iou_predictions, low_res_masks = super(SamGeoPredictor, self).predict(\n            point_coords, point_labels, box, mask_input, multimask_output, return_logits\n        )\n\n        return masks, iou_predictions, low_res_masks\n\n    def masks_to_geotiff(self, src_fp, dst_fp, masks):\n        profile = get_profile(src_fp)\n        write_raster(\n            dst_fp,\n            masks,\n            profile,\n            self.width,\n            self.height,\n            self.geo_transform,\n            self.crs,\n        )\n\n    def geotiff_to_geojson(self, src_fp, dst_fp, bidx=1):\n        gdf = get_features(src_fp, bidx)\n        write_features(gdf, dst_fp)\n        return gdf\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeoPredictor.predict","title":"<code>predict(self, src_fp=None, geo_box=None, point_coords=None, point_labels=None, box=None, mask_input=None, multimask_output=True, return_logits=False)</code>","text":"<p>Predict masks for the given input prompts, using the currently set image.</p> <p>Parameters:</p> Name Type Description Default <code>point_coords</code> <code>np.ndarray or None</code> <p>A Nx2 array of point prompts to the model. Each point is in (X,Y) in pixels.</p> <code>None</code> <code>point_labels</code> <code>np.ndarray or None</code> <p>A length N array of labels for the point prompts. 1 indicates a foreground point and 0 indicates a background point.</p> <code>None</code> <code>box</code> <code>np.ndarray or None</code> <p>A length 4 array given a box prompt to the model, in XYXY format.</p> <code>None</code> <code>mask_input</code> <code>np.ndarray</code> <p>A low resolution mask input to the model, typically coming from a previous prediction iteration. Has form 1xHxW, where for SAM, H=W=256.</p> <code>None</code> <code>multimask_output</code> <code>bool</code> <p>If true, the model will return three masks. For ambiguous input prompts (such as a single click), this will often produce better masks than a single prediction. If only a single mask is needed, the model's predicted quality score can be used to select the best mask. For non-ambiguous prompts, such as multiple input prompts, multimask_output=False can give better results.</p> <code>True</code> <code>return_logits</code> <code>bool</code> <p>If true, returns un-thresholded masks logits instead of a binary mask.</p> <code>False</code> <p>Returns:</p> Type Description <code>(np.ndarray)</code> <p>The output masks in CxHxW format, where C is the   number of masks, and (H, W) is the original image size. (np.ndarray): An array of length C containing the model's   predictions for the quality of each mask. (np.ndarray): An array of shape CxHxW, where C is the number   of masks and H=W=256. These low resolution logits can be passed to   a subsequent iteration as mask input.</p> Source code in <code>samgeo/samgeo.py</code> <pre><code>def predict(\n    self,\n    src_fp=None,\n    geo_box=None,\n    point_coords=None,\n    point_labels=None,\n    box=None,\n    mask_input=None,\n    multimask_output=True,\n    return_logits=False,\n):\n    if geo_box and src_fp:\n        self.crs = \"EPSG:4326\"\n        dst_crs = get_crs(src_fp)\n        sw = transform_coords(geo_box[0], geo_box[1], self.crs, dst_crs)\n        ne = transform_coords(geo_box[2], geo_box[3], self.crs, dst_crs)\n        xs = np.array([sw[0], ne[0]])\n        ys = np.array([sw[1], ne[1]])\n        box = get_pixel_coords(src_fp, xs, ys)\n        self.geo_box = geo_box\n        self.width = box[2] - box[0]\n        self.height = box[3] - box[1]\n        self.geo_transform = set_transform(geo_box, self.width, self.height)\n\n    masks, iou_predictions, low_res_masks = super(SamGeoPredictor, self).predict(\n        point_coords, point_labels, box, mask_input, multimask_output, return_logits\n    )\n\n    return masks, iou_predictions, low_res_masks\n</code></pre>"},{"location":"samgeo/#samgeo.samgeo.SamGeoPredictor.set_image","title":"<code>set_image(self, image)</code>","text":"<p>Calculates the image embeddings for the provided image, allowing masks to be predicted with the 'predict' method.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>np.ndarray</code> <p>The image for calculating masks. Expects an image in HWC uint8 format, with pixel values in [0, 255].</p> required <code>image_format</code> <code>str</code> <p>The color format of the image, in ['RGB', 'BGR'].</p> required Source code in <code>samgeo/samgeo.py</code> <pre><code>def set_image(self, image):\n    super(SamGeoPredictor, self).set_image(image)\n</code></pre>"},{"location":"text_sam/","title":"text_sam module","text":"<p>The LangSAM model for segmenting objects from satellite images using text prompts. The source code is adapted from the https://github.com/luca-medeiros/lang-segment-anything repository. Credits to Luca Medeiros for the original implementation.</p>"},{"location":"text_sam/#samgeo.text_sam.LangSAM","title":"<code> LangSAM        </code>","text":"<p>A Language-based Segment-Anything Model (LangSAM) class which combines GroundingDINO and SAM.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>class LangSAM:\n    \"\"\"\n    A Language-based Segment-Anything Model (LangSAM) class which combines GroundingDINO and SAM.\n    \"\"\"\n\n    def __init__(self, model_type=\"vit_h\", checkpoint=None):\n        \"\"\"Initialize the LangSAM instance.\n\n        Args:\n            model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n                Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        \"\"\"\n\n        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        self.build_groundingdino()\n        self.build_sam(model_type, checkpoint)\n\n        self.source = None\n        self.image = None\n        self.masks = None\n        self.boxes = None\n        self.phrases = None\n        self.logits = None\n        self.prediction = None\n\n    def build_sam(self, model_type, checkpoint_url=None):\n        \"\"\"Build the SAM model.\n\n        Args:\n            model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n                Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n            checkpoint_url:\n        \"\"\"\n        if checkpoint_url is not None:\n            sam = sam_model_registry[model_type](checkpoint=checkpoint_url)\n        else:\n            checkpoint_url = SAM_MODELS[model_type]\n            sam = sam_model_registry[model_type]()\n            state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)\n            sam.load_state_dict(state_dict, strict=True)\n        sam.to(device=self.device)\n        self.sam = SamPredictor(sam)\n\n    def build_groundingdino(self):\n        \"\"\"Build the GroundingDINO model.\"\"\"\n        ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n        ckpt_filename = \"groundingdino_swinb_cogcoor.pth\"\n        ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n        self.groundingdino = load_model_hf(\n            ckpt_repo_id, ckpt_filename, ckpt_config_filename, self.device\n        )\n\n    def predict_dino(self, image, text_prompt, box_threshold, text_threshold):\n        \"\"\"\n        Run the GroundingDINO model prediction.\n\n        Args:\n            image (Image): Input PIL Image.\n            text_prompt (str): Text prompt for the model.\n            box_threshold (float): Box threshold for the prediction.\n            text_threshold (float): Text threshold for the prediction.\n\n        Returns:\n            tuple: Tuple containing boxes, logits, and phrases.\n        \"\"\"\n\n        image_trans = transform_image(image)\n        boxes, logits, phrases = predict(\n            model=self.groundingdino,\n            image=image_trans,\n            caption=text_prompt,\n            box_threshold=box_threshold,\n            text_threshold=text_threshold,\n            device=self.device,\n        )\n        W, H = image.size\n        boxes = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n\n        return boxes, logits, phrases\n\n    def predict_sam(self, image, boxes):\n        \"\"\"\n        Run the SAM model prediction.\n\n        Args:\n            image (Image): Input PIL Image.\n            boxes (torch.Tensor): Tensor of bounding boxes.\n\n        Returns:\n            Masks tensor.\n        \"\"\"\n        image_array = np.asarray(image)\n        self.sam.set_image(image_array)\n        transformed_boxes = self.sam.transform.apply_boxes_torch(\n            boxes, image_array.shape[:2]\n        )\n        masks, _, _ = self.sam.predict_torch(\n            point_coords=None,\n            point_labels=None,\n            boxes=transformed_boxes.to(self.sam.device),\n            multimask_output=False,\n        )\n        return masks.cpu()\n\n    def set_image(self, image):\n        \"\"\"Set the input image.\n\n        Args:\n            image (str): The path to the image file or a HTTP URL.\n        \"\"\"\n\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n        else:\n            self.source = None\n\n    def predict(\n        self,\n        image,\n        text_prompt,\n        box_threshold,\n        text_threshold,\n        output=None,\n        mask_multiplier=255,\n        dtype=np.uint8,\n        save_args={},\n        return_results=False,\n        return_coords=False,\n        **kwargs,\n    ):\n        \"\"\"\n        Run both GroundingDINO and SAM model prediction.\n\n        Parameters:\n            image (Image): Input PIL Image.\n            text_prompt (str): Text prompt for the model.\n            box_threshold (float): Box threshold for the prediction.\n            text_threshold (float): Text threshold for the prediction.\n            output (str, optional): Output path for the prediction. Defaults to None.\n            mask_multiplier (int, optional): Mask multiplier for the prediction. Defaults to 255.\n            dtype (np.dtype, optional): Data type for the prediction. Defaults to np.uint8.\n            save_args (dict, optional): Save arguments for the prediction. Defaults to {}.\n            return_results (bool, optional): Whether to return the results. Defaults to False.\n\n        Returns:\n            tuple: Tuple containing masks, boxes, phrases, and logits.\n        \"\"\"\n\n        if isinstance(image, str):\n            if image.startswith(\"http\"):\n                image = download_file(image)\n\n            if not os.path.exists(image):\n                raise ValueError(f\"Input path {image} does not exist.\")\n\n            self.source = image\n\n            # Load the georeferenced image\n            with rasterio.open(image) as src:\n                image_np = src.read().transpose(\n                    (1, 2, 0)\n                )  # Convert rasterio image to numpy array\n                self.transform = src.transform  # Save georeferencing information\n                self.crs = src.crs  # Save the Coordinate Reference System\n                image_pil = Image.fromarray(\n                    image_np[:, :, :3]\n                )  # Convert numpy array to PIL image, excluding the alpha channel\n        else:\n            image_pil = image\n            image_np = np.array(image_pil)\n\n        self.image = image_pil\n\n        boxes, logits, phrases = self.predict_dino(\n            image_pil, text_prompt, box_threshold, text_threshold\n        )\n        masks = torch.tensor([])\n        if len(boxes) &gt; 0:\n            masks = self.predict_sam(image_pil, boxes)\n            masks = masks.squeeze(1)\n\n        if boxes.nelement() == 0:  # No \"object\" instances found\n            print(\"No objects found in the image.\")\n            return\n        else:\n            # Create an empty image to store the mask overlays\n            mask_overlay = np.zeros_like(\n                image_np[..., 0], dtype=dtype\n            )  # Adjusted for single channel\n\n            for i, (box, mask) in enumerate(zip(boxes, masks)):\n                # Convert tensor to numpy array if necessary and ensure it contains integers\n                if isinstance(mask, torch.Tensor):\n                    mask = (\n                        mask.cpu().numpy().astype(dtype)\n                    )  # If mask is on GPU, use .cpu() before .numpy()\n                mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                    dtype\n                )  # Assign a unique value for each mask\n\n            # Normalize mask_overlay to be in [0, 255]\n            mask_overlay = (\n                mask_overlay &gt; 0\n            ) * mask_multiplier  # Binary mask in [0, 255]\n\n        if output is not None:\n            array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n\n        self.masks = masks\n        self.boxes = boxes\n        self.phrases = phrases\n        self.logits = logits\n        self.prediction = mask_overlay\n\n        if return_results:\n            return masks, boxes, phrases, logits\n\n        if return_coords:\n            boxlist = []\n            for box in self.boxes:\n                box = box.cpu().numpy()\n                boxlist.append((box[0], box[1]))\n            return boxlist\n\n    def predict_batch(\n        self,\n        images,\n        out_dir,\n        text_prompt,\n        box_threshold,\n        text_threshold,\n        mask_multiplier=255,\n        dtype=np.uint8,\n        save_args={},\n        merge=True,\n        verbose=True,\n        **kwargs,\n    ):\n        \"\"\"\n        Run both GroundingDINO and SAM model prediction for a batch of images.\n\n        Parameters:\n            images (list): List of input PIL Images.\n            out_dir (str): Output directory for the prediction.\n            text_prompt (str): Text prompt for the model.\n            box_threshold (float): Box threshold for the prediction.\n            text_threshold (float): Text threshold for the prediction.\n            mask_multiplier (int, optional): Mask multiplier for the prediction. Defaults to 255.\n            dtype (np.dtype, optional): Data type for the prediction. Defaults to np.uint8.\n            save_args (dict, optional): Save arguments for the prediction. Defaults to {}.\n            merge (bool, optional): Whether to merge the predictions into a single GeoTIFF file. Defaults to True.\n        \"\"\"\n\n        import glob\n\n        if not os.path.exists(out_dir):\n            os.makedirs(out_dir)\n\n        if isinstance(images, str):\n            images = list(glob.glob(os.path.join(images, \"*.tif\")))\n            images.sort()\n\n        if not isinstance(images, list):\n            raise ValueError(\"images must be a list or a directory to GeoTIFF files.\")\n\n        for i, image in enumerate(images):\n            basename = os.path.splitext(os.path.basename(image))[0]\n            if verbose:\n                print(\n                    f\"Processing image {str(i+1).zfill(len(str(len(images))))} of {len(images)}: {image}...\"\n                )\n            output = os.path.join(out_dir, f\"{basename}_mask.tif\")\n            self.predict(\n                image,\n                text_prompt,\n                box_threshold,\n                text_threshold,\n                output=output,\n                mask_multiplier=mask_multiplier,\n                dtype=dtype,\n                save_args=save_args,\n                **kwargs,\n            )\n\n        if merge:\n            output = os.path.join(out_dir, \"merged.tif\")\n            merge_rasters(out_dir, output)\n            if verbose:\n                print(f\"Saved the merged prediction to {output}.\")\n\n    def save_boxes(self, output=None, dst_crs=\"EPSG:4326\", **kwargs):\n        \"\"\"Save the bounding boxes to a vector file.\n\n        Args:\n            output (str): The path to the output vector file.\n            dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n            **kwargs: Additional arguments for boxes_to_vector().\n        \"\"\"\n\n        if self.boxes is None:\n            print(\"Please run predict() first.\")\n            return\n        else:\n            boxes = self.boxes.tolist()\n            coords = rowcol_to_xy(self.source, boxes=boxes, dst_crs=dst_crs, **kwargs)\n            if output is None:\n                return boxes_to_vector(coords, self.crs, dst_crs, output)\n            else:\n                boxes_to_vector(coords, self.crs, dst_crs, output)\n\n    def show_anns(\n        self,\n        figsize=(12, 10),\n        axis=\"off\",\n        cmap=\"viridis\",\n        alpha=0.4,\n        add_boxes=True,\n        box_color=\"r\",\n        box_linewidth=1,\n        title=None,\n        output=None,\n        blend=True,\n        **kwargs,\n    ):\n        \"\"\"Show the annotations (objects with random color) on the input image.\n\n        Args:\n            figsize (tuple, optional): The figure size. Defaults to (12, 10).\n            axis (str, optional): Whether to show the axis. Defaults to \"off\".\n            cmap (str, optional): The colormap for the annotations. Defaults to \"viridis\".\n            alpha (float, optional): The alpha value for the annotations. Defaults to 0.4.\n            add_boxes (bool, optional): Whether to show the bounding boxes. Defaults to True.\n            box_color (str, optional): The color for the bounding boxes. Defaults to \"r\".\n            box_linewidth (int, optional): The line width for the bounding boxes. Defaults to 1.\n            title (str, optional): The title for the image. Defaults to None.\n            output (str, optional): The path to the output image. Defaults to None.\n            blend (bool, optional): Whether to show the input image. Defaults to True.\n            kwargs (dict, optional): Additional arguments for matplotlib.pyplot.savefig().\n        \"\"\"\n\n        import warnings\n        import matplotlib.pyplot as plt\n        import matplotlib.patches as patches\n\n        warnings.filterwarnings(\"ignore\")\n\n        anns = self.prediction\n\n        if anns is None:\n            print(\"Please run predict() first.\")\n            return\n        elif len(anns) == 0:\n            print(\"No objects found in the image.\")\n            return\n\n        plt.figure(figsize=figsize)\n        plt.imshow(self.image)\n\n        if add_boxes:\n            for box in self.boxes:\n                # Draw bounding box\n                box = box.cpu().numpy()  # Convert the tensor to a numpy array\n                rect = patches.Rectangle(\n                    (box[0], box[1]),\n                    box[2] - box[0],\n                    box[3] - box[1],\n                    linewidth=box_linewidth,\n                    edgecolor=box_color,\n                    facecolor=\"none\",\n                )\n                plt.gca().add_patch(rect)\n\n        if \"dpi\" not in kwargs:\n            kwargs[\"dpi\"] = 100\n\n        if \"bbox_inches\" not in kwargs:\n            kwargs[\"bbox_inches\"] = \"tight\"\n\n        plt.imshow(anns, cmap=cmap, alpha=alpha)\n\n        if title is not None:\n            plt.title(title)\n        plt.axis(axis)\n\n        if output is not None:\n            if blend:\n                plt.savefig(output, **kwargs)\n            else:\n                array_to_image(self.prediction, output, self.source)\n\n    def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n        \"\"\"Save the result to a vector file.\n\n        Args:\n            image (str): The path to the image file.\n            output (str): The path to the vector file.\n            simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n                The higher this value, the smaller the number of vertices in the resulting geometry.\n        \"\"\"\n\n        raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n\n    def show_map(self, basemap=\"SATELLITE\", out_dir=None, **kwargs):\n        \"\"\"Show the interactive map.\n\n        Args:\n            basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n            out_dir (str, optional): The path to the output directory. Defaults to None.\n\n        Returns:\n            leafmap.Map: The map object.\n        \"\"\"\n        return text_sam_gui(self, basemap=basemap, out_dir=out_dir, **kwargs)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.__init__","title":"<code>__init__(self, model_type='vit_h', checkpoint=None)</code>  <code>special</code>","text":"<p>Initialize the LangSAM instance.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. It can be one of the following: vit_h, vit_l, vit_b. Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> <code>'vit_h'</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def __init__(self, model_type=\"vit_h\", checkpoint=None):\n    \"\"\"Initialize the LangSAM instance.\n\n    Args:\n        model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n    \"\"\"\n\n    self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    self.build_groundingdino()\n    self.build_sam(model_type, checkpoint)\n\n    self.source = None\n    self.image = None\n    self.masks = None\n    self.boxes = None\n    self.phrases = None\n    self.logits = None\n    self.prediction = None\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.build_groundingdino","title":"<code>build_groundingdino(self)</code>","text":"<p>Build the GroundingDINO model.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def build_groundingdino(self):\n    \"\"\"Build the GroundingDINO model.\"\"\"\n    ckpt_repo_id = \"ShilongLiu/GroundingDINO\"\n    ckpt_filename = \"groundingdino_swinb_cogcoor.pth\"\n    ckpt_config_filename = \"GroundingDINO_SwinB.cfg.py\"\n    self.groundingdino = load_model_hf(\n        ckpt_repo_id, ckpt_filename, ckpt_config_filename, self.device\n    )\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.build_sam","title":"<code>build_sam(self, model_type, checkpoint_url=None)</code>","text":"<p>Build the SAM model.</p> <p>Parameters:</p> Name Type Description Default <code>model_type</code> <code>str</code> <p>The model type. It can be one of the following: vit_h, vit_l, vit_b. Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.</p> required <code>checkpoint_url</code> <code>None</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def build_sam(self, model_type, checkpoint_url=None):\n    \"\"\"Build the SAM model.\n\n    Args:\n        model_type (str, optional): The model type. It can be one of the following: vit_h, vit_l, vit_b.\n            Defaults to 'vit_h'. See https://bit.ly/3VrpxUh for more details.\n        checkpoint_url:\n    \"\"\"\n    if checkpoint_url is not None:\n        sam = sam_model_registry[model_type](checkpoint=checkpoint_url)\n    else:\n        checkpoint_url = SAM_MODELS[model_type]\n        sam = sam_model_registry[model_type]()\n        state_dict = torch.hub.load_state_dict_from_url(checkpoint_url)\n        sam.load_state_dict(state_dict, strict=True)\n    sam.to(device=self.device)\n    self.sam = SamPredictor(sam)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.predict","title":"<code>predict(self, image, text_prompt, box_threshold, text_threshold, output=None, mask_multiplier=255, dtype=&lt;class 'numpy.uint8'&gt;, save_args={}, return_results=False, return_coords=False, **kwargs)</code>","text":"<p>Run both GroundingDINO and SAM model prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input PIL Image.</p> required <code>text_prompt</code> <code>str</code> <p>Text prompt for the model.</p> required <code>box_threshold</code> <code>float</code> <p>Box threshold for the prediction.</p> required <code>text_threshold</code> <code>float</code> <p>Text threshold for the prediction.</p> required <code>output</code> <code>str</code> <p>Output path for the prediction. Defaults to None.</p> <code>None</code> <code>mask_multiplier</code> <code>int</code> <p>Mask multiplier for the prediction. Defaults to 255.</p> <code>255</code> <code>dtype</code> <code>np.dtype</code> <p>Data type for the prediction. Defaults to np.uint8.</p> <code>&lt;class 'numpy.uint8'&gt;</code> <code>save_args</code> <code>dict</code> <p>Save arguments for the prediction. Defaults to {}.</p> <code>{}</code> <code>return_results</code> <code>bool</code> <p>Whether to return the results. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing masks, boxes, phrases, and logits.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def predict(\n    self,\n    image,\n    text_prompt,\n    box_threshold,\n    text_threshold,\n    output=None,\n    mask_multiplier=255,\n    dtype=np.uint8,\n    save_args={},\n    return_results=False,\n    return_coords=False,\n    **kwargs,\n):\n    \"\"\"\n    Run both GroundingDINO and SAM model prediction.\n\n    Parameters:\n        image (Image): Input PIL Image.\n        text_prompt (str): Text prompt for the model.\n        box_threshold (float): Box threshold for the prediction.\n        text_threshold (float): Text threshold for the prediction.\n        output (str, optional): Output path for the prediction. Defaults to None.\n        mask_multiplier (int, optional): Mask multiplier for the prediction. Defaults to 255.\n        dtype (np.dtype, optional): Data type for the prediction. Defaults to np.uint8.\n        save_args (dict, optional): Save arguments for the prediction. Defaults to {}.\n        return_results (bool, optional): Whether to return the results. Defaults to False.\n\n    Returns:\n        tuple: Tuple containing masks, boxes, phrases, and logits.\n    \"\"\"\n\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n\n        # Load the georeferenced image\n        with rasterio.open(image) as src:\n            image_np = src.read().transpose(\n                (1, 2, 0)\n            )  # Convert rasterio image to numpy array\n            self.transform = src.transform  # Save georeferencing information\n            self.crs = src.crs  # Save the Coordinate Reference System\n            image_pil = Image.fromarray(\n                image_np[:, :, :3]\n            )  # Convert numpy array to PIL image, excluding the alpha channel\n    else:\n        image_pil = image\n        image_np = np.array(image_pil)\n\n    self.image = image_pil\n\n    boxes, logits, phrases = self.predict_dino(\n        image_pil, text_prompt, box_threshold, text_threshold\n    )\n    masks = torch.tensor([])\n    if len(boxes) &gt; 0:\n        masks = self.predict_sam(image_pil, boxes)\n        masks = masks.squeeze(1)\n\n    if boxes.nelement() == 0:  # No \"object\" instances found\n        print(\"No objects found in the image.\")\n        return\n    else:\n        # Create an empty image to store the mask overlays\n        mask_overlay = np.zeros_like(\n            image_np[..., 0], dtype=dtype\n        )  # Adjusted for single channel\n\n        for i, (box, mask) in enumerate(zip(boxes, masks)):\n            # Convert tensor to numpy array if necessary and ensure it contains integers\n            if isinstance(mask, torch.Tensor):\n                mask = (\n                    mask.cpu().numpy().astype(dtype)\n                )  # If mask is on GPU, use .cpu() before .numpy()\n            mask_overlay += ((mask &gt; 0) * (i + 1)).astype(\n                dtype\n            )  # Assign a unique value for each mask\n\n        # Normalize mask_overlay to be in [0, 255]\n        mask_overlay = (\n            mask_overlay &gt; 0\n        ) * mask_multiplier  # Binary mask in [0, 255]\n\n    if output is not None:\n        array_to_image(mask_overlay, output, self.source, dtype=dtype, **save_args)\n\n    self.masks = masks\n    self.boxes = boxes\n    self.phrases = phrases\n    self.logits = logits\n    self.prediction = mask_overlay\n\n    if return_results:\n        return masks, boxes, phrases, logits\n\n    if return_coords:\n        boxlist = []\n        for box in self.boxes:\n            box = box.cpu().numpy()\n            boxlist.append((box[0], box[1]))\n        return boxlist\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.predict_batch","title":"<code>predict_batch(self, images, out_dir, text_prompt, box_threshold, text_threshold, mask_multiplier=255, dtype=&lt;class 'numpy.uint8'&gt;, save_args={}, merge=True, verbose=True, **kwargs)</code>","text":"<p>Run both GroundingDINO and SAM model prediction for a batch of images.</p> <p>Parameters:</p> Name Type Description Default <code>images</code> <code>list</code> <p>List of input PIL Images.</p> required <code>out_dir</code> <code>str</code> <p>Output directory for the prediction.</p> required <code>text_prompt</code> <code>str</code> <p>Text prompt for the model.</p> required <code>box_threshold</code> <code>float</code> <p>Box threshold for the prediction.</p> required <code>text_threshold</code> <code>float</code> <p>Text threshold for the prediction.</p> required <code>mask_multiplier</code> <code>int</code> <p>Mask multiplier for the prediction. Defaults to 255.</p> <code>255</code> <code>dtype</code> <code>np.dtype</code> <p>Data type for the prediction. Defaults to np.uint8.</p> <code>&lt;class 'numpy.uint8'&gt;</code> <code>save_args</code> <code>dict</code> <p>Save arguments for the prediction. Defaults to {}.</p> <code>{}</code> <code>merge</code> <code>bool</code> <p>Whether to merge the predictions into a single GeoTIFF file. Defaults to True.</p> <code>True</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def predict_batch(\n    self,\n    images,\n    out_dir,\n    text_prompt,\n    box_threshold,\n    text_threshold,\n    mask_multiplier=255,\n    dtype=np.uint8,\n    save_args={},\n    merge=True,\n    verbose=True,\n    **kwargs,\n):\n    \"\"\"\n    Run both GroundingDINO and SAM model prediction for a batch of images.\n\n    Parameters:\n        images (list): List of input PIL Images.\n        out_dir (str): Output directory for the prediction.\n        text_prompt (str): Text prompt for the model.\n        box_threshold (float): Box threshold for the prediction.\n        text_threshold (float): Text threshold for the prediction.\n        mask_multiplier (int, optional): Mask multiplier for the prediction. Defaults to 255.\n        dtype (np.dtype, optional): Data type for the prediction. Defaults to np.uint8.\n        save_args (dict, optional): Save arguments for the prediction. Defaults to {}.\n        merge (bool, optional): Whether to merge the predictions into a single GeoTIFF file. Defaults to True.\n    \"\"\"\n\n    import glob\n\n    if not os.path.exists(out_dir):\n        os.makedirs(out_dir)\n\n    if isinstance(images, str):\n        images = list(glob.glob(os.path.join(images, \"*.tif\")))\n        images.sort()\n\n    if not isinstance(images, list):\n        raise ValueError(\"images must be a list or a directory to GeoTIFF files.\")\n\n    for i, image in enumerate(images):\n        basename = os.path.splitext(os.path.basename(image))[0]\n        if verbose:\n            print(\n                f\"Processing image {str(i+1).zfill(len(str(len(images))))} of {len(images)}: {image}...\"\n            )\n        output = os.path.join(out_dir, f\"{basename}_mask.tif\")\n        self.predict(\n            image,\n            text_prompt,\n            box_threshold,\n            text_threshold,\n            output=output,\n            mask_multiplier=mask_multiplier,\n            dtype=dtype,\n            save_args=save_args,\n            **kwargs,\n        )\n\n    if merge:\n        output = os.path.join(out_dir, \"merged.tif\")\n        merge_rasters(out_dir, output)\n        if verbose:\n            print(f\"Saved the merged prediction to {output}.\")\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.predict_dino","title":"<code>predict_dino(self, image, text_prompt, box_threshold, text_threshold)</code>","text":"<p>Run the GroundingDINO model prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input PIL Image.</p> required <code>text_prompt</code> <code>str</code> <p>Text prompt for the model.</p> required <code>box_threshold</code> <code>float</code> <p>Box threshold for the prediction.</p> required <code>text_threshold</code> <code>float</code> <p>Text threshold for the prediction.</p> required <p>Returns:</p> Type Description <code>tuple</code> <p>Tuple containing boxes, logits, and phrases.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def predict_dino(self, image, text_prompt, box_threshold, text_threshold):\n    \"\"\"\n    Run the GroundingDINO model prediction.\n\n    Args:\n        image (Image): Input PIL Image.\n        text_prompt (str): Text prompt for the model.\n        box_threshold (float): Box threshold for the prediction.\n        text_threshold (float): Text threshold for the prediction.\n\n    Returns:\n        tuple: Tuple containing boxes, logits, and phrases.\n    \"\"\"\n\n    image_trans = transform_image(image)\n    boxes, logits, phrases = predict(\n        model=self.groundingdino,\n        image=image_trans,\n        caption=text_prompt,\n        box_threshold=box_threshold,\n        text_threshold=text_threshold,\n        device=self.device,\n    )\n    W, H = image.size\n    boxes = box_ops.box_cxcywh_to_xyxy(boxes) * torch.Tensor([W, H, W, H])\n\n    return boxes, logits, phrases\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.predict_sam","title":"<code>predict_sam(self, image, boxes)</code>","text":"<p>Run the SAM model prediction.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>Input PIL Image.</p> required <code>boxes</code> <code>torch.Tensor</code> <p>Tensor of bounding boxes.</p> required <p>Returns:</p> Type Description <p>Masks tensor.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def predict_sam(self, image, boxes):\n    \"\"\"\n    Run the SAM model prediction.\n\n    Args:\n        image (Image): Input PIL Image.\n        boxes (torch.Tensor): Tensor of bounding boxes.\n\n    Returns:\n        Masks tensor.\n    \"\"\"\n    image_array = np.asarray(image)\n    self.sam.set_image(image_array)\n    transformed_boxes = self.sam.transform.apply_boxes_torch(\n        boxes, image_array.shape[:2]\n    )\n    masks, _, _ = self.sam.predict_torch(\n        point_coords=None,\n        point_labels=None,\n        boxes=transformed_boxes.to(self.sam.device),\n        multimask_output=False,\n    )\n    return masks.cpu()\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.raster_to_vector","title":"<code>raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs)</code>","text":"<p>Save the result to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file.</p> required <code>output</code> <code>str</code> <p>The path to the vector file.</p> required <code>simplify_tolerance</code> <code>float</code> <p>The maximum allowed geometry displacement. The higher this value, the smaller the number of vertices in the resulting geometry.</p> <code>None</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def raster_to_vector(self, image, output, simplify_tolerance=None, **kwargs):\n    \"\"\"Save the result to a vector file.\n\n    Args:\n        image (str): The path to the image file.\n        output (str): The path to the vector file.\n        simplify_tolerance (float, optional): The maximum allowed geometry displacement.\n            The higher this value, the smaller the number of vertices in the resulting geometry.\n    \"\"\"\n\n    raster_to_vector(image, output, simplify_tolerance=simplify_tolerance, **kwargs)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.save_boxes","title":"<code>save_boxes(self, output=None, dst_crs='EPSG:4326', **kwargs)</code>","text":"<p>Save the bounding boxes to a vector file.</p> <p>Parameters:</p> Name Type Description Default <code>output</code> <code>str</code> <p>The path to the output vector file.</p> <code>None</code> <code>dst_crs</code> <code>str</code> <p>The destination CRS. Defaults to \"EPSG:4326\".</p> <code>'EPSG:4326'</code> <code>**kwargs</code> <p>Additional arguments for boxes_to_vector().</p> <code>{}</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def save_boxes(self, output=None, dst_crs=\"EPSG:4326\", **kwargs):\n    \"\"\"Save the bounding boxes to a vector file.\n\n    Args:\n        output (str): The path to the output vector file.\n        dst_crs (str, optional): The destination CRS. Defaults to \"EPSG:4326\".\n        **kwargs: Additional arguments for boxes_to_vector().\n    \"\"\"\n\n    if self.boxes is None:\n        print(\"Please run predict() first.\")\n        return\n    else:\n        boxes = self.boxes.tolist()\n        coords = rowcol_to_xy(self.source, boxes=boxes, dst_crs=dst_crs, **kwargs)\n        if output is None:\n            return boxes_to_vector(coords, self.crs, dst_crs, output)\n        else:\n            boxes_to_vector(coords, self.crs, dst_crs, output)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.set_image","title":"<code>set_image(self, image)</code>","text":"<p>Set the input image.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>str</code> <p>The path to the image file or a HTTP URL.</p> required Source code in <code>samgeo/text_sam.py</code> <pre><code>def set_image(self, image):\n    \"\"\"Set the input image.\n\n    Args:\n        image (str): The path to the image file or a HTTP URL.\n    \"\"\"\n\n    if isinstance(image, str):\n        if image.startswith(\"http\"):\n            image = download_file(image)\n\n        if not os.path.exists(image):\n            raise ValueError(f\"Input path {image} does not exist.\")\n\n        self.source = image\n    else:\n        self.source = None\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.show_anns","title":"<code>show_anns(self, figsize=(12, 10), axis='off', cmap='viridis', alpha=0.4, add_boxes=True, box_color='r', box_linewidth=1, title=None, output=None, blend=True, **kwargs)</code>","text":"<p>Show the annotations (objects with random color) on the input image.</p> <p>Parameters:</p> Name Type Description Default <code>figsize</code> <code>tuple</code> <p>The figure size. Defaults to (12, 10).</p> <code>(12, 10)</code> <code>axis</code> <code>str</code> <p>Whether to show the axis. Defaults to \"off\".</p> <code>'off'</code> <code>cmap</code> <code>str</code> <p>The colormap for the annotations. Defaults to \"viridis\".</p> <code>'viridis'</code> <code>alpha</code> <code>float</code> <p>The alpha value for the annotations. Defaults to 0.4.</p> <code>0.4</code> <code>add_boxes</code> <code>bool</code> <p>Whether to show the bounding boxes. Defaults to True.</p> <code>True</code> <code>box_color</code> <code>str</code> <p>The color for the bounding boxes. Defaults to \"r\".</p> <code>'r'</code> <code>box_linewidth</code> <code>int</code> <p>The line width for the bounding boxes. Defaults to 1.</p> <code>1</code> <code>title</code> <code>str</code> <p>The title for the image. Defaults to None.</p> <code>None</code> <code>output</code> <code>str</code> <p>The path to the output image. Defaults to None.</p> <code>None</code> <code>blend</code> <code>bool</code> <p>Whether to show the input image. Defaults to True.</p> <code>True</code> <code>kwargs</code> <code>dict</code> <p>Additional arguments for matplotlib.pyplot.savefig().</p> <code>{}</code> Source code in <code>samgeo/text_sam.py</code> <pre><code>def show_anns(\n    self,\n    figsize=(12, 10),\n    axis=\"off\",\n    cmap=\"viridis\",\n    alpha=0.4,\n    add_boxes=True,\n    box_color=\"r\",\n    box_linewidth=1,\n    title=None,\n    output=None,\n    blend=True,\n    **kwargs,\n):\n    \"\"\"Show the annotations (objects with random color) on the input image.\n\n    Args:\n        figsize (tuple, optional): The figure size. Defaults to (12, 10).\n        axis (str, optional): Whether to show the axis. Defaults to \"off\".\n        cmap (str, optional): The colormap for the annotations. Defaults to \"viridis\".\n        alpha (float, optional): The alpha value for the annotations. Defaults to 0.4.\n        add_boxes (bool, optional): Whether to show the bounding boxes. Defaults to True.\n        box_color (str, optional): The color for the bounding boxes. Defaults to \"r\".\n        box_linewidth (int, optional): The line width for the bounding boxes. Defaults to 1.\n        title (str, optional): The title for the image. Defaults to None.\n        output (str, optional): The path to the output image. Defaults to None.\n        blend (bool, optional): Whether to show the input image. Defaults to True.\n        kwargs (dict, optional): Additional arguments for matplotlib.pyplot.savefig().\n    \"\"\"\n\n    import warnings\n    import matplotlib.pyplot as plt\n    import matplotlib.patches as patches\n\n    warnings.filterwarnings(\"ignore\")\n\n    anns = self.prediction\n\n    if anns is None:\n        print(\"Please run predict() first.\")\n        return\n    elif len(anns) == 0:\n        print(\"No objects found in the image.\")\n        return\n\n    plt.figure(figsize=figsize)\n    plt.imshow(self.image)\n\n    if add_boxes:\n        for box in self.boxes:\n            # Draw bounding box\n            box = box.cpu().numpy()  # Convert the tensor to a numpy array\n            rect = patches.Rectangle(\n                (box[0], box[1]),\n                box[2] - box[0],\n                box[3] - box[1],\n                linewidth=box_linewidth,\n                edgecolor=box_color,\n                facecolor=\"none\",\n            )\n            plt.gca().add_patch(rect)\n\n    if \"dpi\" not in kwargs:\n        kwargs[\"dpi\"] = 100\n\n    if \"bbox_inches\" not in kwargs:\n        kwargs[\"bbox_inches\"] = \"tight\"\n\n    plt.imshow(anns, cmap=cmap, alpha=alpha)\n\n    if title is not None:\n        plt.title(title)\n    plt.axis(axis)\n\n    if output is not None:\n        if blend:\n            plt.savefig(output, **kwargs)\n        else:\n            array_to_image(self.prediction, output, self.source)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.LangSAM.show_map","title":"<code>show_map(self, basemap='SATELLITE', out_dir=None, **kwargs)</code>","text":"<p>Show the interactive map.</p> <p>Parameters:</p> Name Type Description Default <code>basemap</code> <code>str</code> <p>The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.</p> <code>'SATELLITE'</code> <code>out_dir</code> <code>str</code> <p>The path to the output directory. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>leafmap.Map</code> <p>The map object.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def show_map(self, basemap=\"SATELLITE\", out_dir=None, **kwargs):\n    \"\"\"Show the interactive map.\n\n    Args:\n        basemap (str, optional): The basemap. It can be one of the following: SATELLITE, ROADMAP, TERRAIN, HYBRID.\n        out_dir (str, optional): The path to the output directory. Defaults to None.\n\n    Returns:\n        leafmap.Map: The map object.\n    \"\"\"\n    return text_sam_gui(self, basemap=basemap, out_dir=out_dir, **kwargs)\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.load_model_hf","title":"<code>load_model_hf(repo_id, filename, ckpt_config_filename, device='cpu')</code>","text":"<p>Loads a model from HuggingFace Model Hub.</p> <p>Parameters:</p> Name Type Description Default <code>repo_id</code> <code>str</code> <p>Repository ID on HuggingFace Model Hub.</p> required <code>filename</code> <code>str</code> <p>Name of the model file in the repository.</p> required <code>ckpt_config_filename</code> <code>str</code> <p>Name of the config file for the model in the repository.</p> required <code>device</code> <code>str</code> <p>Device to load the model onto. Default is 'cpu'.</p> <code>'cpu'</code> <p>Returns:</p> Type Description <code>torch.nn.Module</code> <p>The loaded model.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def load_model_hf(\n    repo_id: str, filename: str, ckpt_config_filename: str, device: str = \"cpu\"\n) -&gt; torch.nn.Module:\n    \"\"\"\n    Loads a model from HuggingFace Model Hub.\n\n    Args:\n        repo_id (str): Repository ID on HuggingFace Model Hub.\n        filename (str): Name of the model file in the repository.\n        ckpt_config_filename (str): Name of the config file for the model in the repository.\n        device (str): Device to load the model onto. Default is 'cpu'.\n\n    Returns:\n        torch.nn.Module: The loaded model.\n    \"\"\"\n\n    cache_config_file = hf_hub_download(\n        repo_id=repo_id,\n        filename=ckpt_config_filename,\n        force_filename=ckpt_config_filename,\n    )\n    args = SLConfig.fromfile(cache_config_file)\n    model = build_model(args)\n    model.to(device)\n    cache_file = hf_hub_download(\n        repo_id=repo_id, filename=filename, force_filename=filename\n    )\n    checkpoint = torch.load(cache_file, map_location=\"cpu\")\n    model.load_state_dict(clean_state_dict(checkpoint[\"model\"]), strict=False)\n    model.eval()\n    return model\n</code></pre>"},{"location":"text_sam/#samgeo.text_sam.transform_image","title":"<code>transform_image(image)</code>","text":"<p>Transforms an image using standard transformations for image-based models.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>Image</code> <p>The PIL Image to be transformed.</p> required <p>Returns:</p> Type Description <code>torch.Tensor</code> <p>The transformed image as a tensor.</p> Source code in <code>samgeo/text_sam.py</code> <pre><code>def transform_image(image: Image) -&gt; torch.Tensor:\n    \"\"\"\n    Transforms an image using standard transformations for image-based models.\n\n    Args:\n        image (Image): The PIL Image to be transformed.\n\n    Returns:\n        torch.Tensor: The transformed image as a tensor.\n    \"\"\"\n    transform = T.Compose(\n        [\n            T.RandomResize([800], max_size=1333),\n            T.ToTensor(),\n            T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n        ]\n    )\n    image_transformed, _ = transform(image, None)\n    return image_transformed\n</code></pre>"},{"location":"usage/","title":"Usage","text":"<p>To use segment-geospatial in a project:</p> <pre><code>import samgeo\n</code></pre> <p>Here is a simple example of using segment-geospatial to generate a segmentation mask from a satellite image:</p> <pre><code>import os\nimport torch\nfrom samgeo import SamGeo, tms_to_geotiff\n\nbbox = [-95.3704, 29.6762, -95.368, 29.6775]\nimage = 'satellite.tif'\ntms_to_geotiff(output=image, bbox=bbox, zoom=20, source='Satellite')\n\nout_dir = os.path.join(os.path.expanduser('~'), 'Downloads')\ncheckpoint = os.path.join(out_dir, 'sam_vit_h_4b8939.pth')\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nsam = SamGeo(\n    checkpoint=checkpoint,\n    model_type='vit_h',\n    device=device,\n    erosion_kernel=(3, 3),\n    mask_multiplier=255,\n    sam_kwargs=None,\n)\n\nmask = 'segment.tif'\nsam.generate(image, mask)\n\nvector = 'segment.gpkg'\nsam.tiff_to_gpkg(mask, vector, simplify_tolerance=None)\n</code></pre> <p></p>"},{"location":"examples/arcgis/","title":"Arcgis","text":"In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo\n\n%matplotlib inline\n</pre> import os import leafmap from samgeo import SamGeo  %matplotlib inline In\u00a0[\u00a0]: Copied! <pre>workspace = os.path.dirname(arcpy.env.workspace)\nos.chdir(workspace)\narcpy.env.overwriteOutput = True\n</pre> workspace = os.path.dirname(arcpy.env.workspace) os.chdir(workspace) arcpy.env.overwriteOutput = True In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/buildings.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/agriculture.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(\n    url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",\n    quiet=True,\n    overwrite=True,\n)\n</pre> leafmap.download_file(     url=\"https://github.com/opengeos/data/blob/main/naip/water.tif\",     quiet=True,     overwrite=True, ) In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>image = \"agriculture.tif\"\n</pre> image = \"agriculture.tif\" <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p> In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"ag_masks.tif\", foreground=True, unique=True) <p>If you run into GPU memory errors, uncomment the following code block and run it to empty cuda cache then rerun the code block above.</p> In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() <p>Show the segmentation result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"ag_annotations.tif\") <p>Add layers to ArcGIS Pro.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.arc_active_map()\n</pre> m = leafmap.arc_active_map() In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"agriculture.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"ag_annotations.tif\")) <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"ag_masks.tif\")\nout_shp = os.path.join(workspace, \"ag_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"ag_masks.tif\") out_shp = os.path.join(workspace, \"ag_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>image = \"water.tif\"\n</pre> image = \"water.tif\" In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"water_masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre># sam.clear_cuda_cache()\n</pre> # sam.clear_cuda_cache() In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"water_annotations.tif\") In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water.tif\")) In\u00a0[\u00a0]: Copied! <pre>m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\"))\n</pre> m.addDataFromPath(os.path.join(workspace, \"water_annotations.tif\")) In\u00a0[\u00a0]: Copied! <pre>in_raster = os.path.join(workspace, \"water_masks.tif\")\nout_shp = os.path.join(workspace, \"water_masks.shp\")\n</pre> in_raster = os.path.join(workspace, \"water_masks.tif\") out_shp = os.path.join(workspace, \"water_masks.shp\") In\u00a0[\u00a0]: Copied! <pre>arcpy.conversion.RasterToPolygon(in_raster, out_shp)\n</pre> arcpy.conversion.RasterToPolygon(in_raster, out_shp) In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True)\n</pre> sam.generate(\"agriculture.tif\", output=\"ag_masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=0.5, output=\"ag_annotations2.tif\")"},{"location":"examples/arcgis/#using-the-segment-geospatial-python-package-with-arcgis-pro","title":"Using the Segment-Geospatial Python Package with ArcGIS Pro\u00b6","text":"<p>The notebook shows step-by-step instructions for using the Segment Anything Model (SAM) with ArcGIS Pro. Check out the YouTube tutorial here and the Resources for Unlocking the Power of Deep Learning Applications Using ArcGIS. Credit goes to Esri.</p> <p></p>"},{"location":"examples/arcgis/#installation","title":"Installation\u00b6","text":"<ol> <li><p>Open Windows Registry Editor (<code>regedit.exe</code>) and navigate to <code>Computer\\HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem</code>. Change the value of <code>LongPathsEnabled</code> to <code>1</code>. See this screenshot. This is a known issue with the deep learning libraries for ArcGIS Pro 3.1. A future release might fix this issue.</p> </li> <li><p>Navigate to the Start Menu -&gt; All apps -&gt; ArcGIS folder, then open the Python Command Prompt.</p> </li> <li><p>Create a new conda environment and install mamba and Python 3.9.x from the Esri Anaconda channel. Mamba is a drop-in replacement for conda that is mach faster for installing Python packages and their dependencies.</p> <p><code>conda create conda-forge::mamba esri::python --name geo</code></p> </li> <li><p>Activate the new conda environment.</p> <p><code>conda activate geo</code></p> </li> <li><p>This step is optional. If you get an error message saying that <code>Download error (60) SSL peer certificate or SSH remote key was not OK</code> when installing packages in the next step, run the following command to fix the issue.</p> <p><code>conda config --set ssl_verify false</code></p> </li> <li><p>Install arcpy, deep-learning-essentials, segment-geospatial, and other dependencies (~4GB download).</p> <p><code>mamba install arcpy deep-learning-essentials segment-geospatial pygis -c esri -c conda-forge</code></p> </li> <li><p>Activate the new environment in ArcGIS Pro.</p> <p><code>proswap geo</code></p> </li> <li><p>Close the Python Command Prompt and open ArcGIS Pro.</p> </li> <li><p>Download this notebook and run it in ArcGIS Pro.</p> </li> </ol>"},{"location":"examples/arcgis/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/arcgis/#download-sample-data","title":"Download sample data\u00b6","text":"<p>In this example, we will use the high-resolution aerial imagery from the USDA National Agricultural Imagery Program (NAIP). You can download NAIP imagery using the USDA Data Gateway or the USDA NCRS Box Drive. I have downloaded some NAIP imagery and clipped them to a smaller area, which are available here.</p>"},{"location":"examples/arcgis/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/arcgis/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Specify the file path to the image we downloaded earlier.</p>"},{"location":"examples/arcgis/#segment-waterbodies","title":"Segment waterbodies\u00b6","text":""},{"location":"examples/arcgis/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/automatic_mask_generator/","title":"Automatic mask generator","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, show_image, download_file, overlay_images, tms_to_geotiff\n</pre> import os import leafmap from samgeo import SamGeo, show_image, download_file, overlay_images, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/automatic_mask_generator/#automatically-generating-object-masks-with-sam","title":"Automatically generating object masks with SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/automatic_mask_generator_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/automatic_mask_generator/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/automatic_mask_generator/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/automatic_mask_generator/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/automatic_mask_generator/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/automatic_mask_generator/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/automatic_mask_generator/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/automatic_mask_generator_hq/","title":"Automatic mask generator hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo.hq_sam import (\n    SamGeo,\n    show_image,\n    download_file,\n    overlay_images,\n    tms_to_geotiff,\n)\n</pre> import os import leafmap from samgeo.hq_sam import (     SamGeo,     show_image,     download_file,     overlay_images,     tms_to_geotiff, ) In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.8713, -122.2580], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.2659, 37.8682, -122.2521, 37.8741]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.2659, 37.8682, -122.2521, 37.8741] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=17, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"satellite.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"satellite.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", alpha=0.5, layer_name=\"Masks\") m <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\")\n</pre> sam.tiff_to_vector(\"masks.tif\", \"masks.gpkg\") In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\")\n</pre> overlay_images(image, \"annotations2.tif\", backend=\"TkAgg\") <p></p>"},{"location":"examples/automatic_mask_generator_hq/#automatically-generating-object-masks-with-hq-sam","title":"Automatically generating object masks with HQ-SAM\u00b6","text":"<p>This notebook shows how to segment objects from an image using the High-Quality Segment Anything Model (HQ-SAM) with a few lines of code.</p>"},{"location":"examples/automatic_mask_generator_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/automatic_mask_generator_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/automatic_mask_generator_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":""},{"location":"examples/automatic_mask_generator_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/automatic_mask_generator_hq/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"examples/automatic_mask_generator_hq/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"examples/box_prompts/","title":"Box prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo import SamGeo\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) <p>Display the map. Use the drawing tools to draw some rectangles around the features you want to extract, such as trees, buildings.</p> In\u00a0[\u00a0]: Copied! <pre>m\n</pre> m In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-51.2546, -22.1771, -51.2541, -22.1767],\n        [-51.2538, -22.1764, -51.2535, -22.1761],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-51.2546, -22.1771, -51.2541, -22.1767],         [-51.2538, -22.1764, -51.2535, -22.1761],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m In\u00a0[\u00a0]: Copied! <pre>url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\"\ngeojson = \"tree_boxes.geojson\"\nleafmap.download_file(url, geojson)\n</pre> url = \"https://opengeos.github.io/data/sam/tree_boxes.geojson\" geojson = \"tree_boxes.geojson\" leafmap.download_file(url, geojson) <p>Display the vector data on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map()\nm.add_raster(\"Image.tif\", layer_name=\"image\")\nstyle = {\n    \"color\": \"#ffff00\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0,\n}\nm.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\")\nm\n</pre> m = leafmap.Map() m.add_raster(\"Image.tif\", layer_name=\"image\") style = {     \"color\": \"#ffff00\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0, } m.add_vector(geojson, style=style, zoom_to_layer=True, layer_name=\"Bounding boxes\") m In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=geojson, point_crs=\"EPSG:4326\", output=\"mask2.tif\", dtype=\"uint8\") <p>Display the segmented masks on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\")\nm\n</pre> m.add_raster(\"mask2.tif\", cmap=\"Greens\", nodata=0, opacity=0.5, layer_name=\"Tree masks\") m <p></p>"},{"location":"examples/box_prompts/#segmenting-remote-sensing-imagery-with-box-prompts","title":"Segmenting remote sensing imagery with box prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/box_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/box_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/box_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/box_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p> <p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p> <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p>"},{"location":"examples/box_prompts/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"examples/box_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"examples/box_prompts/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"examples/box_prompts/#use-an-existing-vector-file-as-box-prompts","title":"Use an existing vector file as box prompts\u00b6","text":"<p>Alternatively, you can specify a file path to a vector file. Let's download a sample vector file from GitHub.</p>"},{"location":"examples/box_prompts/#segment-image-with-box-prompts","title":"Segment image with box prompts\u00b6","text":"<p>Segment the image using the specified file path to the vector mask.</p>"},{"location":"examples/fast_sam/","title":"Fast sam","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial segment-anything-fast\n</pre> # %pip install segment-geospatial segment-anything-fast In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.fast_sam import SamGeo\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.fast_sam import SamGeo In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>from samgeo.fast_sam import SamGeo\n\nsam = SamGeo(model=\"FastSAM-x.pt\")\n</pre> from samgeo.fast_sam import SamGeo  sam = SamGeo(model=\"FastSAM-x.pt\") <p>Set the image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(\"Image.tif\")\n</pre> sam.set_image(\"Image.tif\") <p>Segment the image with <code>everything_prompt</code>. You can also try <code>point_prompt</code>, <code>box_prompt</code>, or <code>text_prompt</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam.everything_prompt(output=\"mask.tif\")\n</pre> sam.everything_prompt(output=\"mask.tif\") <p>Show the annotated image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\"mask.png\")\n</pre> sam.show_anns(\"mask.png\") <p></p> <p>Convert the segmentation results from GeoTIFF to vector.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"mask.tif\", \"mask.geojson\")\n</pre> sam.raster_to_vector(\"mask.tif\", \"mask.geojson\") <p>Show the segmentation results on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\")\nm.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\")\nm\n</pre> m.add_raster(\"mask.tif\", opacity=0.5, layer_name=\"Mask\") m.add_vector(\"mask.geojson\", layer_name=\"Mask Vector\") m <p></p>"},{"location":"examples/fast_sam/#segmenting-remote-sensing-imagery-with-fastsam","title":"Segmenting remote sensing imagery with FastSAM\u00b6","text":"<p>FastSAM: https://github.com/CASIA-IVA-Lab/FastSAM</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/fast_sam/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/fast_sam/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/fast_sam/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/fast_sam/#initialize-samgeo-class","title":"Initialize SamGeo class\u00b6","text":"<p>The initialization of the SamGeo class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/input_prompts/","title":"Input prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, tms_to_geotiff\n</pre> import os import leafmap from samgeo import SamGeo, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/input_prompts/#generating-object-masks-from-input-prompts-with-sam","title":"Generating object masks from input prompts with SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p> <p>The notebook is adapted from segment-anything/notebooks/predictor_example.ipynb, but I have made it much easier to save the segmentation results and visualize them.</p>"},{"location":"examples/input_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/input_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/input_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/input_prompts/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/input_prompts/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/input_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/input_prompts_hq/","title":"Input prompts hq","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo.hq_sam import SamGeo, tms_to_geotiff\n</pre> import os import leafmap from samgeo.hq_sam import SamGeo, tms_to_geotiff In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[37.6412, -122.1353], zoom=15, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>if m.user_roi is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-122.1497, 37.6311, -122.1203, 37.6458]\n</pre> if m.user_roi is not None:     bbox = m.user_roi_bounds() else:     bbox = [-122.1497, 37.6311, -122.1203, 37.6458] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True)\n</pre> image = \"satellite.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=16, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",  # can be vit_h, vit_b, vit_l, vit_tiny     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1419, 37.6383]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-122.1419, 37.6383]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [[-122.1464, 37.6431], [-122.1449, 37.6415], [-122.1451, 37.6395]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p>"},{"location":"examples/input_prompts_hq/#generating-object-masks-from-input-prompts-with-hq-sam","title":"Generating object masks from input prompts with HQ-SAM\u00b6","text":"<p>This notebook shows how to generate object masks from input prompts with the High-Quality Segment Anything Model (HQ-SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/input_prompts_hq/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/input_prompts_hq/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/input_prompts_hq/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/input_prompts_hq/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":"<p>Specify the file path to the model checkpoint. If it is not specified, the model will to downloaded to the working directory.</p>"},{"location":"examples/input_prompts_hq/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"examples/input_prompts_hq/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"examples/maxar_open_data/","title":"Maxar open data","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, raster_to_vector, overlay_images\n</pre> import os import leafmap from samgeo import SamGeo, raster_to_vector, overlay_images In\u00a0[\u00a0]: Copied! <pre>url = \"https://github.com/opengeos/datasets/releases/download/raster/Derna_sample.tif\"\n</pre> url = \"https://github.com/opengeos/datasets/releases/download/raster/Derna_sample.tif\" In\u00a0[\u00a0]: Copied! <pre>leafmap.download_file(url, output=\"image.tif\")\n</pre> leafmap.download_file(url, output=\"image.tif\") In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm.add_raster(\"image.tif\", layer_name=\"Image\")\nm.add_layer_manager()\nm\n</pre> m = leafmap.Map(height=\"600px\") m.add_basemap(\"SATELLITE\") m.add_raster(\"image.tif\", layer_name=\"Image\") m.add_layer_manager() m <p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 80,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 80, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True)\n</pre> sam.generate(\"image.tif\", output=\"mask.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>raster_to_vector(\"mask.tif\", output=\"mask.shp\")\n</pre> raster_to_vector(\"mask.tif\", output=\"mask.shp\") In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p>Display the annotations (each mask with a random color).</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotation.tif\") In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotation.tif\",\n    label1=\"Image\",\n    label2=\"Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotation.tif\",     label1=\"Image\",     label2=\"Segmentation\", ) <p>Overlay the annotations on the image and use the slider to change the opacity interactively.</p> In\u00a0[\u00a0]: Copied! <pre>overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\")\n</pre> overlay_images(\"image.tif\", \"annotation.tif\", backend=\"TkAgg\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0)\nm.add_raster(\"annotation.tif\", layer_name=\"Annotation\")\nm\n</pre> m.add_raster(\"mask.tif\", layer_name=\"Mask\", nodata=0) m.add_raster(\"annotation.tif\", layer_name=\"Annotation\") m In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None)\n</pre> m.add_vector(\"mask.shp\", layer_name=\"Vector\", info_mode=None) <p></p>"},{"location":"examples/maxar_open_data/#segmenting-satellite-imagery-from-the-maxar-open-data-program","title":"Segmenting satellite imagery from the Maxar Open Data Program\u00b6","text":"<p>This notebook shows how to segment satellite imagery from the Maxar Open Data program for Libya floods.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/maxar_open_data/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/maxar_open_data/#download-sample-data","title":"Download sample data\u00b6","text":"<p>First, let's download a sample image of Derna, Libya from here.</p>"},{"location":"examples/maxar_open_data/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/maxar_open_data/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/maxar_open_data/#segment-the-image","title":"Segment the image\u00b6","text":""},{"location":"examples/maxar_open_data/#convert-raster-to-vector","title":"Convert raster to vector\u00b6","text":""},{"location":"examples/maxar_open_data/#display-the-segmentation-result","title":"Display the segmentation result\u00b6","text":"<p>First, let's show the result as a binary image.</p>"},{"location":"examples/maxar_open_data/#compare-images-with-a-slider","title":"Compare images with a slider\u00b6","text":""},{"location":"examples/maxar_open_data/#display-images-on-an-interactive-map","title":"Display images on an interactive map.\u00b6","text":""},{"location":"examples/satellite-predictor/","title":"Satellite predictor","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeoPredictor, tms_to_geotiff, get_basemaps\nfrom segment_anything import sam_model_registry\n</pre> import os import leafmap from samgeo import SamGeoPredictor, tms_to_geotiff, get_basemaps from segment_anything import sam_model_registry In\u00a0[\u00a0]: Copied! <pre>zoom = 16\nm = leafmap.Map(center=[45, -123], zoom=zoom)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> zoom = 16 m = leafmap.Map(center=[45, -123], zoom=zoom) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-123.0127, 44.9957, -122.9874, 45.0045]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-123.0127, 44.9957, -122.9874, 45.0045] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n# image = '/path/to/your/own/image.tif'\n</pre> image = \"satellite.tif\" # image = '/path/to/your/own/image.tif' <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(\n    output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True\n)\n</pre> tms_to_geotiff(     output=image, bbox=bbox, zoom=zoom + 1, source=\"Satellite\", overwrite=True ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.add_raster(image, layer_name=\"Image\") m <p>Use the draw tools to draw a rectangle from which to subset segmentations on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    clip_box = m.user_roi_bounds()\nelse:\n    clip_box = [-123.0064, 44.9988, -123.0005, 45.0025]\n</pre> if m.user_roi_bounds() is not None:     clip_box = m.user_roi_bounds() else:     clip_box = [-123.0064, 44.9988, -123.0005, 45.0025] In\u00a0[\u00a0]: Copied! <pre>clip_box\n</pre> clip_box In\u00a0[\u00a0]: Copied! <pre>out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\")\ncheckpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\")\n</pre> out_dir = os.path.join(os.path.expanduser(\"~\"), \"Downloads\") checkpoint = os.path.join(out_dir, \"sam_vit_h_4b8939.pth\") In\u00a0[\u00a0]: Copied! <pre>import cv2\n\nimg_arr = cv2.imread(image)\n\nmodel_type = \"vit_h\"\n\nsam = sam_model_registry[model_type](checkpoint=checkpoint)\n\npredictor = SamGeoPredictor(sam)\n\npredictor.set_image(img_arr)\n\nmasks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box)\n</pre> import cv2  img_arr = cv2.imread(image)  model_type = \"vit_h\"  sam = sam_model_registry[model_type](checkpoint=checkpoint)  predictor = SamGeoPredictor(sam)  predictor.set_image(img_arr)  masks, _, _ = predictor.predict(src_fp=image, geo_box=clip_box) In\u00a0[\u00a0]: Copied! <pre>masks_img = \"preds.tif\"\npredictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\"))\n</pre> masks_img = \"preds.tif\" predictor.masks_to_geotiff(image, masks_img, masks.astype(\"uint8\")) In\u00a0[\u00a0]: Copied! <pre>vector = \"feats.geojson\"\ngdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1)\ngdf.plot()\n</pre> vector = \"feats.geojson\" gdf = predictor.geotiff_to_geojson(masks_img, vector, bidx=1) gdf.plot() In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m"},{"location":"examples/satellite-predictor/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/satellite-predictor/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/satellite-predictor/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/satellite-predictor/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/satellite-predictor/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/satellite-predictor/#initialize-samgeopredictor-class","title":"Initialize SamGeoPredictor class\u00b6","text":""},{"location":"examples/satellite-predictor/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/satellite/","title":"Satellite","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial\n</pre> # %pip install segment-geospatial In\u00a0[\u00a0]: Copied! <pre>import os\nimport leafmap\nfrom samgeo import SamGeo, tms_to_geotiff, get_basemaps\n</pre> import os import leafmap from samgeo import SamGeo, tms_to_geotiff, get_basemaps In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[29.676840, -95.369222], zoom=19)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[29.676840, -95.369222], zoom=19) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-95.3704, 29.6762, -95.368, 29.6775]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-95.3704, 29.6762, -95.368, 29.6775] In\u00a0[\u00a0]: Copied! <pre>image = \"satellite.tif\"\n</pre> image = \"satellite.tif\" <p>Besides the <code>satellite</code> basemap, you can use any of the following basemaps returned by the <code>get_basemaps()</code> function:</p> In\u00a0[\u00a0]: Copied! <pre># get_basemaps().keys()\n</pre> # get_basemaps().keys() <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True)\n</pre> tms_to_geotiff(output=image, bbox=bbox, zoom=20, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    checkpoint=\"sam_vit_h_4b8939.pth\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     checkpoint=\"sam_vit_h_4b8939.pth\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>mask = \"segment.tif\"\nsam.generate(\n    image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255\n)\n</pre> mask = \"segment.tif\" sam.generate(     image, mask, batch=True, foreground=True, erosion_kernel=(3, 3), mask_multiplier=255 ) In\u00a0[\u00a0]: Copied! <pre>vector = \"segment.gpkg\"\nsam.tiff_to_gpkg(mask, vector, simplify_tolerance=None)\n</pre> vector = \"segment.gpkg\" sam.tiff_to_gpkg(mask, vector, simplify_tolerance=None) <p>You can also save the segmentation results as any vector data format supported by GeoPandas.</p> In\u00a0[\u00a0]: Copied! <pre>shapefile = \"segment.shp\"\nsam.tiff_to_vector(mask, shapefile)\n</pre> shapefile = \"segment.shp\" sam.tiff_to_vector(mask, shapefile) In\u00a0[\u00a0]: Copied! <pre>style = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(vector, layer_name=\"Vector\", style=style)\nm\n</pre> style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(vector, layer_name=\"Vector\", style=style) m <p></p>"},{"location":"examples/satellite/#segment-anything-model-for-geospatial-data","title":"Segment Anything Model for Geospatial Data\u00b6","text":"<p>This notebook shows how to use segment satellite imagery using the Segment Anything Model (SAM) with a few lines of code.</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/satellite/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/satellite/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"examples/satellite/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/satellite/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"examples/satellite/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"examples/satellite/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Set <code>batch=True</code> to segment the image in batches. This is useful for large images that cannot fit in memory.</p>"},{"location":"examples/satellite/#polygonize-the-raster-data","title":"Polygonize the raster data\u00b6","text":"<p>Save the segmentation results as a GeoPackage file.</p>"},{"location":"examples/satellite/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"examples/swimming_pools/","title":"Swimming pools","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[34.040984, -118.491668], zoom=19, height=\"600px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-118.4932, 34.0404, -118.4903, 34.0417]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-118.4932, 34.0404, -118.4903, 34.0417] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"swimming pool\"\n</pre> text_prompt = \"swimming pool\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     box_color=\"red\",     title=\"Automatic Segmentation of Swimming Pools\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Blues\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Swimming Pools\",\n)\n</pre> sam.show_anns(     cmap=\"Blues\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Swimming Pools\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Swimming Pools\",\n    blend=False,\n    output=\"pools.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Swimming Pools\",     blend=False,     output=\"pools.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"pools.tif\", \"pools.shp\")\n</pre> sam.raster_to_vector(\"pools.tif\", \"pools.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"pools.tif\", layer_name=\"Pools\", palette=\"Blues\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"pools.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/swimming_pools/#mapping-swimming-pools-with-text-prompts","title":"Mapping swimming pools with text prompts\u00b6","text":"<p>This notebook shows how to map swimming pools with text prompts and the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/swimming_pools/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/swimming_pools/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/swimming_pools/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/swimming_pools/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/swimming_pools/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/swimming_pools/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/swimming_pools/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/swimming_pools/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/text_prompts/","title":"Text prompts","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.17615, -51.253043], zoom=18, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.2565, -22.1777, -51.2512, -22.175]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.2565, -22.1777, -51.2512, -22.175] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p></p> <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"examples/text_prompts/#segmenting-remote-sensing-imagery-with-text-prompts-and-the-segment-anything-model-sam","title":"Segmenting remote sensing imagery with text prompts and the Segment Anything Model (SAM)\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/text_prompts/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/text_prompts/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/text_prompts/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/text_prompts/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/text_prompts/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/text_prompts/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/text_prompts/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"examples/text_prompts/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"examples/text_prompts_batch/","title":"Text prompts batch","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import tms_to_geotiff, split_raster\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import tms_to_geotiff, split_raster from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\")\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[-22.1278, -51.4430], zoom=17, height=\"800px\") m.add_basemap(\"SATELLITE\") m In\u00a0[\u00a0]: Copied! <pre>bbox = m.user_roi_bounds()\nif bbox is None:\n    bbox = [-51.4494, -22.1307, -51.4371, -22.1244]\n</pre> bbox = m.user_roi_bounds() if bbox is None:     bbox = [-51.4494, -22.1307, -51.4371, -22.1244] In\u00a0[\u00a0]: Copied! <pre>image = \"Image.tif\"\ntms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True)\n</pre> image = \"Image.tif\" tms_to_geotiff(output=image, bbox=bbox, zoom=19, source=\"Satellite\", overwrite=True) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0)\n</pre> split_raster(image, out_dir=\"tiles\", tile_size=(1000, 1000), overlap=0) In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict_batch(\n    images=\"tiles\",\n    out_dir=\"masks\",\n    text_prompt=text_prompt,\n    box_threshold=0.24,\n    text_threshold=0.24,\n    mask_multiplier=255,\n    dtype=\"uint8\",\n    merge=True,\n    verbose=True,\n)\n</pre> sam.predict_batch(     images=\"tiles\",     out_dir=\"masks\",     text_prompt=text_prompt,     box_threshold=0.24,     text_threshold=0.24,     mask_multiplier=255,     dtype=\"uint8\",     merge=True,     verbose=True, ) In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\")\nm.add_layer_manager()\nm\n</pre> m.add_raster(\"masks/merged.tif\", cmap=\"viridis\", nodata=0, layer_name=\"Mask\") m.add_layer_manager() m <p></p>"},{"location":"examples/text_prompts_batch/#batch-segmentation-with-text-prompts","title":"Batch segmentation with text prompts\u00b6","text":"<p>This notebook shows how to generate object masks from text prompts with the Segment Anything Model (SAM).</p> <p>Make sure you use GPU runtime for this notebook. For Google Colab, go to <code>Runtime</code> -&gt; <code>Change runtime type</code> and select <code>GPU</code> as the hardware accelerator.</p>"},{"location":"examples/text_prompts_batch/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"examples/text_prompts_batch/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"examples/text_prompts_batch/#download-a-sample-image","title":"Download a sample image\u00b6","text":"<p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p>"},{"location":"examples/text_prompts_batch/#split-the-image-into-tiles","title":"Split the image into tiles\u00b6","text":""},{"location":"examples/text_prompts_batch/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"examples/text_prompts_batch/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"examples/text_prompts_batch/#segment-images","title":"Segment images\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"examples/text_prompts_batch/#visualize-the-results","title":"Visualize the results\u00b6","text":""},{"location":"workshops/cn_workshop/","title":"Cn workshop","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import SamGeo from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-86.9167, 40.4262, -86.9105, 40.4289]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-86.9167, 40.4262, -86.9105, 40.4289] In\u00a0[\u00a0]: Copied! <pre>image = \"image.tif\"\n</pre> image = \"image.tif\" <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p></p> <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p></p> <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", opacity=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", opacity=0.5, layer_name=\"Masks\") m <p></p> <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"masks.tif\", \"masks.shp\")\n</pre> sam.raster_to_vector(\"masks.tif\", \"masks.shp\") In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"masks.shp\", layer_name=\"Masks vector\")\n</pre> m.add_vector(\"masks.shp\", layer_name=\"Masks vector\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-86.913162, 40.427157]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-86.913162, 40.427157]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p></p> <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [\n    [-86.913162, 40.427157],\n    [-86.913425, 40.427157],\n    [-86.91343, 40.427721],\n    [-86.913012, 40.427741],\n]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [     [-86.913162, 40.427157],     [-86.913425, 40.427157],     [-86.91343, 40.427721],     [-86.913012, 40.427741], ] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-86.913654, 40.426967, -86.912774, 40.427881],\n        [-86.914780, 40.426256, -86.913997, 40.426852],\n        [-86.913632, 40.426215, -86.912581, 40.426820],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-86.913654, 40.426967, -86.912774, 40.427881],         [-86.914780, 40.426256, -86.913997, 40.426852],         [-86.913632, 40.426215, -86.912581, 40.426820],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.6, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.6, layer_name=\"Mask\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"workshops/cn_workshop/#samgeo-workshop","title":"SamGeo Workshop\u00b6","text":"<p>This notebook is for the workshop presented at the \u7b2c\u4e03\u5c4a\u5730\u7403\u7a7a\u95f4\u5927\u6570\u636e\u4e0e\u4e91\u8ba1\u7b97\u524d\u6cbf\u4f1a\u8bae\u4e0e\u96c6\u4e2d\u5b66\u4e60.</p>"},{"location":"workshops/cn_workshop/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"workshops/cn_workshop/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"workshops/cn_workshop/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"workshops/cn_workshop/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"workshops/cn_workshop/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"workshops/cn_workshop/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":""},{"location":"workshops/cn_workshop/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/cn_workshop/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"workshops/cn_workshop/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"workshops/cn_workshop/#use-points-as-input-prompts","title":"Use points as input prompts\u00b6","text":""},{"location":"workshops/cn_workshop/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/cn_workshop/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"workshops/cn_workshop/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"workshops/cn_workshop/#bounding-box-input-prompts","title":"Bounding box input prompts\u00b6","text":""},{"location":"workshops/cn_workshop/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"workshops/cn_workshop/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"workshops/cn_workshop/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"workshops/cn_workshop/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"workshops/cn_workshop/#text-promots","title":"Text promots\u00b6","text":""},{"location":"workshops/cn_workshop/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"workshops/cn_workshop/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"workshops/cn_workshop/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"workshops/cn_workshop/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"workshops/cn_workshop/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""},{"location":"workshops/purdue/","title":"Purdue","text":"In\u00a0[\u00a0]: Copied! <pre># %pip install segment-geospatial groundingdino-py leafmap localtileserver\n</pre> # %pip install segment-geospatial groundingdino-py leafmap localtileserver In\u00a0[\u00a0]: Copied! <pre>import leafmap\nfrom samgeo import SamGeo\nfrom samgeo.text_sam import LangSAM\n</pre> import leafmap from samgeo import SamGeo from samgeo.text_sam import LangSAM In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nm.add_basemap(\"SATELLITE\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) m.add_basemap(\"SATELLITE\") m <p>Pan and zoom the map to select the area of interest. Use the draw tools to draw a polygon or rectangle on the map</p> In\u00a0[\u00a0]: Copied! <pre>if m.user_roi_bounds() is not None:\n    bbox = m.user_roi_bounds()\nelse:\n    bbox = [-86.9167, 40.4262, -86.9105, 40.4289]\n</pre> if m.user_roi_bounds() is not None:     bbox = m.user_roi_bounds() else:     bbox = [-86.9167, 40.4262, -86.9105, 40.4289] In\u00a0[\u00a0]: Copied! <pre>image = \"image.tif\"\n</pre> image = \"image.tif\" <p>Specify the basemap as the source.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.map_tiles_to_geotiff(\n    output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True\n)\n</pre> leafmap.map_tiles_to_geotiff(     output=image, bbox=bbox, zoom=18, source=\"Satellite\", overwrite=True ) <p>You can also use your own image. Uncomment and run the following cell to use your own image.</p> In\u00a0[\u00a0]: Copied! <pre># image = '/path/to/your/own/image.tif'\n</pre> # image = '/path/to/your/own/image.tif' <p>Display the downloaded image on the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.layers[-1].visible = False  # turn off the basemap\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m.layers[-1].visible = False  # turn off the basemap m.add_raster(image, layer_name=\"Image\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=None, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks.tif\", foreground=True, unique=True)\n</pre> sam.generate(image, output=\"masks.tif\", foreground=True, unique=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p></p> <p>Show the object annotations (objects with random color) on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\")\n</pre> sam.show_anns(axis=\"off\", alpha=1, output=\"annotations.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    \"image.tif\",\n    \"annotations.tif\",\n    label1=\"Satellite Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     \"image.tif\",     \"annotations.tif\",     label1=\"Satellite Image\",     label2=\"Image Segmentation\", ) <p></p> <p>Add image to the map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"annotations.tif\", opacity=0.5, layer_name=\"Masks\")\nm\n</pre> m.add_raster(\"annotations.tif\", opacity=0.5, layer_name=\"Masks\") m <p></p> <p>Convert the object annotations to vector format, such as GeoPackage, Shapefile, or GeoJSON.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"masks.tif\", \"masks.shp\")\n</pre> sam.raster_to_vector(\"masks.tif\", \"masks.shp\") In\u00a0[\u00a0]: Copied! <pre>m.add_vector(\"masks.shp\", layer_name=\"Masks vector\")\n</pre> m.add_vector(\"masks.shp\", layer_name=\"Masks vector\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam_kwargs = {\n    \"points_per_side\": 32,\n    \"pred_iou_thresh\": 0.86,\n    \"stability_score_thresh\": 0.92,\n    \"crop_n_layers\": 1,\n    \"crop_n_points_downscale_factor\": 2,\n    \"min_mask_region_area\": 100,\n}\n</pre> sam_kwargs = {     \"points_per_side\": 32,     \"pred_iou_thresh\": 0.86,     \"stability_score_thresh\": 0.92,     \"crop_n_layers\": 1,     \"crop_n_points_downscale_factor\": 2,     \"min_mask_region_area\": 100, } In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    sam_kwargs=sam_kwargs,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     sam_kwargs=sam_kwargs, ) In\u00a0[\u00a0]: Copied! <pre>sam.generate(image, output=\"masks2.tif\", foreground=True)\n</pre> sam.generate(image, output=\"masks2.tif\", foreground=True) In\u00a0[\u00a0]: Copied! <pre>sam.show_masks(cmap=\"binary_r\")\n</pre> sam.show_masks(cmap=\"binary_r\") <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\")\n</pre> sam.show_anns(axis=\"off\", opacity=1, output=\"annotations2.tif\") <p></p> <p>Compare images with a slider.</p> In\u00a0[\u00a0]: Copied! <pre>leafmap.image_comparison(\n    image,\n    \"annotations.tif\",\n    label1=\"Image\",\n    label2=\"Image Segmentation\",\n)\n</pre> leafmap.image_comparison(     image,     \"annotations.tif\",     label1=\"Image\",     label2=\"Image Segmentation\", ) <p>Set <code>automatic=False</code> to disable the <code>SamAutomaticMaskGenerator</code> and enable the <code>SamPredictor</code>.</p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>point_coords = [[-86.913162, 40.427157]]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\")\nm.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1)\nm\n</pre> point_coords = [[-86.913162, 40.427157]] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask1.tif\") m.add_raster(\"mask1.tif\", layer_name=\"Mask1\", nodata=0, cmap=\"Blues\", opacity=1) m <p></p> <p>Try multiple points input:</p> In\u00a0[\u00a0]: Copied! <pre>point_coords = [\n    [-86.913162, 40.427157],\n    [-86.913425, 40.427157],\n    [-86.91343, 40.427721],\n    [-86.913012, 40.427741],\n]\nsam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\")\nm.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1)\nm\n</pre> point_coords = [     [-86.913162, 40.427157],     [-86.913425, 40.427157],     [-86.91343, 40.427721],     [-86.913012, 40.427741], ] sam.predict(point_coords, point_labels=1, point_crs=\"EPSG:4326\", output=\"mask2.tif\") m.add_raster(\"mask2.tif\", layer_name=\"Mask2\", nodata=0, cmap=\"Greens\", opacity=1) m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = sam.show_map()\nm\n</pre> m = sam.show_map() m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = SamGeo(\n    model_type=\"vit_h\",\n    automatic=False,\n    sam_kwargs=None,\n)\n</pre> sam = SamGeo(     model_type=\"vit_h\",     automatic=False,     sam_kwargs=None, ) <p>Specify the image to segment.</p> In\u00a0[\u00a0]: Copied! <pre>sam.set_image(image)\n</pre> sam.set_image(image) In\u00a0[\u00a0]: Copied! <pre>if m.user_rois is not None:\n    boxes = m.user_rois\nelse:\n    boxes = [\n        [-86.913654, 40.426967, -86.912774, 40.427881],\n        [-86.914780, 40.426256, -86.913997, 40.426852],\n        [-86.913632, 40.426215, -86.912581, 40.426820],\n    ]\n</pre> if m.user_rois is not None:     boxes = m.user_rois else:     boxes = [         [-86.913654, 40.426967, -86.912774, 40.427881],         [-86.914780, 40.426256, -86.913997, 40.426852],         [-86.913632, 40.426215, -86.912581, 40.426820],     ] In\u00a0[\u00a0]: Copied! <pre>sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\")\n</pre> sam.predict(boxes=boxes, point_crs=\"EPSG:4326\", output=\"mask.tif\", dtype=\"uint8\") In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.6, layer_name=\"Mask\")\nm\n</pre> m.add_raster(\"mask.tif\", cmap=\"viridis\", nodata=0, opacity=0.6, layer_name=\"Mask\") m <p></p> In\u00a0[\u00a0]: Copied! <pre>m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700)\nimage = \"image.tif\"\nm.add_raster(image, layer_name=\"Image\")\nm\n</pre> m = leafmap.Map(center=[40.427495, -86.913638], zoom=18, height=700) image = \"image.tif\" m.add_raster(image, layer_name=\"Image\") m In\u00a0[\u00a0]: Copied! <pre>sam = LangSAM()\n</pre> sam = LangSAM() In\u00a0[\u00a0]: Copied! <pre>text_prompt = \"tree\"\n</pre> text_prompt = \"tree\" In\u00a0[\u00a0]: Copied! <pre>sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24)\n</pre> sam.predict(image, text_prompt, box_threshold=0.24, text_threshold=0.24) In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    box_color=\"red\",\n    title=\"Automatic Segmentation of Trees\",\n    blend=True,\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     box_color=\"red\",     title=\"Automatic Segmentation of Trees\",     blend=True, ) <p></p> <p>Show the result without bounding boxes on the map.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greens\",\n    add_boxes=False,\n    alpha=0.5,\n    title=\"Automatic Segmentation of Trees\",\n)\n</pre> sam.show_anns(     cmap=\"Greens\",     add_boxes=False,     alpha=0.5,     title=\"Automatic Segmentation of Trees\", ) <p></p> <p>Show the result as a grayscale image.</p> In\u00a0[\u00a0]: Copied! <pre>sam.show_anns(\n    cmap=\"Greys_r\",\n    add_boxes=False,\n    alpha=1,\n    title=\"Automatic Segmentation of Trees\",\n    blend=False,\n    output=\"trees.tif\",\n)\n</pre> sam.show_anns(     cmap=\"Greys_r\",     add_boxes=False,     alpha=1,     title=\"Automatic Segmentation of Trees\",     blend=False,     output=\"trees.tif\", ) <p>Convert the result to a vector format.</p> In\u00a0[\u00a0]: Copied! <pre>sam.raster_to_vector(\"trees.tif\", \"trees.shp\")\n</pre> sam.raster_to_vector(\"trees.tif\", \"trees.shp\") <p>Show the results on the interactive map.</p> In\u00a0[\u00a0]: Copied! <pre>m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0)\nstyle = {\n    \"color\": \"#3388ff\",\n    \"weight\": 2,\n    \"fillColor\": \"#7c4185\",\n    \"fillOpacity\": 0.5,\n}\nm.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style)\nm\n</pre> m.add_raster(\"trees.tif\", layer_name=\"Trees\", palette=\"Greens\", opacity=0.5, nodata=0) style = {     \"color\": \"#3388ff\",     \"weight\": 2,     \"fillColor\": \"#7c4185\",     \"fillOpacity\": 0.5, } m.add_vector(\"trees.shp\", layer_name=\"Vector\", style=style) m <p></p> In\u00a0[\u00a0]: Copied! <pre>sam.show_map()\n</pre> sam.show_map() <p></p>"},{"location":"workshops/purdue/#purdue-samgeo-workshop","title":"Purdue SamGeo Workshop\u00b6","text":"<p>This notebook is for the workshop presented at the Purdue GIS Day 2023.</p> <ul> <li>Slides: https://bit.ly/purdue-samgeo</li> <li>Notebook: https://samgeo.gishub.org/workshops/purdue</li> </ul>"},{"location":"workshops/purdue/#install-dependencies","title":"Install dependencies\u00b6","text":"<p>Uncomment and run the following cell to install the required dependencies.</p>"},{"location":"workshops/purdue/#import-libraries","title":"Import libraries\u00b6","text":""},{"location":"workshops/purdue/#download-sample-data","title":"Download sample data\u00b6","text":""},{"location":"workshops/purdue/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"workshops/purdue/#download-map-tiles","title":"Download map tiles\u00b6","text":"<p>Download maps tiles and mosaic them into a single GeoTIFF file</p>"},{"location":"workshops/purdue/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":""},{"location":"workshops/purdue/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/purdue/#automatic-mask-generation","title":"Automatic mask generation\u00b6","text":"<p>Segment the image and save the results to a GeoTIFF file. Set <code>unique=True</code> to assign a unique ID to each object.</p>"},{"location":"workshops/purdue/#automatic-mask-generation-options","title":"Automatic mask generation options\u00b6","text":"<p>There are several tunable parameters in automatic mask generation that control how densely points are sampled and what the thresholds are for removing low quality or duplicate masks. Additionally, generation can be automatically run on crops of the image to get improved performance on smaller objects, and post-processing can remove stray pixels and holes. Here is an example configuration that samples more masks:</p>"},{"location":"workshops/purdue/#use-points-as-input-prompts","title":"Use points as input prompts\u00b6","text":""},{"location":"workshops/purdue/#initialize-sam-class","title":"Initialize SAM class\u00b6","text":""},{"location":"workshops/purdue/#image-segmentation-with-input-points","title":"Image segmentation with input points\u00b6","text":"<p>A single point can be used to segment an object. The point can be specified as a tuple of (x, y), such as (col, row) or (lon, lat). The points can also be specified as a file path to a vector dataset. For non (col, row) input points, specify the <code>point_crs</code> parameter, which will automatically transform the points to the image column and row coordinates.</p> <p>Try a single point input:</p>"},{"location":"workshops/purdue/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":"<p>Display the interactive map and use the marker tool to draw points on the map. Then click on the <code>Segment</code> button to segment the objects. The results will be added to the map automatically. Click on the <code>Reset</code> button to clear the points and the results.</p>"},{"location":"workshops/purdue/#bounding-box-input-prompts","title":"Bounding box input prompts\u00b6","text":""},{"location":"workshops/purdue/#create-an-interactive-map","title":"Create an interactive map\u00b6","text":""},{"location":"workshops/purdue/#create-bounding-boxes","title":"Create bounding boxes\u00b6","text":"<p>If no rectangles are drawn, the default bounding boxes will be used as follows:</p>"},{"location":"workshops/purdue/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Use the <code>predict()</code> method to segment the image with specified bounding boxes. The <code>boxes</code> parameter accepts a list of bounding box coordinates in the format of [[left, bottom, right, top], [left, bottom, right, top], ...], a GeoJSON dictionary, or a file path to a GeoJSON file.</p>"},{"location":"workshops/purdue/#display-the-result","title":"Display the result\u00b6","text":"<p>Add the segmented image to the map.</p>"},{"location":"workshops/purdue/#text-promots","title":"Text promots\u00b6","text":""},{"location":"workshops/purdue/#initialize-langsam-class","title":"Initialize LangSAM class\u00b6","text":"<p>The initialization of the LangSAM class might take a few minutes. The initialization downloads the model weights and sets up the model for inference.</p>"},{"location":"workshops/purdue/#specify-text-prompts","title":"Specify text prompts\u00b6","text":""},{"location":"workshops/purdue/#segment-the-image","title":"Segment the image\u00b6","text":"<p>Part of the model prediction includes setting appropriate thresholds for object detection and text association with the detected objects. These threshold values range from 0 to 1 and are set while calling the predict method of the LangSAM class.</p> <p><code>box_threshold</code>: This value is used for object detection in the image. A higher value makes the model more selective, identifying only the most confident object instances, leading to fewer overall detections. A lower value, conversely, makes the model more tolerant, leading to increased detections, including potentially less confident ones.</p> <p><code>text_threshold</code>: This value is used to associate the detected objects with the provided text prompt. A higher value requires a stronger association between the object and the text prompt, leading to more precise but potentially fewer associations. A lower value allows for looser associations, which could increase the number of associations but also introduce less precise matches.</p> <p>Remember to test different threshold values on your specific data. The optimal threshold can vary depending on the quality and nature of your images, as well as the specificity of your text prompts. Make sure to choose a balance that suits your requirements, whether that's precision or recall.</p>"},{"location":"workshops/purdue/#visualize-the-results","title":"Visualize the results\u00b6","text":"<p>Show the result with bounding boxes on the map.</p>"},{"location":"workshops/purdue/#interactive-segmentation","title":"Interactive segmentation\u00b6","text":""}]}